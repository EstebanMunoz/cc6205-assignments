{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UwaDuQqCOyLJ"
      },
      "source": [
        "# **Tarea 4 - CC6205 Natural Language Processing üìö**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "X4lL5hGw07yP"
      },
      "source": [
        "Bienvenid@s a la cuarta tarea del curso de Natural Language Processing (NLP).\n",
        "En esta tarea estaremos tratando el problema de **tagging** (generaci√≥n de secuencias de etiquetas del mismo largo que la secuencia de input), el uso de **Convolutional Neural Networks** y **Recurrent Neural Networks**, e implementaremos una red usando PyTorch."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ANqzQ3G9WNw3"
      },
      "source": [
        "# Hidden Markov Models (HMM), Maximum Entropy Markov Models (MEMM) and Conditional Random Field(CRF) (1,5 puntos)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bWXD3D7RYKJ-"
      },
      "source": [
        "### Pregunta 1 (1 pt)\n",
        "Para un problema de POS tagging se define el conjunto de etiquetas $S = \\{ \\text{DET}, \\text{NOUN}, \\text{VERB}, \\text{ADP} \\}$ y se tiene un Hidden Markov Model con los siguientes par√°metros estimados a partir de un corpus de entrenamiento:\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{split}\n",
        "q(\\text{NOUN}| \\text{ VERB}, \\text{DET}) &= 0.3 \\\\\n",
        "q(\\text{NOUN}|\\ w, \\text{DET}) &= 0 \\qquad \\forall w \\in S, w \\neq \\text{VERB} \\\\\n",
        "q(\\text{DET}| \\text{ VERB}, \\text{NOUN}) &= 0.4 \\\\\n",
        "q(\\text{DET}|\\ w, \\text{NOUN}) &= 0 \\qquad \\forall w \\in S, w \\neq \\text{VERB} \\\\\n",
        "e(the|\\text{ DET}) &= 0.5 \\\\\n",
        "e(pasta|\\text{ NOUN}) &= 0.6\n",
        "\\end{split}\n",
        "\\end{equation}\n",
        "\n",
        "Luego para la oraci√≥n: `the man is pouring sauce on the pasta`, se tiene una tabla de programaci√≥n din√°mica con los siguientes valores:\n",
        "\\begin{equation}\n",
        "\\begin{split}\n",
        "\\pi(7,\\text{DET},\\text{DET})&=0.1\\\\\n",
        "\\pi(7,\\text{NOUN},\\text{DET})&=0.2\\\\\n",
        "\\pi(7,\\text{VERB},\\text{DET})&=0.01\\\\\n",
        "\\pi(7,\\text{ADP},\\text{DET})&=0.5\n",
        "\\end{split}\n",
        "\\end{equation}\n",
        "\n",
        "Con esta informaci√≥n, calcule el valor de $\\pi(8,\\text{DET},\\text{NOUN})$. Puede dejar el resultado expresado como una fracci√≥n.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5EzgysW9kGi-"
      },
      "source": [
        ">En base a la definici√≥n recursiva, se busca calcular lo siguiente:\n",
        ">\n",
        ">\\begin{equation}\n",
        ">\\begin{split}\n",
        ">\\pi(8,\\text{DET},\\text{NOUN}) & = max_{w\\in S_{6}} ( \\pi(7,w,\\text{DET}) \\times q(\\text{NOUN}|w,\\text{DET}) \\times q(\\text{pasta}|\\text{NOUN}) )\\\\\n",
        ">\\end{split}\n",
        ">\\end{equation}\n",
        ">\n",
        ">\n",
        ">Se tiene directamente el valor de $q(\\text{pasta}|\\text{NOUN})=0.6$.\n",
        ">Luego, es importante considerar que $q(\\text{NOUN}|w,\\text{DET}) = 0$ para los casos $w \\neq \\text{VERB}$ y que $\\pi(7,w,\\text{DET}) > 0$ para $w \\in S$.\n",
        ">\n",
        ">Combinando ambos casos se llega a que $\\pi(7,w,\\text{DET}) \\times q(\\text{NOUN}|w,\\text{DET}) > 0$ solamente para $w=\\text{VERB}$.\n",
        ">Por ende, se concluye que el argumento se maximiza con $w=\\text{VERB}$.\n",
        ">\n",
        ">Luego, la ecuaci√≥n queda como:\n",
        ">\n",
        ">\\begin{equation}\n",
        ">\\begin{split}\n",
        ">\\pi(8,\\text{DET},\\text{NOUN}) & = \\pi(7,\\text{VERB},\\text{DET}) \\times q(\\text{NOUN}|\\text{VERB},\\text{DET}) \\times 0.6 \\\\\n",
        ">& = \\pi(7,\\text{VERB},\\text{DET}) \\times 0.3 \\times 0.6\\\\\n",
        ">& = 0.01 \\times 0.3 \\times 0.6\\\\\n",
        ">\\pi(8,\\text{DET},\\text{NOUN}) & = 0.0018\n",
        ">\\end{split}\n",
        ">\\end{equation}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oiwJb_vmkKLZ"
      },
      "source": [
        "### Pregunta 2 (0.5 pts)\n",
        "Comente  sobre las similitudes o diferencias entre los HMMs, MEMMs y CRFs. Para esto, responda las siguientes preguntas.\n",
        "\n",
        "#### 2.1. ¬øPara qu√© tipo de tarea sirven? D√© dos ejemplo de este tipo de tarea y descr√≠balos brevemente. (0.1 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ">Las tareas que pueden resolver estos modelos involucran etiquetado de secuencias, como por ejemplo POS tagging (*part of speech tagging*) y NER (*Named Entity Recognition*). POS Tagging entrega un tag a cada palabra de un texto, indicando el tipo de palabra (adverbio, sustantivo, verbo, etc). Por otra parte, NER identifica los pronombres personales de un texto y los asocia con lugares, personas, empresas, etc. Un ejemplo de ello es \"[...] *New York* [...]\", con etiquetas *Start Location* y *Continue Location*, respectivamente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2.2. ¬øQu√© modelos usan features? ¬øQu√© ventajas conlleva esto? (0.1 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ">Los modelos que usan features son MEMMs y CRFs. Esto tiene la ventaja de que pueden capturar relaciones de la entrada con las etiquetas correspondientes, lo que puede aportar informaci√≥n muy valiosa para determinar la secuencia de etiquetas. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2.3. ¬øC√≥mo maneja cada uno de los modelos las palabras con baja frecuencia en el set de train? (0.1 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ">HMM divide el conjunto en palabras en baja y alta frecuencia. Las palabras de baja frecuencia se agrupan en distintos subconjuntos, de modo que su probabilidad sea mayor en comparaci√≥n al conteo de la palabra sin agrupar. Un ejemplo de ello son los n√∫meros de 2 d√≠gitos, que se agrupan todos como si fuesen un mismo token.\n",
        ">\n",
        ">Como MEMM y CRF son modelos discriminativos, no tratan de forma directa el problema de palabras con baja frecuencia debido a que no necesitan estimar la probabilida de las palabras, sino que estiman una probabilidad condicional por medio de vectores de caracter√≠sticas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2.4. ¬øQu√© le permite a los CRF realizar decisiones globales? ¬øQu√© diferencia con respecto a los MEMMs permite lograr esto? ¬øPor qu√© los HMMs tampoco son capaces de tomar decisiones globales? (0.1 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ">Los CRF pueden tomar decisiones globales debido a la normalizaci√≥n que presentan. Dicha normalizaci√≥n es global, ya que considera todas las combinaciones de secuencias posibles, capturando mucha m√°s informaci√≥n. En el caso de MEMMs, su normalizaci√≥n es local, por lo que logra mirar solo un par de etiquetas atr√°s. Esto produce que MEMMs pierda mucho del contexto por su falta de memoria, pero que CRFs pueda capturarlo de mejor manera.\n",
        ">\n",
        ">En el caso de los HMMs tampoco tiene memoria global. Esto se debe a que a diferencia de CRF y MEMMs, no se relaciona el input con las etiquetas, lo cual no considera el contexto de la secuencia de entrada pues no tiene una funci√≥n $\\phi$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2.5 Dado una secuencia de $x_1, ..., x_m$ ¬øCu√°ntas posibles secuencias de etiquetas se pueden generar para un conjunto de etiquetas $S$ con $|S|=k$ ? ¬øAnalizarlas todas ser√≠a computacionalmente tratable? (0.1 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ">Un secuencia de tama√±o $m$ con $k$ opciones distintas para cada elemento de la secuencia, posee un total de $k^m$ posibles secuencias. Esto quiere decir que el n√∫mero posible de secuencias crece exponencialmente a medida que aumenta el n√∫mero de tokens. Es por lo anterior que se vuelve intratable intentar estimar por fuerza bruta cada una de las secuencias para la mayor√≠a de los problemas, motivo por el que se usa un modelo generativo como HMM, o modelos discriminativos como MEMMs o CRFs."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "44ACHHZIWGF1"
      },
      "source": [
        "# Convolutional Neural Networks (0,5 puntos)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ClRAHR95Y8aB"
      },
      "source": [
        "### Pregunta 3 (0,5 puntos)\n",
        "\n",
        "Considere la frase $w_{1..7}=$ `El agua moja y el fuego quema` $=[El, agua, moja, y, el, fuego, quema]$.\n",
        "\n",
        "La siguiente matriz de embeddings, donde la i-√©sima fila corresponde al vector de embedding de la i-√©sima palabra, ordenadas seg√∫n aparecen en la frase. (vectores de largo 2).\n",
        "\\begin{equation}\n",
        "E = \\begin{pmatrix}\n",
        "2 & 2\\\\\n",
        "0 & -2\\\\\n",
        "0 & 1\\\\\n",
        "-2 & 1\\\\\n",
        "1 & 0\\\\\n",
        "-1 & 1\\\\\n",
        "1 & 1\n",
        "\\end{pmatrix}\n",
        "\\end{equation}\n",
        "\n",
        "Los siguientes 3 filtros\n",
        "\\begin{equation}\n",
        "U = \\begin{pmatrix}\n",
        "-1 & 1 & 0\\\\\n",
        "1 & 1 & 0\\\\\n",
        "0 & 0 & -1\\\\\n",
        "1 & -1 & -1\\\\\n",
        "-1 & -1 & 1\\\\\n",
        "1 & 0 & -1\n",
        "\\end{pmatrix}\n",
        "\\end{equation}\n",
        "\n",
        "Y la funci√≥n de activaci√≥n\n",
        "\\begin{equation}\n",
        "tanh = \\frac{e^{2x} - 1}{e^{2x} + 1}\n",
        "\\end{equation}\n",
        "\n",
        "Usando estos param√°tros escriba los pasos para calcular la representaci√≥n (vector) resultante de aplicar la operaci√≥n de convoluci√≥n (sin padding) + max pooling. ¬øDe qu√© tama√±o ser√≠a la ventana que debemos usar?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SlQ30Arkq0u4"
      },
      "source": [
        ">Para que la convoluci√≥n sin padding tenga sentido y se obtenga un valor escalar, se requiere que cada vector de caracter√≠sticas tenga la misma dimensi√≥n de los filtros, en este caso, 6. Para ello la ventana debe ser de tama√±o 3. Con un stride de tama√±o 1, se obtiene una matriz de caracter√≠sticas de dimensiones 5x6.\n",
        ">\n",
        ">$$ F = \\begin{pmatrix}\n",
        ">2 & 2 & 0 & -2 & 0 & 1\\\\\n",
        ">0 & -2 & 0 & 1 & -2 & 1\\\\\n",
        ">0 & 1 & -2 & 1 & 1 & 0\\\\\n",
        ">-2 & 1 & 1 & 0 & -1 & 1\\\\\n",
        ">1 & 0 & -1 & 1 & 1 & 1\n",
        ">\\end{pmatrix} $$\n",
        ">\n",
        ">Luego, al realizar la convoluci√≥n con $U$ se obtiene una matriz de dimensiones 5x3, donde cada celda adquiere el siguiente valor:\n",
        ">\n",
        ">$$ \\begin{array}{rcl}\n",
        ">    C_{i,j} & = & \\sum_{k=1}^6 F_{i,k}\\cdot U_{6+1-k,j} \\\\\n",
        ">    C & = & \\begin{pmatrix}\n",
        ">        -1 & -1 & 2 \\\\\n",
        ">        -1 & 1 & -3 \\\\\n",
        ">        -2 & 2 & 2 \\\\\n",
        ">        -4 & -2 & 2 \\\\\n",
        ">        0 & 3 & -1\n",
        ">    \\end{pmatrix}\n",
        ">\\end{array} $$\n",
        ">\n",
        ">A continuaci√≥n, se aplica la funci√≥n de activaci√≥n sobre cada uno de los elementos de $C$. Dado que un vector $b$ no ha sido entregado se asume que es un vector de 0.\n",
        ">\n",
        ">$$ \\tanh(C) = \\begin{pmatrix}\n",
        ">    -0.761 & -0.761 & 0.964 \\\\\n",
        ">    -0.761 & 0.761 & -0.995 \\\\\n",
        ">    -0.964 & 0.964 & 0.964 \\\\\n",
        ">    -0.999 & -0.964 & 0.964 \\\\\n",
        ">    0 & 0.995 & -0.761\n",
        ">\\end{pmatrix} $$\n",
        ">\n",
        ">Y finalmente, se filtran los valores por medio de un max pooling, al retornar el valor m√°s grande por cada columna.\n",
        ">\n",
        ">$$ O = \\begin{pmatrix} 0 & 0.995 & 0.964 \\end{pmatrix} $$"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "A0rCwen3WREC"
      },
      "source": [
        "# Recurrent Neural Networks (1 punto)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "U0et78Z4oKIq"
      },
      "source": [
        "### Pregunta 4 (0,5 puntos)\n",
        "Usando los embeddings de dos dimensiones de la pregunta anteror, la oraci√≥n `el fuego quema` la podemos representar por una secuencia de vectores $(\\vec{x}_1,\\vec{x}_2,\\vec{x}_3)$, con $\\vec{x}_i \\in \\mathbb{R}^{d_x}$ y $d_x=2$.\n",
        "\n",
        "Tenemos una red recurrente *Elman* definidad como:\n",
        "\\begin{equation}\n",
        "\\begin{split}\n",
        "\\vec{s}_i &= R_{SRNN}\\left (\\vec{x}_i, \\vec{s}_{i-1}\\right ) = g \\left (\\vec{s}_{i-1}W^s + \\vec{x}_i W^x + \\vec{b}\\right ) \\\\\n",
        "\\vec{y}_i &= O_{SRNN}\\left(\\vec{s}_i\\right) = \\vec{s}_i \\\\\n",
        "\\end{split}\n",
        "\\end{equation}\n",
        "donde\n",
        "\\begin{equation}\n",
        "\\vec{s}_i, \\vec{y}_i \\in \\mathbb{R}^{d_s}, \\quad W^x \\in \\mathbb{R}^{d_x \\times d_s}, \\quad W^s \\in \\mathbb{R}^{d_s \\times d_s}, \\quad \\vec{b} \\in \\mathbb{R}^{d_s},\n",
        "\\end{equation}\n",
        "y los vectores de estado $s_i$ son de tres dimensiones, $ds= 3$.\n",
        "\n",
        "Sea\n",
        "\\begin{equation}\n",
        "\\begin{split}\n",
        "\\vec{s}_0 &= [0,0,0]\\\\\n",
        "W^x &= \\begin{pmatrix}\n",
        "0 &  0 & 1\\\\\n",
        "1 & -1 & 0\n",
        "\\end{pmatrix} \\\\\n",
        "W^s &= \\begin{pmatrix}\n",
        "1 & 0 &  1\\\\\n",
        "0 & 1 & -1\\\\\n",
        "1 & 1 &  1\n",
        "\\end{pmatrix} \\\\\n",
        "\\vec{b} &= [0, 0, 0] \\\\\n",
        "g(x) &= ReLu(x) = max(0, x)\n",
        "\\end{split}\n",
        "\\end{equation}\n",
        "\n",
        "<br>\n",
        "\n",
        "Calcule manualmente los valores de los vectores $\\vec{s}_1, \\vec{s}_2,\\vec{s}_3$ y de $\\vec{y}_1, \\vec{y}_2,\\vec{y}_3$."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fim2W8JioPhL"
      },
      "source": [
        ">Dado que $y_i = s_i$ s√≥lo se requiere calcular los estados $s_i$ para obtener tambi√©n las salidas de la RNN.\n",
        ">\n",
        ">C√°lculo $s_1$\n",
        ">$$ \\begin{array}{rcl}\n",
        ">s_1 & = & ReLu(s_0W^s + x_1W^x + b) \\\\\n",
        ">s_1 & = & ReLu(\\begin{pmatrix} 0 & 0 & 0 \\end{pmatrix} \\begin{pmatrix}\n",
        ">1 & 0 &  1\\\\\n",
        ">0 & 1 & -1\\\\\n",
        ">1 & 1 &  1\n",
        ">\\end{pmatrix} + \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix}\n",
        ">0 &  0 & 1\\\\\n",
        ">1 & -1 & 0\n",
        ">\\end{pmatrix} + \\begin{pmatrix} 0 & 0 & 0 \\end{pmatrix}) \\\\\n",
        ">s_1 & = & ReLu(\\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix}\n",
        ">0 &  0 & 1\\\\\n",
        ">1 & -1 & 0\n",
        ">\\end{pmatrix}) \\\\\n",
        ">s_1 & = & ReLu(\\begin{pmatrix} 0 & 0 & 1 \\end{pmatrix}) \\\\\n",
        ">s_1 & = & \\begin{pmatrix} 0 & 0 & 1 \\end{pmatrix} \\\\\n",
        ">y_1 & = & \\begin{pmatrix} 0 & 0 & 1 \\end{pmatrix}\n",
        ">\\end{array} $$\n",
        ">\n",
        ">\n",
        ">C√°lculo $s_2$\n",
        ">$$ \\begin{array}{rcl}\n",
        ">s_2 & = & ReLu(s_1W^s + x_2W^x + b) \\\\\n",
        ">s_2 & = & ReLu(\\begin{pmatrix} 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix}\n",
        ">1 & 0 &  1\\\\\n",
        ">0 & 1 & -1\\\\\n",
        ">1 & 1 &  1\n",
        ">\\end{pmatrix} + \\begin{pmatrix} -1 & 1 \\end{pmatrix} \\begin{pmatrix}\n",
        ">0 &  0 & 1\\\\\n",
        ">1 & -1 & 0\n",
        ">\\end{pmatrix} + \\begin{pmatrix} 0 & 0 & 0 \\end{pmatrix}) \\\\\n",
        ">s_2 & = & ReLu(\\begin{pmatrix} 1 & 1 & 1 \\end{pmatrix} + \\begin{pmatrix} 1 & -1 & -1 \\end{pmatrix}) \\\\\n",
        ">s_2 & = & ReLu(\\begin{pmatrix} 2 & 0 & 0 \\end{pmatrix}) \\\\\n",
        ">s_2 & = & \\begin{pmatrix} 2 & 0 & 0 \\end{pmatrix} \\\\\n",
        ">y_2 & = & \\begin{pmatrix} 2 & 0 & 0 \\end{pmatrix}\n",
        ">\\end{array} $$\n",
        ">\n",
        ">\n",
        ">C√°lculo $s_3$\n",
        ">$$ \\begin{array}{rcl}\n",
        ">s_3 & = & ReLu(s_2W^s + x_3W^x + b) \\\\\n",
        ">s_3 & = & ReLu(\\begin{pmatrix} 2 & 0 & 0 \\end{pmatrix} \\begin{pmatrix}\n",
        ">1 & 0 &  1\\\\\n",
        ">0 & 1 & -1\\\\\n",
        ">1 & 1 &  1\n",
        ">\\end{pmatrix} + \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\begin{pmatrix}\n",
        ">0 &  0 & 1\\\\\n",
        ">1 & -1 & 0\n",
        ">\\end{pmatrix} + \\begin{pmatrix} 0 & 0 & 0 \\end{pmatrix}) \\\\\n",
        ">s_3 & = & ReLu(\\begin{pmatrix} 2 & 0 & 2 \\end{pmatrix} + \\begin{pmatrix} 1 & -1 & 1 \\end{pmatrix}) \\\\\n",
        ">s_3 & = & ReLu(\\begin{pmatrix} 3 & -1 & 3 \\end{pmatrix}) \\\\\n",
        ">s_3 & = & \\begin{pmatrix} 3 & 0 & 3 \\end{pmatrix} \\\\\n",
        ">y_3 & = & \\begin{pmatrix} 3 & 0 & 3 \\end{pmatrix}\n",
        ">\\end{array} $$"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "W4rAT6ELxRZW"
      },
      "source": [
        "### Pregunta 5 (0.5 puntos)\n",
        "¬øDe qu√© forma las RNN y las CNN logran aprender representaciones espec√≠ficas\n",
        "para la tarea objetivo? Compare la forma en que las RNN y las CNN aprenden con los modelos que usan *features* dise√±adas manualmente."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "b6AXbQSgA_t8"
      },
      "source": [
        "**Respuesta**\n",
        "\n",
        "Tanto las CNN como las RNN son redes neuronales que aprenden gracias a la minimizaci√≥n de una funci√≥n de p√©rdidas, utilizando como heur√≠stica alguna variante del algoritmo de gradiente descendente. Cabe destacar que este tipo de configuraciones permite obtener informaci√≥n sobre todo el texto entregado, en redes CNN por medio de las operaciones de pooling y en redes RNN por medio de la recurrencia de la red. Por otra parte, los modelos manuales son generalemente modelos markovianos que asumen independencia entre tokens, de modo que no logran capturar toda la informaci√≥n contenida en el texto."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FRJkBpjWyHnb"
      },
      "source": [
        "# Pregunta 6: Redes Neuronales con Pytorch (3 puntos) üí¨\n",
        "\n",
        "<center>\n",
        "<img src=\"https://www.anda.cl/wp-content/uploads/2021/03/0_5vNAtimPjYQr4W72.gif\" alt=\"chatbot\" width=\"400\">\n",
        "</center>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GEla92bUymrQ"
      },
      "source": [
        "En esta secci√≥n de la tarea deber√°n implementar un Chatbot que sea capaz de generar una conversaci√≥n *‚Äúb√°sica‚Äù* utilizando un dataset de *Star Wars*. **El objetivo** de esta pregunta es que puedan aplicar lo aprendido sobre redes neuronales utilizando Pytorch en un ejemplo pr√°ctico.  Durante el desarrollo, se espera que puedan dise√±ar un bot (que tendr√° por atr√°s un clasificador) que sea capaz de clasificar diferentes etiquetas, cosa que una vez identificada la etiqueta entregue una respuesta acorde a lo preguntado.\n",
        "\n",
        "**Aviso:** Antes de comenzar con una descripci√≥n mas profunda de esta secci√≥n, les recomendamos que visualicen y se familiaricen con el dataset entregado, de esta forma comprender√°n mejor la descripci√≥n del enunciado (aqu√≠ una peque√±a ayudita üÜò)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eKOGlMs3Dx-",
        "outputId": "7baf9d9a-fde4-4f3a-b831-02a8620c1e0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cantidad de tags:  16\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "star_wars_chatbot_path = \"../../data/star_wars_chatbot.json\"\n",
        "example_data = pd.read_json(star_wars_chatbot_path)\n",
        "print(\"Cantidad de tags: \", example_data['intents'].shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVNT1NUlXQs7",
        "outputId": "95a1cf34-006b-44e9-952a-f6c0f53c3691"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keys de cada fila: dict_keys(['tag', 'patterns', 'responses'])\n",
            "Ejemplo de tag: greeting\n",
            "Ejemplo de patterns: ['Hi', 'Hey', 'How are you', 'Is anyone there?', 'Hello', 'Good day', \"What's up\", 'Yo!', 'Howdy', 'Nice to meet you.']\n",
            "Ejemplo de responses: ['Hey', 'Hello, thanks for visiting.', 'Hi there, what can I do for you?', 'Hi there, how can I help?', 'Hello, there.', 'Hello Dear', 'Ooooo Hello, looking for someone or something?', 'Yes, I am here.', 'Listening carefully.', 'Ok, I am with you.']\n"
          ]
        }
      ],
      "source": [
        "print( \"Keys de cada fila:\", example_data.head(10).iloc[0]['intents'].keys() )\n",
        "print( \"Ejemplo de tag:\", example_data.head(10).iloc[0]['intents']['tag'] )\n",
        "print( \"Ejemplo de patterns:\", example_data.head(10).iloc[0]['intents']['patterns'] )\n",
        "print( \"Ejemplo de responses:\", example_data.head(10).iloc[0]['intents']['responses'] )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "V-6fCE5fHkNS"
      },
      "source": [
        "A continuaci√≥n, ejemplos del contenido del primer registro:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axsi27BpHGOx",
        "outputId": "265068dc-f5d0-42ca-a65d-14eda8ff24ca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Hi',\n",
              " 'Hey',\n",
              " 'How are you',\n",
              " 'Is anyone there?',\n",
              " 'Hello',\n",
              " 'Good day',\n",
              " \"What's up\",\n",
              " 'Yo!',\n",
              " 'Howdy',\n",
              " 'Nice to meet you.']"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example_data['intents'][0]['patterns']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OV0vGdwoHeg3",
        "outputId": "884f6ecd-5e87-4f0d-d477-5aad81488485"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Hey',\n",
              " 'Hello, thanks for visiting.',\n",
              " 'Hi there, what can I do for you?',\n",
              " 'Hi there, how can I help?',\n",
              " 'Hello, there.',\n",
              " 'Hello Dear',\n",
              " 'Ooooo Hello, looking for someone or something?',\n",
              " 'Yes, I am here.',\n",
              " 'Listening carefully.',\n",
              " 'Ok, I am with you.']"
            ]
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example_data['intents'][0]['responses']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0BnYez1oGtx3",
        "outputId": "9f89ae14-5e34-4366-995d-351f4b3954ae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'greeting'"
            ]
          },
          "execution_count": 95,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example_data['intents'][0]['tag']"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "v6BvAWCw3zPM"
      },
      "source": [
        "Del dataset cargado podemos notar que este viene en un formato `JSON`, por lo que sus datos est√°n almacenados en diccionarios. Las llaves de los diccionarios no son aleatorias y estos nos sirven para identificar puntos relevantes en el desarrollo del bot. A continuaci√≥n, se realiza una peque√±a descripci√≥n de las llaves:\n",
        "\n",
        "- `patterns`: Almacena los patrones con los que entrenaremos el modelo üòÆ, en otras palabras, es el corpus de entrenamiento que contiene solo preguntas o expresiones que deber√° responder el bot.\n",
        "- `responses`: Son las respuestas üôã relacionadas a los `patterns`, estas las utilizaremos en una etapa posterior a la clasificaci√≥n, para dar una respuesta aleator√≠a al usuario.\n",
        "- `tag`: Son las labels con las que entrenaremos nuestro modelo üíª.\n",
        "\n",
        "En s√≠ntesis, las `keys` relevantes para el entrenamiento de nuestra red neuronal ser√°n `patterns` (corpus) y `tag` (etiquetas)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KlOAdMjSSzNN"
      },
      "source": [
        "#### Explicaci√≥n de la tarea a realizar:"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9yGApnWVI4cO"
      },
      "source": [
        "**Explicaci√≥n de la tarea a realizar**: Implemente una Class llamada `CNNClassifier` que sea capaz de entrenar un modelo de texto a trav√©s de una red neuronal Feed Forward y una arquitectura convolucional (CNN 1D) [`torch.nn.Conv1d`](https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html#conv1d) . Para el dise√±o de las redes tienen completa libertad, pero se le aconseja que se gu√≠en de la √∫ltima auxiliar para la construcci√≥n. Es **important√≠simo** que el modelo a crear posea una capa de `Embedding` que se genere en base al entrenamiento del modelo. Creado el modelo, construya una funci√≥n batch para cargar los datos de entrenamiento del modelo.\n",
        "\n",
        "Construido el modelo, compare los resultados obtenidos para una red feed forward y una cnn. Para la comprobaci√≥n de sus resultados ejecute el chatbot y pruebelo, ¬øqu√© configuraci√≥n tiene mejores resultados?, ¬øa qu√© se deberan estos resultados?\n",
        "\n",
        "Ojo que un ejemplo de prueba con el chatbot puede ser (agregue mas preguntas ud):\n",
        "\n",
        "```\n",
        "Let's chat! (type 'finish_chat' to finish the chat)\n",
        "You: hi\n",
        "GA-97: Yes, I am here.\n",
        "You: can you tell me a joke?\n",
        "GA-97: Have you tried the gluten-free Wookiee treats? No, but I heard they are a little Chewy.\n",
        "```\n",
        "\n",
        "El resto del c√≥digo referido a la ejecuci√≥n del chat se los entregamos, por lo que no deber√≠an tener mayores problemas üò∏ (en caso de tener problemas con su c√≥digo, puede modificar cualquier parte sugerida siempre y cuando cumpla lo solicitado).\n",
        "\n",
        "**Igual [mucho texto](https://i0.wp.com/elgeneracionalpost.com/wp-content/uploads/2020/07/mucho-texto.jpg?fit=1280%2C720&ssl=1).... En resumen, ¬øQu√© se solicita?:**\n",
        "\n",
        "- [ ] Dise√±ar una red neuronal Feed Forward.\n",
        "- [ ] Dise√±ar un red convolucional.\n",
        "- [ ] Utilizar una capa de embeddings para generar representaciones vectoriales del corpus.\n",
        "- [ ] Crear el m√©todo forward de la clase `CNNClassifier`.\n",
        "- [ ] Crear la funci√≥n BATCH.\n",
        "- [ ] Probar el modelo y comparar los resultados obtenidos con la red Feed Forward y la red CNN. Comente sus resultados de forma cualitativa, se√±alando con qu√© tipo de red obtuvo mejores resultados con el chatbot.\n",
        "\n",
        "**Nota-1:** El modelo creado debe tener la opci√≥n de entrenar a traves de una feed forward y una CNN. Esto no significa que entrenar√° una FF y una CN, el modelo deber√° recibir un booleano que especifique que tipo de red utilizar√°.\n",
        "\n",
        "**Nota-2:** El dataset se descargar√° autom√°ticamente en la secci√≥n `Carga de Dataset üìö`, no os preocup√©is."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "a4bKfAdEy3oD"
      },
      "source": [
        "#### Pasemos al C√≥digo ü¶æ\n",
        "\n",
        "Esqueleto propuesto (se **RECOMIENDA** que cambien **SOLO** la red neuronal y la funci√≥n Batch) ü¶¥:"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RUwxivx2MpMV"
      },
      "source": [
        "##### Instalamos librerias necesarias e importamos üòÄ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "RfZ6SL-Q1Kwd"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import random\n",
        "from itertools import zip_longest\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torch.optim import SGD, lr_scheduler"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oj-Epe7XJLrL"
      },
      "source": [
        "##### Carga de Dataset üìö"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "MbbIsFUG1TXW"
      },
      "outputs": [],
      "source": [
        "# Load the dataset using json\n",
        "with open(star_wars_chatbot_path, 'r') as f:\n",
        "    dataset = json.load(f)\n",
        "\n",
        "# Create a vocab with the dataset and get the number of classes that have\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "vocab = build_vocab_from_iterator(tokenizer(x) for list_words in dataset['intents'] for x in list_words['patterns'])\n",
        "num_classes = len(dataset['intents'])\n",
        "\n",
        "# Define a list with the labels\n",
        "labels = sorted(set([tag for tag in [intents['tag'] for intents in dataset['intents']]]))\n",
        "# Define a train_list where we can find the info in the format: [(tag_0, text_0)...,(tag_n-1, text_n-1)]\n",
        "train_list = [(labels.index(intents['tag']), text) for intents in dataset['intents'] for text in intents['patterns']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se a√±ade un token para tokens desconocidos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "bnn7d0r6iw8-"
      },
      "outputs": [],
      "source": [
        "UNK_IDX = 0\n",
        "vocab.insert_token('<pad>', 0)\n",
        "vocab.set_default_index(UNK_IDX)\n",
        "\n",
        "stoi = vocab.get_stoi()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "a52SUNKPJQxi"
      },
      "source": [
        "##### Creaci√≥n del modelo (2 puntos en total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "n-vQ24tMJG5H"
      },
      "outputs": [],
      "source": [
        "# Construya el modelo\n",
        "class CNNClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=32, num_classes=10,\n",
        "                 use_cnn=False, cnn_pool_channels=24, cnn_kernel_size=3):\n",
        "        super().__init__()\n",
        "\n",
        "        self.cnn_kernel_size = cnn_kernel_size\n",
        "        self.use_cnn = use_cnn\n",
        "\n",
        "        # Capa de embedding\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "        if use_cnn:\n",
        "            # Capa de convoluci√≥n\n",
        "            self.conv = nn.Conv1d(\n",
        "                in_channels = 1, # un canal de entrada\n",
        "                out_channels = cnn_pool_channels,\n",
        "                kernel_size = cnn_kernel_size * embed_dim,\n",
        "                stride = embed_dim\n",
        "            )\n",
        "\n",
        "            # Tama√±o de entrada de capa lineal\n",
        "            fc_in_size = cnn_pool_channels\n",
        "\n",
        "            # Creaci√≥n capa lineal\n",
        "            self.fc = nn.Linear(fc_in_size, num_classes)\n",
        "\n",
        "            # Inicializacion de pesos de las capas\n",
        "            self.init_weights()\n",
        "\n",
        "        else:\n",
        "            # capas de la MLP\n",
        "            self.fc = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "\n",
        "    def init_weights(self):\n",
        "        # Valor inicial de los pesos\n",
        "        initrange = 0.5\n",
        "\n",
        "        # Inicializaci√≥n de pesos en capa de embedding\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "        # Inicializaci√≥n de pesos en la capa lineal\n",
        "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "        # Inicilizaci√≥n de bias de la capa lineal en cero\n",
        "        self.fc.bias.data.zero_()\n",
        "\n",
        "    def forward(self, text, offsets):\n",
        "        # Preparamos el input de la capa de embeddings a partir de text y offsets\n",
        "        text = torch.tensor(\n",
        "            list(\n",
        "                zip(\n",
        "                    *zip_longest(\n",
        "                        *([text[o:offsets[i+1]] for i, o in enumerate(offsets[:-1])] + [text[offsets[-1]:len(text)]]),\n",
        "                        fillvalue=vocab[\"<pad>\"]\n",
        "                    )\n",
        "                )\n",
        "            )\n",
        "        ).to(text.device)\n",
        "        if text.shape[1] < self.cnn_kernel_size:\n",
        "            zeros = torch.zeros((text.shape[0], self.cnn_kernel_size - text.shape[1]), dtype=torch.long).to(text.device)\n",
        "            text = torch.cat((text, zeros), dim=1).to(text.device)\n",
        "\n",
        "        # Obtenemos la representaci√≥n de la frase a partir de la capa de embedding\n",
        "        h = self.embedding(text)\n",
        "\n",
        "        if self.use_cnn:\n",
        "            # Aplicamos la capa de convoluci√≥n\n",
        "            h = h.view(h.size(0), 1, -1)\n",
        "            h = torch.relu(self.conv(h))\n",
        "            h = h.mean(dim=2)\n",
        "\n",
        "            # Obtenemos el resultado final a partir de la capa lineal\n",
        "            output = self.fc(h)\n",
        "\n",
        "        else:\n",
        "            # La representacion de un documento sera el promedio de los\n",
        "            # embeddings de sus palabras.\n",
        "            h = h.mean(dim=1)\n",
        "\n",
        "            # computar las capas de la red MLP\n",
        "            output = self.fc(h)\n",
        "        \n",
        "        # Aplicamos la funci√≥n de activaci√≥n log-softmax\n",
        "        return F.log_softmax(output, dim=1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dGN-T0JoJtmS"
      },
      "source": [
        "##### Funci√≥n Batch üë∑ (0,5 puntos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "K1AZpXc7JxTa"
      },
      "outputs": [],
      "source": [
        "# Defina su funci√≥n de BATCH\n",
        "def generate_batch(batch):#, use_cnn):\n",
        "    # if use_cnn:\n",
        "    label = torch.tensor([ entry[0] for entry in batch ])\n",
        "    texts = [ tokenizer(entry[1]) for entry in batch ]\n",
        "    offsets = [0] + [len(text) for text in texts]\n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "    big_text = torch.cat([torch.tensor([vocab[t] if t in stoi else 0 for t in text]) for text in texts])\n",
        "\n",
        "    return big_text, offsets, label"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YChwpNrrNRBe"
      },
      "source": [
        "##### Entrenamiento ü•ä"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5eRWRD_J0Km",
        "outputId": "deea606a-5cd0-4f2b-b137-d13e79cb9c4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU is avaible: cuda\n",
            "train: 97 elements\n",
            "Epoch: 002 \t iter-Loss: 2.799\n",
            "Epoch: 003 \t iter-Loss: 3.125\n",
            "Epoch: 004 \t iter-Loss: 2.853\n",
            "Epoch: 005 \t iter-Loss: 2.792\n",
            "Epoch: 006 \t iter-Loss: 2.637\n",
            "Epoch: 007 \t iter-Loss: 2.396\n",
            "Epoch: 008 \t iter-Loss: 2.904\n",
            "Epoch: 009 \t iter-Loss: 2.601\n",
            "Epoch: 010 \t iter-Loss: 2.941\n",
            "Epoch: 011 \t iter-Loss: 2.331\n",
            "Epoch: 012 \t iter-Loss: 2.999\n",
            "Epoch: 013 \t iter-Loss: 2.078\n",
            "Epoch: 014 \t iter-Loss: 2.156\n",
            "Epoch: 015 \t iter-Loss: 3.103\n",
            "Epoch: 016 \t iter-Loss: 2.518\n",
            "Epoch: 017 \t iter-Loss: 2.053\n",
            "Epoch: 018 \t iter-Loss: 3.181\n",
            "Epoch: 019 \t iter-Loss: 1.550\n",
            "Epoch: 020 \t iter-Loss: 3.078\n",
            "Epoch: 021 \t iter-Loss: 2.376\n",
            "Epoch: 022 \t iter-Loss: 1.802\n",
            "Epoch: 023 \t iter-Loss: 3.094\n",
            "Epoch: 024 \t iter-Loss: 2.913\n",
            "Epoch: 025 \t iter-Loss: 1.412\n",
            "Epoch: 026 \t iter-Loss: 1.995\n",
            "Epoch: 027 \t iter-Loss: 1.876\n",
            "Epoch: 028 \t iter-Loss: 2.909\n",
            "Epoch: 029 \t iter-Loss: 2.989\n",
            "Epoch: 030 \t iter-Loss: 3.167\n",
            "Epoch: 031 \t iter-Loss: 1.058\n",
            "Epoch: 032 \t iter-Loss: 3.596\n",
            "Epoch: 033 \t iter-Loss: 2.371\n",
            "Epoch: 034 \t iter-Loss: 1.964\n",
            "Epoch: 035 \t iter-Loss: 2.390\n",
            "Epoch: 036 \t iter-Loss: 1.822\n",
            "Epoch: 037 \t iter-Loss: 3.488\n",
            "Epoch: 038 \t iter-Loss: 2.763\n",
            "Epoch: 039 \t iter-Loss: 1.426\n",
            "Epoch: 040 \t iter-Loss: 2.290\n",
            "Epoch: 041 \t iter-Loss: 2.273\n",
            "Epoch: 042 \t iter-Loss: 2.703\n",
            "Epoch: 043 \t iter-Loss: 1.396\n",
            "Epoch: 044 \t iter-Loss: 2.568\n",
            "Epoch: 045 \t iter-Loss: 3.084\n",
            "Epoch: 046 \t iter-Loss: 0.662\n",
            "Epoch: 047 \t iter-Loss: 3.360\n",
            "Epoch: 048 \t iter-Loss: 1.538\n",
            "Epoch: 049 \t iter-Loss: 2.752\n",
            "Epoch: 050 \t iter-Loss: 2.613\n",
            "Epoch: 051 \t iter-Loss: 2.745\n",
            "Epoch: 052 \t iter-Loss: 1.791\n",
            "Epoch: 053 \t iter-Loss: 2.344\n",
            "Epoch: 054 \t iter-Loss: 2.184\n",
            "Epoch: 055 \t iter-Loss: 2.420\n",
            "Epoch: 056 \t iter-Loss: 1.750\n",
            "Epoch: 057 \t iter-Loss: 2.523\n",
            "Epoch: 058 \t iter-Loss: 1.729\n",
            "Epoch: 059 \t iter-Loss: 3.491\n",
            "Epoch: 060 \t iter-Loss: 2.019\n",
            "Epoch: 061 \t iter-Loss: 1.559\n",
            "Epoch: 062 \t iter-Loss: 1.488\n",
            "Epoch: 063 \t iter-Loss: 1.713\n",
            "Epoch: 064 \t iter-Loss: 1.460\n",
            "Epoch: 065 \t iter-Loss: 1.128\n",
            "Epoch: 066 \t iter-Loss: 0.450\n",
            "Epoch: 067 \t iter-Loss: 1.090\n",
            "Epoch: 068 \t iter-Loss: 7.647\n",
            "Epoch: 069 \t iter-Loss: 0.979\n",
            "Epoch: 070 \t iter-Loss: 0.847\n",
            "Epoch: 071 \t iter-Loss: 3.294\n",
            "Epoch: 072 \t iter-Loss: 3.262\n",
            "Epoch: 073 \t iter-Loss: 0.502\n",
            "Epoch: 074 \t iter-Loss: 1.016\n",
            "Epoch: 075 \t iter-Loss: 1.384\n",
            "Epoch: 076 \t iter-Loss: 0.115\n",
            "Epoch: 077 \t iter-Loss: 1.596\n",
            "Epoch: 078 \t iter-Loss: 2.443\n",
            "Epoch: 079 \t iter-Loss: 3.122\n",
            "Epoch: 080 \t iter-Loss: 0.990\n",
            "Epoch: 081 \t iter-Loss: 1.362\n",
            "Epoch: 082 \t iter-Loss: 1.587\n",
            "Epoch: 083 \t iter-Loss: 0.539\n",
            "Epoch: 084 \t iter-Loss: 0.330\n",
            "Epoch: 085 \t iter-Loss: 2.315\n",
            "Epoch: 086 \t iter-Loss: 0.037\n",
            "Epoch: 087 \t iter-Loss: 3.142\n",
            "Epoch: 088 \t iter-Loss: 1.994\n",
            "Epoch: 089 \t iter-Loss: 0.001\n",
            "Epoch: 090 \t iter-Loss: 0.048\n",
            "Epoch: 091 \t iter-Loss: 0.690\n",
            "Epoch: 092 \t iter-Loss: 0.290\n",
            "Epoch: 093 \t iter-Loss: 0.414\n",
            "Epoch: 094 \t iter-Loss: 0.066\n",
            "Epoch: 095 \t iter-Loss: 2.971\n",
            "Epoch: 096 \t iter-Loss: 1.105\n",
            "Epoch: 097 \t iter-Loss: 0.756\n",
            "Epoch: 098 \t iter-Loss: 0.685\n",
            "Epoch: 099 \t iter-Loss: 2.408\n",
            "Epoch: 100 \t iter-Loss: 0.043\n",
            "Epoch: 101 \t iter-Loss: 0.111\n",
            "Epoch: 102 \t iter-Loss: 0.498\n",
            "Epoch: 103 \t iter-Loss: 0.011\n",
            "Epoch: 104 \t iter-Loss: 1.953\n",
            "Epoch: 105 \t iter-Loss: 0.815\n",
            "Epoch: 106 \t iter-Loss: 4.718\n",
            "Epoch: 107 \t iter-Loss: 1.990\n",
            "Epoch: 108 \t iter-Loss: 0.360\n",
            "Epoch: 109 \t iter-Loss: 0.910\n",
            "Epoch: 110 \t iter-Loss: 0.175\n",
            "Epoch: 111 \t iter-Loss: 0.108\n",
            "Epoch: 112 \t iter-Loss: 1.357\n",
            "Epoch: 113 \t iter-Loss: 1.577\n",
            "Epoch: 114 \t iter-Loss: 0.592\n",
            "Epoch: 115 \t iter-Loss: 0.086\n",
            "Epoch: 116 \t iter-Loss: 0.451\n",
            "Epoch: 117 \t iter-Loss: 0.010\n",
            "Epoch: 118 \t iter-Loss: 0.002\n",
            "Epoch: 119 \t iter-Loss: 0.075\n",
            "Epoch: 120 \t iter-Loss: 0.229\n",
            "Epoch: 121 \t iter-Loss: 0.243\n",
            "Epoch: 122 \t iter-Loss: 1.020\n",
            "Epoch: 123 \t iter-Loss: 1.737\n",
            "Epoch: 124 \t iter-Loss: 0.169\n",
            "Epoch: 125 \t iter-Loss: 0.038\n",
            "Epoch: 126 \t iter-Loss: 0.076\n",
            "Epoch: 127 \t iter-Loss: 0.629\n",
            "Epoch: 128 \t iter-Loss: 1.375\n",
            "Epoch: 129 \t iter-Loss: 0.016\n",
            "Epoch: 130 \t iter-Loss: 0.084\n",
            "Epoch: 131 \t iter-Loss: 0.079\n",
            "Epoch: 132 \t iter-Loss: 0.799\n",
            "Epoch: 133 \t iter-Loss: 0.199\n",
            "Epoch: 134 \t iter-Loss: 0.732\n",
            "Epoch: 135 \t iter-Loss: 0.212\n",
            "Epoch: 136 \t iter-Loss: 2.245\n",
            "Epoch: 137 \t iter-Loss: 0.519\n",
            "Epoch: 138 \t iter-Loss: 0.187\n",
            "Epoch: 139 \t iter-Loss: 0.013\n",
            "Epoch: 140 \t iter-Loss: 0.101\n",
            "Epoch: 141 \t iter-Loss: 0.057\n",
            "Epoch: 142 \t iter-Loss: 0.226\n",
            "Epoch: 143 \t iter-Loss: 2.616\n",
            "Epoch: 144 \t iter-Loss: 0.433\n",
            "Epoch: 145 \t iter-Loss: 0.004\n",
            "Epoch: 146 \t iter-Loss: 0.100\n",
            "Epoch: 147 \t iter-Loss: 0.228\n",
            "Epoch: 148 \t iter-Loss: 0.053\n",
            "Epoch: 149 \t iter-Loss: 0.008\n",
            "Epoch: 150 \t iter-Loss: 0.310\n",
            "Epoch: 151 \t iter-Loss: 0.668\n",
            "Epoch: 152 \t iter-Loss: 1.203\n",
            "Epoch: 153 \t iter-Loss: 0.103\n",
            "Epoch: 154 \t iter-Loss: 0.298\n",
            "Epoch: 155 \t iter-Loss: 2.109\n",
            "Epoch: 156 \t iter-Loss: 0.017\n",
            "Epoch: 157 \t iter-Loss: 0.010\n",
            "Epoch: 158 \t iter-Loss: 4.868\n",
            "Epoch: 159 \t iter-Loss: 0.007\n",
            "Epoch: 160 \t iter-Loss: 0.031\n",
            "Epoch: 161 \t iter-Loss: 0.001\n",
            "Epoch: 162 \t iter-Loss: 0.056\n",
            "Epoch: 163 \t iter-Loss: 0.001\n",
            "Epoch: 164 \t iter-Loss: 0.579\n",
            "Epoch: 165 \t iter-Loss: 0.036\n",
            "Epoch: 166 \t iter-Loss: 0.262\n",
            "Epoch: 167 \t iter-Loss: 0.001\n",
            "Epoch: 168 \t iter-Loss: 0.133\n",
            "Epoch: 169 \t iter-Loss: 0.016\n",
            "Epoch: 170 \t iter-Loss: 0.078\n",
            "Epoch: 171 \t iter-Loss: 0.030\n",
            "Epoch: 172 \t iter-Loss: 0.011\n",
            "Epoch: 173 \t iter-Loss: 0.001\n",
            "Epoch: 174 \t iter-Loss: 2.013\n",
            "Epoch: 175 \t iter-Loss: 0.027\n",
            "Epoch: 176 \t iter-Loss: 0.010\n",
            "Epoch: 177 \t iter-Loss: 0.001\n",
            "Epoch: 178 \t iter-Loss: 0.001\n",
            "Epoch: 179 \t iter-Loss: 0.045\n",
            "Epoch: 180 \t iter-Loss: 0.001\n",
            "Epoch: 181 \t iter-Loss: 0.197\n",
            "Epoch: 182 \t iter-Loss: 0.001\n",
            "Epoch: 183 \t iter-Loss: 0.044\n",
            "Epoch: 184 \t iter-Loss: 0.031\n",
            "Epoch: 185 \t iter-Loss: 0.000\n",
            "Epoch: 186 \t iter-Loss: 0.034\n",
            "Epoch: 187 \t iter-Loss: 0.031\n",
            "Epoch: 188 \t iter-Loss: 0.047\n",
            "Epoch: 189 \t iter-Loss: 0.340\n",
            "Epoch: 190 \t iter-Loss: 0.140\n",
            "Epoch: 191 \t iter-Loss: 0.325\n",
            "Epoch: 192 \t iter-Loss: 0.013\n",
            "Epoch: 193 \t iter-Loss: 0.035\n",
            "Epoch: 194 \t iter-Loss: 0.032\n",
            "Epoch: 195 \t iter-Loss: 0.001\n",
            "Epoch: 196 \t iter-Loss: 0.020\n",
            "Epoch: 197 \t iter-Loss: 0.024\n",
            "Epoch: 198 \t iter-Loss: 0.430\n",
            "Epoch: 199 \t iter-Loss: 0.001\n",
            "Epoch: 200 \t iter-Loss: 0.014\n",
            "Epoch: 201 \t iter-Loss: 0.000\n",
            "Epoch: 202 \t iter-Loss: 0.212\n",
            "Epoch: 203 \t iter-Loss: 0.063\n",
            "Epoch: 204 \t iter-Loss: 0.001\n",
            "Epoch: 205 \t iter-Loss: 0.009\n",
            "Epoch: 206 \t iter-Loss: 0.019\n",
            "Epoch: 207 \t iter-Loss: 0.050\n",
            "Epoch: 208 \t iter-Loss: 0.059\n",
            "Epoch: 209 \t iter-Loss: 0.023\n",
            "Epoch: 210 \t iter-Loss: 0.025\n",
            "Epoch: 211 \t iter-Loss: 0.008\n",
            "Epoch: 212 \t iter-Loss: 0.002\n",
            "Epoch: 213 \t iter-Loss: 0.002\n",
            "Epoch: 214 \t iter-Loss: 0.078\n",
            "Epoch: 215 \t iter-Loss: 0.032\n",
            "Epoch: 216 \t iter-Loss: 0.007\n",
            "Epoch: 217 \t iter-Loss: 0.096\n",
            "Epoch: 218 \t iter-Loss: 0.053\n",
            "Epoch: 219 \t iter-Loss: 0.000\n",
            "Epoch: 220 \t iter-Loss: 0.000\n",
            "Epoch: 221 \t iter-Loss: 0.000\n",
            "Epoch: 222 \t iter-Loss: 0.001\n",
            "Epoch: 223 \t iter-Loss: 0.568\n",
            "Epoch: 224 \t iter-Loss: 0.015\n",
            "Epoch: 225 \t iter-Loss: 0.023\n",
            "Epoch: 226 \t iter-Loss: 0.450\n",
            "Epoch: 227 \t iter-Loss: 0.010\n",
            "Epoch: 228 \t iter-Loss: 0.000\n",
            "Epoch: 229 \t iter-Loss: 0.002\n",
            "Epoch: 230 \t iter-Loss: 0.000\n",
            "Epoch: 231 \t iter-Loss: 0.049\n",
            "Epoch: 232 \t iter-Loss: 0.011\n",
            "Epoch: 233 \t iter-Loss: 0.102\n",
            "Epoch: 234 \t iter-Loss: 0.004\n",
            "Epoch: 235 \t iter-Loss: 0.081\n",
            "Epoch: 236 \t iter-Loss: 0.001\n",
            "Epoch: 237 \t iter-Loss: 0.002\n",
            "Epoch: 238 \t iter-Loss: 0.001\n",
            "Epoch: 239 \t iter-Loss: 0.002\n",
            "Epoch: 240 \t iter-Loss: 0.031\n",
            "Epoch: 241 \t iter-Loss: 0.003\n",
            "Epoch: 242 \t iter-Loss: 0.000\n",
            "Epoch: 243 \t iter-Loss: 0.006\n",
            "Epoch: 244 \t iter-Loss: 0.042\n",
            "Epoch: 245 \t iter-Loss: 0.006\n",
            "Epoch: 246 \t iter-Loss: 0.005\n",
            "Epoch: 247 \t iter-Loss: 0.000\n",
            "Epoch: 248 \t iter-Loss: 0.007\n",
            "Epoch: 249 \t iter-Loss: 0.010\n",
            "Epoch: 250 \t iter-Loss: 0.006\n",
            "Epoch: 251 \t iter-Loss: 0.018\n",
            "Epoch: 252 \t iter-Loss: 0.000\n",
            "Epoch: 253 \t iter-Loss: 0.004\n",
            "Epoch: 254 \t iter-Loss: 0.183\n",
            "Epoch: 255 \t iter-Loss: 0.000\n",
            "Epoch: 256 \t iter-Loss: 0.015\n",
            "Epoch: 257 \t iter-Loss: 0.014\n",
            "Epoch: 258 \t iter-Loss: 0.008\n",
            "Epoch: 259 \t iter-Loss: 0.002\n",
            "Epoch: 260 \t iter-Loss: 0.015\n",
            "Epoch: 261 \t iter-Loss: 0.000\n",
            "Epoch: 262 \t iter-Loss: 0.000\n",
            "Epoch: 263 \t iter-Loss: 0.017\n",
            "Epoch: 264 \t iter-Loss: 0.001\n",
            "Epoch: 265 \t iter-Loss: 0.362\n",
            "Epoch: 266 \t iter-Loss: 0.005\n",
            "Epoch: 267 \t iter-Loss: 0.005\n",
            "Epoch: 268 \t iter-Loss: 0.000\n",
            "Epoch: 269 \t iter-Loss: 0.036\n",
            "Epoch: 270 \t iter-Loss: 0.002\n",
            "Epoch: 271 \t iter-Loss: 0.001\n",
            "Epoch: 272 \t iter-Loss: 0.002\n",
            "Epoch: 273 \t iter-Loss: 0.004\n",
            "Epoch: 274 \t iter-Loss: 0.001\n",
            "Epoch: 275 \t iter-Loss: 0.000\n",
            "Epoch: 276 \t iter-Loss: 0.001\n",
            "Epoch: 277 \t iter-Loss: 0.001\n",
            "Epoch: 278 \t iter-Loss: 0.000\n",
            "Epoch: 279 \t iter-Loss: 0.002\n",
            "Epoch: 280 \t iter-Loss: 0.001\n",
            "Epoch: 281 \t iter-Loss: 0.009\n",
            "Epoch: 282 \t iter-Loss: 0.315\n",
            "Epoch: 283 \t iter-Loss: 0.006\n",
            "Epoch: 284 \t iter-Loss: 0.014\n",
            "Epoch: 285 \t iter-Loss: 0.000\n",
            "Epoch: 286 \t iter-Loss: 0.007\n",
            "Epoch: 287 \t iter-Loss: 0.002\n",
            "Epoch: 288 \t iter-Loss: 0.028\n",
            "Epoch: 289 \t iter-Loss: 0.001\n",
            "Epoch: 290 \t iter-Loss: 0.006\n",
            "Epoch: 291 \t iter-Loss: 0.001\n",
            "Epoch: 292 \t iter-Loss: 0.003\n",
            "Epoch: 293 \t iter-Loss: 0.000\n",
            "Epoch: 294 \t iter-Loss: 0.031\n",
            "Epoch: 295 \t iter-Loss: 0.006\n",
            "Epoch: 296 \t iter-Loss: 0.000\n",
            "Epoch: 297 \t iter-Loss: 0.015\n",
            "Epoch: 298 \t iter-Loss: 0.003\n",
            "Epoch: 299 \t iter-Loss: 0.000\n",
            "Epoch: 300 \t iter-Loss: 0.000\n",
            "Epoch: 301 \t iter-Loss: 0.004\n",
            "Epoch: 302 \t iter-Loss: 0.001\n",
            "Epoch: 303 \t iter-Loss: 0.010\n",
            "Epoch: 304 \t iter-Loss: 0.003\n",
            "Epoch: 305 \t iter-Loss: 0.001\n",
            "Epoch: 306 \t iter-Loss: 0.000\n",
            "Epoch: 307 \t iter-Loss: 0.003\n",
            "Epoch: 308 \t iter-Loss: 0.001\n",
            "Epoch: 309 \t iter-Loss: 0.024\n",
            "Epoch: 310 \t iter-Loss: 0.000\n",
            "Epoch: 311 \t iter-Loss: 0.000\n",
            "Epoch: 312 \t iter-Loss: 0.001\n",
            "Epoch: 313 \t iter-Loss: 0.007\n",
            "Epoch: 314 \t iter-Loss: 0.000\n",
            "Epoch: 315 \t iter-Loss: 0.000\n",
            "Epoch: 316 \t iter-Loss: 0.004\n",
            "Epoch: 317 \t iter-Loss: 0.001\n",
            "Epoch: 318 \t iter-Loss: 0.000\n",
            "Epoch: 319 \t iter-Loss: 0.019\n",
            "Epoch: 320 \t iter-Loss: 0.008\n",
            "Epoch: 321 \t iter-Loss: 0.008\n",
            "Epoch: 322 \t iter-Loss: 0.001\n",
            "Epoch: 323 \t iter-Loss: 0.006\n",
            "Epoch: 324 \t iter-Loss: 0.006\n",
            "Epoch: 325 \t iter-Loss: 0.001\n",
            "Epoch: 326 \t iter-Loss: 0.000\n",
            "Epoch: 327 \t iter-Loss: 0.005\n",
            "Epoch: 328 \t iter-Loss: 0.004\n",
            "Epoch: 329 \t iter-Loss: 0.005\n",
            "Epoch: 330 \t iter-Loss: 0.008\n",
            "Epoch: 331 \t iter-Loss: 0.022\n",
            "Epoch: 332 \t iter-Loss: 0.005\n",
            "Epoch: 333 \t iter-Loss: 0.000\n",
            "Epoch: 334 \t iter-Loss: 0.001\n",
            "Epoch: 335 \t iter-Loss: 0.007\n",
            "Epoch: 336 \t iter-Loss: 0.005\n",
            "Epoch: 337 \t iter-Loss: 0.000\n",
            "Epoch: 338 \t iter-Loss: 0.000\n",
            "Epoch: 339 \t iter-Loss: 0.000\n",
            "Epoch: 340 \t iter-Loss: 0.000\n",
            "Epoch: 341 \t iter-Loss: 0.002\n",
            "Epoch: 342 \t iter-Loss: 0.001\n",
            "Epoch: 343 \t iter-Loss: 0.004\n",
            "Epoch: 344 \t iter-Loss: 0.000\n",
            "Epoch: 345 \t iter-Loss: 0.000\n",
            "Epoch: 346 \t iter-Loss: 0.003\n",
            "Epoch: 347 \t iter-Loss: 0.000\n",
            "Epoch: 348 \t iter-Loss: 0.000\n",
            "Epoch: 349 \t iter-Loss: 0.000\n",
            "Epoch: 350 \t iter-Loss: 0.000\n",
            "Epoch: 351 \t iter-Loss: 0.001\n",
            "Epoch: 352 \t iter-Loss: 0.001\n",
            "Epoch: 353 \t iter-Loss: 0.000\n",
            "Epoch: 354 \t iter-Loss: 0.000\n",
            "Epoch: 355 \t iter-Loss: 0.003\n",
            "Epoch: 356 \t iter-Loss: 0.000\n",
            "Epoch: 357 \t iter-Loss: 1.373\n",
            "Epoch: 358 \t iter-Loss: 0.005\n",
            "Epoch: 359 \t iter-Loss: 0.000\n",
            "Epoch: 360 \t iter-Loss: 0.000\n",
            "Epoch: 361 \t iter-Loss: 0.001\n",
            "Epoch: 362 \t iter-Loss: 0.000\n",
            "Epoch: 363 \t iter-Loss: 0.000\n",
            "Epoch: 364 \t iter-Loss: 0.000\n",
            "Epoch: 365 \t iter-Loss: 0.000\n",
            "Epoch: 366 \t iter-Loss: 0.000\n",
            "Epoch: 367 \t iter-Loss: 0.000\n",
            "Epoch: 368 \t iter-Loss: 0.001\n",
            "Epoch: 369 \t iter-Loss: 0.000\n",
            "Epoch: 370 \t iter-Loss: 0.000\n",
            "Epoch: 371 \t iter-Loss: 0.000\n",
            "Epoch: 372 \t iter-Loss: 0.000\n",
            "Epoch: 373 \t iter-Loss: 0.004\n",
            "Epoch: 374 \t iter-Loss: 0.000\n",
            "Epoch: 375 \t iter-Loss: 0.000\n",
            "Epoch: 376 \t iter-Loss: 0.001\n",
            "Epoch: 377 \t iter-Loss: 0.000\n",
            "Epoch: 378 \t iter-Loss: 0.001\n",
            "Epoch: 379 \t iter-Loss: 0.024\n",
            "Epoch: 380 \t iter-Loss: 0.000\n",
            "Epoch: 381 \t iter-Loss: 0.005\n",
            "Epoch: 382 \t iter-Loss: 0.002\n",
            "Epoch: 383 \t iter-Loss: 0.000\n",
            "Epoch: 384 \t iter-Loss: 0.001\n",
            "Epoch: 385 \t iter-Loss: 0.003\n",
            "Epoch: 386 \t iter-Loss: 0.000\n",
            "Epoch: 387 \t iter-Loss: 0.001\n",
            "Epoch: 388 \t iter-Loss: 0.000\n",
            "Epoch: 389 \t iter-Loss: 0.003\n",
            "Epoch: 390 \t iter-Loss: 0.000\n",
            "Epoch: 391 \t iter-Loss: 0.000\n",
            "Epoch: 392 \t iter-Loss: 0.000\n",
            "Epoch: 393 \t iter-Loss: 0.001\n",
            "Epoch: 394 \t iter-Loss: 0.000\n",
            "Epoch: 395 \t iter-Loss: 0.002\n",
            "Epoch: 396 \t iter-Loss: 0.005\n",
            "Epoch: 397 \t iter-Loss: 0.000\n",
            "Epoch: 398 \t iter-Loss: 0.013\n",
            "Epoch: 399 \t iter-Loss: 0.002\n",
            "Epoch: 400 \t iter-Loss: 0.000\n",
            "Epoch: 401 \t iter-Loss: 0.000\n",
            "Epoch: 402 \t iter-Loss: 0.000\n",
            "Epoch: 403 \t iter-Loss: 0.090\n",
            "Epoch: 404 \t iter-Loss: 0.001\n",
            "Epoch: 405 \t iter-Loss: 0.001\n",
            "Epoch: 406 \t iter-Loss: 0.001\n",
            "Epoch: 407 \t iter-Loss: 0.002\n",
            "Epoch: 408 \t iter-Loss: 0.001\n",
            "Epoch: 409 \t iter-Loss: 0.000\n",
            "Epoch: 410 \t iter-Loss: 0.000\n",
            "Epoch: 411 \t iter-Loss: 0.000\n",
            "Epoch: 412 \t iter-Loss: 0.000\n",
            "Epoch: 413 \t iter-Loss: 0.001\n",
            "Epoch: 414 \t iter-Loss: 0.000\n",
            "Epoch: 415 \t iter-Loss: 0.000\n",
            "Epoch: 416 \t iter-Loss: 0.000\n",
            "Epoch: 417 \t iter-Loss: 0.007\n",
            "Epoch: 418 \t iter-Loss: 0.000\n",
            "Epoch: 419 \t iter-Loss: 0.010\n",
            "Epoch: 420 \t iter-Loss: 0.000\n",
            "Epoch: 421 \t iter-Loss: 0.003\n",
            "Epoch: 422 \t iter-Loss: 0.008\n",
            "Epoch: 423 \t iter-Loss: 0.000\n",
            "Epoch: 424 \t iter-Loss: 0.000\n",
            "Epoch: 425 \t iter-Loss: 0.001\n",
            "Epoch: 426 \t iter-Loss: 0.004\n",
            "Epoch: 427 \t iter-Loss: 0.000\n",
            "Epoch: 428 \t iter-Loss: 0.000\n",
            "Epoch: 429 \t iter-Loss: 0.003\n",
            "Epoch: 430 \t iter-Loss: 0.000\n",
            "Epoch: 431 \t iter-Loss: 0.001\n",
            "Epoch: 432 \t iter-Loss: 0.000\n",
            "Epoch: 433 \t iter-Loss: 0.001\n",
            "Epoch: 434 \t iter-Loss: 0.007\n",
            "Epoch: 435 \t iter-Loss: 0.000\n",
            "Epoch: 436 \t iter-Loss: 0.000\n",
            "Epoch: 437 \t iter-Loss: 0.000\n",
            "Epoch: 438 \t iter-Loss: 0.001\n",
            "Epoch: 439 \t iter-Loss: 0.000\n",
            "Epoch: 440 \t iter-Loss: 0.001\n",
            "Epoch: 441 \t iter-Loss: 0.006\n",
            "Epoch: 442 \t iter-Loss: 0.000\n",
            "Epoch: 443 \t iter-Loss: 0.000\n",
            "Epoch: 444 \t iter-Loss: 0.001\n",
            "Epoch: 445 \t iter-Loss: 0.000\n",
            "Epoch: 446 \t iter-Loss: 0.000\n",
            "Epoch: 447 \t iter-Loss: 0.000\n",
            "Epoch: 448 \t iter-Loss: 0.000\n",
            "Epoch: 449 \t iter-Loss: 0.000\n",
            "Epoch: 450 \t iter-Loss: 0.000\n",
            "Epoch: 451 \t iter-Loss: 0.000\n",
            "Epoch: 452 \t iter-Loss: 0.000\n",
            "Epoch: 453 \t iter-Loss: 0.000\n",
            "Epoch: 454 \t iter-Loss: 0.001\n",
            "Epoch: 455 \t iter-Loss: 0.000\n",
            "Epoch: 456 \t iter-Loss: 0.000\n",
            "Epoch: 457 \t iter-Loss: 0.000\n",
            "Epoch: 458 \t iter-Loss: 0.000\n",
            "Epoch: 459 \t iter-Loss: 0.000\n",
            "Epoch: 460 \t iter-Loss: 0.001\n",
            "Epoch: 461 \t iter-Loss: 0.006\n",
            "Epoch: 462 \t iter-Loss: 0.000\n",
            "Epoch: 463 \t iter-Loss: 0.002\n",
            "Epoch: 464 \t iter-Loss: 0.001\n",
            "Epoch: 465 \t iter-Loss: 0.000\n",
            "Epoch: 466 \t iter-Loss: 0.000\n",
            "Epoch: 467 \t iter-Loss: 0.000\n",
            "Epoch: 468 \t iter-Loss: 0.013\n",
            "Epoch: 469 \t iter-Loss: 0.004\n",
            "Epoch: 470 \t iter-Loss: 0.000\n",
            "Epoch: 471 \t iter-Loss: 0.000\n",
            "Epoch: 472 \t iter-Loss: 0.003\n",
            "Epoch: 473 \t iter-Loss: 0.000\n",
            "Epoch: 474 \t iter-Loss: 0.000\n",
            "Epoch: 475 \t iter-Loss: 0.005\n",
            "Epoch: 476 \t iter-Loss: 0.000\n",
            "Epoch: 477 \t iter-Loss: 0.000\n",
            "Epoch: 478 \t iter-Loss: 0.001\n",
            "Epoch: 479 \t iter-Loss: 0.000\n",
            "Epoch: 480 \t iter-Loss: 0.000\n",
            "Epoch: 481 \t iter-Loss: 0.000\n",
            "Epoch: 482 \t iter-Loss: 0.000\n",
            "Epoch: 483 \t iter-Loss: 0.000\n",
            "Epoch: 484 \t iter-Loss: 0.002\n",
            "Epoch: 485 \t iter-Loss: 0.000\n",
            "Epoch: 486 \t iter-Loss: 0.000\n",
            "Epoch: 487 \t iter-Loss: 0.000\n",
            "Epoch: 488 \t iter-Loss: 0.005\n",
            "Epoch: 489 \t iter-Loss: 0.001\n",
            "Epoch: 490 \t iter-Loss: 0.000\n",
            "Epoch: 491 \t iter-Loss: 0.000\n",
            "Epoch: 492 \t iter-Loss: 0.001\n",
            "Epoch: 493 \t iter-Loss: 0.000\n",
            "Epoch: 494 \t iter-Loss: 0.000\n",
            "Epoch: 495 \t iter-Loss: 0.000\n",
            "Epoch: 496 \t iter-Loss: 0.001\n",
            "Epoch: 497 \t iter-Loss: 0.000\n",
            "Epoch: 498 \t iter-Loss: 0.000\n",
            "Epoch: 499 \t iter-Loss: 0.001\n",
            "Epoch: 500 \t iter-Loss: 0.000\n",
            "Epoch: 501 \t iter-Loss: 0.000\n",
            "Epoch: 502 \t iter-Loss: 0.000\n",
            "Epoch: 503 \t iter-Loss: 0.000\n",
            "Epoch: 504 \t iter-Loss: 0.014\n",
            "Epoch: 505 \t iter-Loss: 0.000\n",
            "Epoch: 506 \t iter-Loss: 0.000\n",
            "Epoch: 507 \t iter-Loss: 0.000\n",
            "Epoch: 508 \t iter-Loss: 0.003\n",
            "Epoch: 509 \t iter-Loss: 0.000\n",
            "Epoch: 510 \t iter-Loss: 0.000\n",
            "Epoch: 511 \t iter-Loss: 0.003\n",
            "Epoch: 512 \t iter-Loss: 0.000\n",
            "Epoch: 513 \t iter-Loss: 0.000\n",
            "Epoch: 514 \t iter-Loss: 0.001\n",
            "Epoch: 515 \t iter-Loss: 0.000\n",
            "Epoch: 516 \t iter-Loss: 0.001\n",
            "Epoch: 517 \t iter-Loss: 0.000\n",
            "Epoch: 518 \t iter-Loss: 0.000\n",
            "Epoch: 519 \t iter-Loss: 0.006\n",
            "Epoch: 520 \t iter-Loss: 0.000\n",
            "Epoch: 521 \t iter-Loss: 0.000\n",
            "Epoch: 522 \t iter-Loss: 0.000\n",
            "Epoch: 523 \t iter-Loss: 0.003\n",
            "Epoch: 524 \t iter-Loss: 0.000\n",
            "Epoch: 525 \t iter-Loss: 0.000\n",
            "Epoch: 526 \t iter-Loss: 0.001\n",
            "Epoch: 527 \t iter-Loss: 0.000\n",
            "Epoch: 528 \t iter-Loss: 0.000\n",
            "Epoch: 529 \t iter-Loss: 0.000\n",
            "Epoch: 530 \t iter-Loss: 0.000\n",
            "Epoch: 531 \t iter-Loss: 0.001\n",
            "Epoch: 532 \t iter-Loss: 0.000\n",
            "Epoch: 533 \t iter-Loss: 0.000\n",
            "Epoch: 534 \t iter-Loss: 0.000\n",
            "Epoch: 535 \t iter-Loss: 0.000\n",
            "Epoch: 536 \t iter-Loss: 0.000\n",
            "Epoch: 537 \t iter-Loss: 0.000\n",
            "Epoch: 538 \t iter-Loss: 0.000\n",
            "Epoch: 539 \t iter-Loss: 0.056\n",
            "Epoch: 540 \t iter-Loss: 0.000\n",
            "Epoch: 541 \t iter-Loss: 0.006\n",
            "Epoch: 542 \t iter-Loss: 0.000\n",
            "Epoch: 543 \t iter-Loss: 0.000\n",
            "Epoch: 544 \t iter-Loss: 0.000\n",
            "Epoch: 545 \t iter-Loss: 0.000\n",
            "Epoch: 546 \t iter-Loss: 0.001\n",
            "Epoch: 547 \t iter-Loss: 0.000\n",
            "Epoch: 548 \t iter-Loss: 0.004\n",
            "Epoch: 549 \t iter-Loss: 0.008\n",
            "Epoch: 550 \t iter-Loss: 0.001\n",
            "Epoch: 551 \t iter-Loss: 0.000\n",
            "Epoch: 552 \t iter-Loss: 0.001\n",
            "Epoch: 553 \t iter-Loss: 0.005\n",
            "Epoch: 554 \t iter-Loss: 0.001\n",
            "Epoch: 555 \t iter-Loss: 0.000\n",
            "Epoch: 556 \t iter-Loss: 0.006\n",
            "Epoch: 557 \t iter-Loss: 0.000\n",
            "Epoch: 558 \t iter-Loss: 0.000\n",
            "Epoch: 559 \t iter-Loss: 0.000\n",
            "Epoch: 560 \t iter-Loss: 0.000\n",
            "Epoch: 561 \t iter-Loss: 0.000\n",
            "Epoch: 562 \t iter-Loss: 0.006\n",
            "Epoch: 563 \t iter-Loss: 0.000\n",
            "Epoch: 564 \t iter-Loss: 0.000\n",
            "Epoch: 565 \t iter-Loss: 0.000\n",
            "Epoch: 566 \t iter-Loss: 0.000\n",
            "Epoch: 567 \t iter-Loss: 0.011\n",
            "Epoch: 568 \t iter-Loss: 0.000\n",
            "Epoch: 569 \t iter-Loss: 0.000\n",
            "Epoch: 570 \t iter-Loss: 0.001\n",
            "Epoch: 571 \t iter-Loss: 0.000\n",
            "Epoch: 572 \t iter-Loss: 0.000\n",
            "Epoch: 573 \t iter-Loss: 0.004\n",
            "Epoch: 574 \t iter-Loss: 0.001\n",
            "Epoch: 575 \t iter-Loss: 0.000\n",
            "Epoch: 576 \t iter-Loss: 0.000\n",
            "Epoch: 577 \t iter-Loss: 0.000\n",
            "Epoch: 578 \t iter-Loss: 0.001\n",
            "Epoch: 579 \t iter-Loss: 0.000\n",
            "Epoch: 580 \t iter-Loss: 0.000\n",
            "Epoch: 581 \t iter-Loss: 0.000\n",
            "Epoch: 582 \t iter-Loss: 0.000\n",
            "Epoch: 583 \t iter-Loss: 0.000\n",
            "Epoch: 584 \t iter-Loss: 0.000\n",
            "Epoch: 585 \t iter-Loss: 0.000\n",
            "Epoch: 586 \t iter-Loss: 0.042\n",
            "Epoch: 587 \t iter-Loss: 0.001\n",
            "Epoch: 588 \t iter-Loss: 0.000\n",
            "Epoch: 589 \t iter-Loss: 0.001\n",
            "Epoch: 590 \t iter-Loss: 0.000\n",
            "Epoch: 591 \t iter-Loss: 0.000\n",
            "Epoch: 592 \t iter-Loss: 0.001\n",
            "Epoch: 593 \t iter-Loss: 0.000\n",
            "Epoch: 594 \t iter-Loss: 0.000\n",
            "Epoch: 595 \t iter-Loss: 0.002\n",
            "Epoch: 596 \t iter-Loss: 0.000\n",
            "Epoch: 597 \t iter-Loss: 0.001\n",
            "Epoch: 598 \t iter-Loss: 0.000\n",
            "Epoch: 599 \t iter-Loss: 0.000\n",
            "Epoch: 600 \t iter-Loss: 0.000\n",
            "Epoch: 601 \t iter-Loss: 0.000\n",
            "Epoch: 602 \t iter-Loss: 0.000\n",
            "Epoch: 603 \t iter-Loss: 0.010\n",
            "Epoch: 604 \t iter-Loss: 0.000\n",
            "Epoch: 605 \t iter-Loss: 0.001\n",
            "Epoch: 606 \t iter-Loss: 0.000\n",
            "Epoch: 607 \t iter-Loss: 0.000\n",
            "Epoch: 608 \t iter-Loss: 0.000\n",
            "Epoch: 609 \t iter-Loss: 0.000\n",
            "Epoch: 610 \t iter-Loss: 0.002\n",
            "Epoch: 611 \t iter-Loss: 0.000\n",
            "Epoch: 612 \t iter-Loss: 0.000\n",
            "Epoch: 613 \t iter-Loss: 0.000\n",
            "Epoch: 614 \t iter-Loss: 0.000\n",
            "Epoch: 615 \t iter-Loss: 0.000\n",
            "Epoch: 616 \t iter-Loss: 0.000\n",
            "Epoch: 617 \t iter-Loss: 0.000\n",
            "Epoch: 618 \t iter-Loss: 0.000\n",
            "Epoch: 619 \t iter-Loss: 0.000\n",
            "Epoch: 620 \t iter-Loss: 0.000\n",
            "Epoch: 621 \t iter-Loss: 0.000\n",
            "Epoch: 622 \t iter-Loss: 0.000\n",
            "Epoch: 623 \t iter-Loss: 0.000\n",
            "Epoch: 624 \t iter-Loss: 0.000\n",
            "Epoch: 625 \t iter-Loss: 0.001\n",
            "Epoch: 626 \t iter-Loss: 0.000\n",
            "Epoch: 627 \t iter-Loss: 0.000\n",
            "Epoch: 628 \t iter-Loss: 0.000\n",
            "Epoch: 629 \t iter-Loss: 0.000\n",
            "Epoch: 630 \t iter-Loss: 0.001\n",
            "Epoch: 631 \t iter-Loss: 0.000\n",
            "Epoch: 632 \t iter-Loss: 0.000\n",
            "Epoch: 633 \t iter-Loss: 0.000\n",
            "Epoch: 634 \t iter-Loss: 0.008\n",
            "Epoch: 635 \t iter-Loss: 0.005\n",
            "Epoch: 636 \t iter-Loss: 0.000\n",
            "Epoch: 637 \t iter-Loss: 0.000\n",
            "Epoch: 638 \t iter-Loss: 0.000\n",
            "Epoch: 639 \t iter-Loss: 0.000\n",
            "Epoch: 640 \t iter-Loss: 0.000\n",
            "Epoch: 641 \t iter-Loss: 0.000\n",
            "Epoch: 642 \t iter-Loss: 0.000\n",
            "Epoch: 643 \t iter-Loss: 0.000\n",
            "Epoch: 644 \t iter-Loss: 0.004\n",
            "Epoch: 645 \t iter-Loss: 0.000\n",
            "Epoch: 646 \t iter-Loss: 0.024\n",
            "Epoch: 647 \t iter-Loss: 0.001\n",
            "Epoch: 648 \t iter-Loss: 0.000\n",
            "Epoch: 649 \t iter-Loss: 0.000\n",
            "Epoch: 650 \t iter-Loss: 0.000\n",
            "Epoch: 651 \t iter-Loss: 0.001\n",
            "Epoch: 652 \t iter-Loss: 0.001\n",
            "Epoch: 653 \t iter-Loss: 0.000\n",
            "Epoch: 654 \t iter-Loss: 0.000\n",
            "Epoch: 655 \t iter-Loss: 0.000\n",
            "Epoch: 656 \t iter-Loss: 0.000\n",
            "Epoch: 657 \t iter-Loss: 0.016\n",
            "Epoch: 658 \t iter-Loss: 0.000\n",
            "Epoch: 659 \t iter-Loss: 0.004\n",
            "Epoch: 660 \t iter-Loss: 0.000\n",
            "Epoch: 661 \t iter-Loss: 0.000\n",
            "Epoch: 662 \t iter-Loss: 0.000\n",
            "Epoch: 663 \t iter-Loss: 0.000\n",
            "Epoch: 664 \t iter-Loss: 0.000\n",
            "Epoch: 665 \t iter-Loss: 0.000\n",
            "Epoch: 666 \t iter-Loss: 0.000\n",
            "Epoch: 667 \t iter-Loss: 0.001\n",
            "Epoch: 668 \t iter-Loss: 0.000\n",
            "Epoch: 669 \t iter-Loss: 0.002\n",
            "Epoch: 670 \t iter-Loss: 0.000\n",
            "Epoch: 671 \t iter-Loss: 0.000\n",
            "Epoch: 672 \t iter-Loss: 0.006\n",
            "Epoch: 673 \t iter-Loss: 0.000\n",
            "Epoch: 674 \t iter-Loss: 0.000\n",
            "Epoch: 675 \t iter-Loss: 0.000\n",
            "Epoch: 676 \t iter-Loss: 0.000\n",
            "Epoch: 677 \t iter-Loss: 0.000\n",
            "Epoch: 678 \t iter-Loss: 0.000\n",
            "Epoch: 679 \t iter-Loss: 0.002\n",
            "Epoch: 680 \t iter-Loss: 0.000\n",
            "Epoch: 681 \t iter-Loss: 0.000\n",
            "Epoch: 682 \t iter-Loss: 0.000\n",
            "Epoch: 683 \t iter-Loss: 0.000\n",
            "Epoch: 684 \t iter-Loss: 0.000\n",
            "Epoch: 685 \t iter-Loss: 0.000\n",
            "Epoch: 686 \t iter-Loss: 0.000\n",
            "Epoch: 687 \t iter-Loss: 0.000\n",
            "Epoch: 688 \t iter-Loss: 0.000\n",
            "Epoch: 689 \t iter-Loss: 0.000\n",
            "Epoch: 690 \t iter-Loss: 0.000\n",
            "Epoch: 691 \t iter-Loss: 0.000\n",
            "Epoch: 692 \t iter-Loss: 0.000\n",
            "Epoch: 693 \t iter-Loss: 0.000\n",
            "Epoch: 694 \t iter-Loss: 0.000\n",
            "Epoch: 695 \t iter-Loss: 0.003\n",
            "Epoch: 696 \t iter-Loss: 0.000\n",
            "Epoch: 697 \t iter-Loss: 0.000\n",
            "Epoch: 698 \t iter-Loss: 0.000\n",
            "Epoch: 699 \t iter-Loss: 0.000\n",
            "Epoch: 700 \t iter-Loss: 0.000\n",
            "Epoch: 701 \t iter-Loss: 0.001\n",
            "Epoch: 702 \t iter-Loss: 0.000\n",
            "Epoch: 703 \t iter-Loss: 0.000\n",
            "Epoch: 704 \t iter-Loss: 0.000\n",
            "Epoch: 705 \t iter-Loss: 0.002\n",
            "Epoch: 706 \t iter-Loss: 0.001\n",
            "Epoch: 707 \t iter-Loss: 0.000\n",
            "Epoch: 708 \t iter-Loss: 0.000\n",
            "Epoch: 709 \t iter-Loss: 0.001\n",
            "Epoch: 710 \t iter-Loss: 0.000\n",
            "Epoch: 711 \t iter-Loss: 0.001\n",
            "Epoch: 712 \t iter-Loss: 0.000\n",
            "Epoch: 713 \t iter-Loss: 0.000\n",
            "Epoch: 714 \t iter-Loss: 0.000\n",
            "Epoch: 715 \t iter-Loss: 0.000\n",
            "Epoch: 716 \t iter-Loss: 0.001\n",
            "Epoch: 717 \t iter-Loss: 0.000\n",
            "Epoch: 718 \t iter-Loss: 0.000\n",
            "Epoch: 719 \t iter-Loss: 0.000\n",
            "Epoch: 720 \t iter-Loss: 0.006\n",
            "Epoch: 721 \t iter-Loss: 0.000\n",
            "Epoch: 722 \t iter-Loss: 0.000\n",
            "Epoch: 723 \t iter-Loss: 0.000\n",
            "Epoch: 724 \t iter-Loss: 0.000\n",
            "Epoch: 725 \t iter-Loss: 0.001\n",
            "Epoch: 726 \t iter-Loss: 0.000\n",
            "Epoch: 727 \t iter-Loss: 0.000\n",
            "Epoch: 728 \t iter-Loss: 0.001\n",
            "Epoch: 729 \t iter-Loss: 0.003\n",
            "Epoch: 730 \t iter-Loss: 0.000\n",
            "Epoch: 731 \t iter-Loss: 0.000\n",
            "Epoch: 732 \t iter-Loss: 0.000\n",
            "Epoch: 733 \t iter-Loss: 0.000\n",
            "Epoch: 734 \t iter-Loss: 0.000\n",
            "Epoch: 735 \t iter-Loss: 0.000\n",
            "Epoch: 736 \t iter-Loss: 0.001\n",
            "Epoch: 737 \t iter-Loss: 0.000\n",
            "Epoch: 738 \t iter-Loss: 0.000\n",
            "Epoch: 739 \t iter-Loss: 0.000\n",
            "Epoch: 740 \t iter-Loss: 0.003\n",
            "Epoch: 741 \t iter-Loss: 0.000\n",
            "Epoch: 742 \t iter-Loss: 0.000\n",
            "Epoch: 743 \t iter-Loss: 0.000\n",
            "Epoch: 744 \t iter-Loss: 0.000\n",
            "Epoch: 745 \t iter-Loss: 0.000\n",
            "Epoch: 746 \t iter-Loss: 0.001\n",
            "Epoch: 747 \t iter-Loss: 0.001\n",
            "Epoch: 748 \t iter-Loss: 0.000\n",
            "Epoch: 749 \t iter-Loss: 0.000\n",
            "Epoch: 750 \t iter-Loss: 0.000\n",
            "Epoch: 751 \t iter-Loss: 0.000\n",
            "Epoch: 752 \t iter-Loss: 0.001\n",
            "Epoch: 753 \t iter-Loss: 0.019\n",
            "Epoch: 754 \t iter-Loss: 0.000\n",
            "Epoch: 755 \t iter-Loss: 0.000\n",
            "Epoch: 756 \t iter-Loss: 0.000\n",
            "Epoch: 757 \t iter-Loss: 0.000\n",
            "Epoch: 758 \t iter-Loss: 0.000\n",
            "Epoch: 759 \t iter-Loss: 0.000\n",
            "Epoch: 760 \t iter-Loss: 0.000\n",
            "Epoch: 761 \t iter-Loss: 0.001\n",
            "Epoch: 762 \t iter-Loss: 0.000\n",
            "Epoch: 763 \t iter-Loss: 0.000\n",
            "Epoch: 764 \t iter-Loss: 0.000\n",
            "Epoch: 765 \t iter-Loss: 0.000\n",
            "Epoch: 766 \t iter-Loss: 0.000\n",
            "Epoch: 767 \t iter-Loss: 0.000\n",
            "Epoch: 768 \t iter-Loss: 0.000\n",
            "Epoch: 769 \t iter-Loss: 0.000\n",
            "Epoch: 770 \t iter-Loss: 0.000\n",
            "Epoch: 771 \t iter-Loss: 0.000\n",
            "Epoch: 772 \t iter-Loss: 0.000\n",
            "Epoch: 773 \t iter-Loss: 0.000\n",
            "Epoch: 774 \t iter-Loss: 0.000\n",
            "Epoch: 775 \t iter-Loss: 0.000\n",
            "Epoch: 776 \t iter-Loss: 0.000\n",
            "Epoch: 777 \t iter-Loss: 0.000\n",
            "Epoch: 778 \t iter-Loss: 0.000\n",
            "Epoch: 779 \t iter-Loss: 0.000\n",
            "Epoch: 780 \t iter-Loss: 0.000\n",
            "Epoch: 781 \t iter-Loss: 0.000\n",
            "Epoch: 782 \t iter-Loss: 0.000\n",
            "Epoch: 783 \t iter-Loss: 0.000\n",
            "Epoch: 784 \t iter-Loss: 0.000\n",
            "Epoch: 785 \t iter-Loss: 0.000\n",
            "Epoch: 786 \t iter-Loss: 0.005\n",
            "Epoch: 787 \t iter-Loss: 0.000\n",
            "Epoch: 788 \t iter-Loss: 0.000\n",
            "Epoch: 789 \t iter-Loss: 0.000\n",
            "Epoch: 790 \t iter-Loss: 0.000\n",
            "Epoch: 791 \t iter-Loss: 0.000\n",
            "Epoch: 792 \t iter-Loss: 0.001\n",
            "Epoch: 793 \t iter-Loss: 0.000\n",
            "Epoch: 794 \t iter-Loss: 0.000\n",
            "Epoch: 795 \t iter-Loss: 0.000\n",
            "Epoch: 796 \t iter-Loss: 0.000\n",
            "Epoch: 797 \t iter-Loss: 0.000\n",
            "Epoch: 798 \t iter-Loss: 0.000\n",
            "Epoch: 799 \t iter-Loss: 0.003\n",
            "Epoch: 800 \t iter-Loss: 0.000\n",
            "Epoch: 801 \t iter-Loss: 0.001\n",
            "Epoch: 802 \t iter-Loss: 0.000\n",
            "Epoch: 803 \t iter-Loss: 0.001\n",
            "Epoch: 804 \t iter-Loss: 0.005\n",
            "Epoch: 805 \t iter-Loss: 0.000\n",
            "Epoch: 806 \t iter-Loss: 0.000\n",
            "Epoch: 807 \t iter-Loss: 0.000\n",
            "Epoch: 808 \t iter-Loss: 0.000\n",
            "Epoch: 809 \t iter-Loss: 0.000\n",
            "Epoch: 810 \t iter-Loss: 0.000\n",
            "Epoch: 811 \t iter-Loss: 0.000\n",
            "Epoch: 812 \t iter-Loss: 0.000\n",
            "Epoch: 813 \t iter-Loss: 0.000\n",
            "Epoch: 814 \t iter-Loss: 0.001\n",
            "Epoch: 815 \t iter-Loss: 0.000\n",
            "Epoch: 816 \t iter-Loss: 0.000\n",
            "Epoch: 817 \t iter-Loss: 0.000\n",
            "Epoch: 818 \t iter-Loss: 0.000\n",
            "Epoch: 819 \t iter-Loss: 0.000\n",
            "Epoch: 820 \t iter-Loss: 0.000\n",
            "Epoch: 821 \t iter-Loss: 0.000\n",
            "Epoch: 822 \t iter-Loss: 0.000\n",
            "Epoch: 823 \t iter-Loss: 0.000\n",
            "Epoch: 824 \t iter-Loss: 0.000\n",
            "Epoch: 825 \t iter-Loss: 0.000\n",
            "Epoch: 826 \t iter-Loss: 0.000\n",
            "Epoch: 827 \t iter-Loss: 0.000\n",
            "Epoch: 828 \t iter-Loss: 0.000\n",
            "Epoch: 829 \t iter-Loss: 0.000\n",
            "Epoch: 830 \t iter-Loss: 0.000\n",
            "Epoch: 831 \t iter-Loss: 0.000\n",
            "Epoch: 832 \t iter-Loss: 0.000\n",
            "Epoch: 833 \t iter-Loss: 0.000\n",
            "Epoch: 834 \t iter-Loss: 0.000\n",
            "Epoch: 835 \t iter-Loss: 0.000\n",
            "Epoch: 836 \t iter-Loss: 0.000\n",
            "Epoch: 837 \t iter-Loss: 0.000\n",
            "Epoch: 838 \t iter-Loss: 0.000\n",
            "Epoch: 839 \t iter-Loss: 0.000\n",
            "Epoch: 840 \t iter-Loss: 0.000\n",
            "Epoch: 841 \t iter-Loss: 0.000\n",
            "Epoch: 842 \t iter-Loss: 0.000\n",
            "Epoch: 843 \t iter-Loss: 0.000\n",
            "Epoch: 844 \t iter-Loss: 0.000\n",
            "Epoch: 845 \t iter-Loss: 0.000\n",
            "Epoch: 846 \t iter-Loss: 0.000\n",
            "Epoch: 847 \t iter-Loss: 0.000\n",
            "Epoch: 848 \t iter-Loss: 0.001\n",
            "Epoch: 849 \t iter-Loss: 0.003\n",
            "Epoch: 850 \t iter-Loss: 0.000\n",
            "Epoch: 851 \t iter-Loss: 0.000\n",
            "Epoch: 852 \t iter-Loss: 0.000\n",
            "Epoch: 853 \t iter-Loss: 0.000\n",
            "Epoch: 854 \t iter-Loss: 0.000\n",
            "Epoch: 855 \t iter-Loss: 0.000\n",
            "Epoch: 856 \t iter-Loss: 0.000\n",
            "Epoch: 857 \t iter-Loss: 0.005\n",
            "Epoch: 858 \t iter-Loss: 0.000\n",
            "Epoch: 859 \t iter-Loss: 0.001\n",
            "Epoch: 860 \t iter-Loss: 0.000\n",
            "Epoch: 861 \t iter-Loss: 0.000\n",
            "Epoch: 862 \t iter-Loss: 0.000\n",
            "Epoch: 863 \t iter-Loss: 0.000\n",
            "Epoch: 864 \t iter-Loss: 0.001\n",
            "Epoch: 865 \t iter-Loss: 0.000\n",
            "Epoch: 866 \t iter-Loss: 0.000\n",
            "Epoch: 867 \t iter-Loss: 0.000\n",
            "Epoch: 868 \t iter-Loss: 0.000\n",
            "Epoch: 869 \t iter-Loss: 0.000\n",
            "Epoch: 870 \t iter-Loss: 0.001\n",
            "Epoch: 871 \t iter-Loss: 0.000\n",
            "Epoch: 872 \t iter-Loss: 0.006\n",
            "Epoch: 873 \t iter-Loss: 0.000\n",
            "Epoch: 874 \t iter-Loss: 0.002\n",
            "Epoch: 875 \t iter-Loss: 0.001\n",
            "Epoch: 876 \t iter-Loss: 0.000\n",
            "Epoch: 877 \t iter-Loss: 0.000\n",
            "Epoch: 878 \t iter-Loss: 0.000\n",
            "Epoch: 879 \t iter-Loss: 0.000\n",
            "Epoch: 880 \t iter-Loss: 0.000\n",
            "Epoch: 881 \t iter-Loss: 0.000\n",
            "Epoch: 882 \t iter-Loss: 0.000\n",
            "Epoch: 883 \t iter-Loss: 0.000\n",
            "Epoch: 884 \t iter-Loss: 0.000\n",
            "Epoch: 885 \t iter-Loss: 0.000\n",
            "Epoch: 886 \t iter-Loss: 0.000\n",
            "Epoch: 887 \t iter-Loss: 0.000\n",
            "Epoch: 888 \t iter-Loss: 0.000\n",
            "Epoch: 889 \t iter-Loss: 0.000\n",
            "Epoch: 890 \t iter-Loss: 0.000\n",
            "Epoch: 891 \t iter-Loss: 0.000\n",
            "Epoch: 892 \t iter-Loss: 0.000\n",
            "Epoch: 893 \t iter-Loss: 0.000\n",
            "Epoch: 894 \t iter-Loss: 0.000\n",
            "Epoch: 895 \t iter-Loss: 0.000\n",
            "Epoch: 896 \t iter-Loss: 0.000\n",
            "Epoch: 897 \t iter-Loss: 0.000\n",
            "Epoch: 898 \t iter-Loss: 0.000\n",
            "Epoch: 899 \t iter-Loss: 0.000\n",
            "Epoch: 900 \t iter-Loss: 0.000\n",
            "Epoch: 901 \t iter-Loss: 0.000\n",
            "Epoch: 902 \t iter-Loss: 0.000\n",
            "Epoch: 903 \t iter-Loss: 0.000\n",
            "Epoch: 904 \t iter-Loss: 0.000\n",
            "Epoch: 905 \t iter-Loss: 0.000\n",
            "Epoch: 906 \t iter-Loss: 0.000\n",
            "Epoch: 907 \t iter-Loss: 0.000\n",
            "Epoch: 908 \t iter-Loss: 0.000\n",
            "Epoch: 909 \t iter-Loss: 0.001\n",
            "Epoch: 910 \t iter-Loss: 0.000\n",
            "Epoch: 911 \t iter-Loss: 0.000\n",
            "Epoch: 912 \t iter-Loss: 0.000\n",
            "Epoch: 913 \t iter-Loss: 0.000\n",
            "Epoch: 914 \t iter-Loss: 0.000\n",
            "Epoch: 915 \t iter-Loss: 0.005\n",
            "Epoch: 916 \t iter-Loss: 0.001\n",
            "Epoch: 917 \t iter-Loss: 0.000\n",
            "Epoch: 918 \t iter-Loss: 0.000\n",
            "Epoch: 919 \t iter-Loss: 0.015\n",
            "Epoch: 920 \t iter-Loss: 0.000\n",
            "Epoch: 921 \t iter-Loss: 0.000\n",
            "Epoch: 922 \t iter-Loss: 0.000\n",
            "Epoch: 923 \t iter-Loss: 0.000\n",
            "Epoch: 924 \t iter-Loss: 0.000\n",
            "Epoch: 925 \t iter-Loss: 0.004\n",
            "Epoch: 926 \t iter-Loss: 0.010\n",
            "Epoch: 927 \t iter-Loss: 0.001\n",
            "Epoch: 928 \t iter-Loss: 0.000\n",
            "Epoch: 929 \t iter-Loss: 0.000\n",
            "Epoch: 930 \t iter-Loss: 0.000\n",
            "Epoch: 931 \t iter-Loss: 0.000\n",
            "Epoch: 932 \t iter-Loss: 0.000\n",
            "Epoch: 933 \t iter-Loss: 0.000\n",
            "Epoch: 934 \t iter-Loss: 0.000\n",
            "Epoch: 935 \t iter-Loss: 0.002\n",
            "Epoch: 936 \t iter-Loss: 0.000\n",
            "Epoch: 937 \t iter-Loss: 0.002\n",
            "Epoch: 938 \t iter-Loss: 0.000\n",
            "Epoch: 939 \t iter-Loss: 0.001\n",
            "Epoch: 940 \t iter-Loss: 0.000\n",
            "Epoch: 941 \t iter-Loss: 0.000\n",
            "Epoch: 942 \t iter-Loss: 0.000\n",
            "Epoch: 943 \t iter-Loss: 0.000\n",
            "Epoch: 944 \t iter-Loss: 0.000\n",
            "Epoch: 945 \t iter-Loss: 0.000\n",
            "Epoch: 946 \t iter-Loss: 0.000\n",
            "Epoch: 947 \t iter-Loss: 0.000\n",
            "Epoch: 948 \t iter-Loss: 0.000\n",
            "Epoch: 949 \t iter-Loss: 0.001\n",
            "Epoch: 950 \t iter-Loss: 0.000\n",
            "Epoch: 951 \t iter-Loss: 0.001\n",
            "Epoch: 952 \t iter-Loss: 0.000\n",
            "Epoch: 953 \t iter-Loss: 0.000\n",
            "Epoch: 954 \t iter-Loss: 0.000\n",
            "Epoch: 955 \t iter-Loss: 0.000\n",
            "Epoch: 956 \t iter-Loss: 0.000\n",
            "Epoch: 957 \t iter-Loss: 0.000\n",
            "Epoch: 958 \t iter-Loss: 0.004\n",
            "Epoch: 959 \t iter-Loss: 0.000\n",
            "Epoch: 960 \t iter-Loss: 0.000\n",
            "Epoch: 961 \t iter-Loss: 0.000\n",
            "Epoch: 962 \t iter-Loss: 0.000\n",
            "Epoch: 963 \t iter-Loss: 0.002\n",
            "Epoch: 964 \t iter-Loss: 0.000\n",
            "Epoch: 965 \t iter-Loss: 0.000\n",
            "Epoch: 966 \t iter-Loss: 0.000\n",
            "Epoch: 967 \t iter-Loss: 0.001\n",
            "Epoch: 968 \t iter-Loss: 0.000\n",
            "Epoch: 969 \t iter-Loss: 0.000\n",
            "Epoch: 970 \t iter-Loss: 0.000\n",
            "Epoch: 971 \t iter-Loss: 0.000\n",
            "Epoch: 972 \t iter-Loss: 0.000\n",
            "Epoch: 973 \t iter-Loss: 0.000\n",
            "Epoch: 974 \t iter-Loss: 0.000\n",
            "Epoch: 975 \t iter-Loss: 0.000\n",
            "Epoch: 976 \t iter-Loss: 0.000\n",
            "Epoch: 977 \t iter-Loss: 0.000\n",
            "Epoch: 978 \t iter-Loss: 0.000\n",
            "Epoch: 979 \t iter-Loss: 0.000\n",
            "Epoch: 980 \t iter-Loss: 0.000\n",
            "Epoch: 981 \t iter-Loss: 0.000\n",
            "Epoch: 982 \t iter-Loss: 0.000\n",
            "Epoch: 983 \t iter-Loss: 0.000\n",
            "Epoch: 984 \t iter-Loss: 0.000\n",
            "Epoch: 985 \t iter-Loss: 0.000\n",
            "Epoch: 986 \t iter-Loss: 0.000\n",
            "Epoch: 987 \t iter-Loss: 0.000\n",
            "Epoch: 988 \t iter-Loss: 0.000\n",
            "Epoch: 989 \t iter-Loss: 0.000\n",
            "Epoch: 990 \t iter-Loss: 0.000\n",
            "Epoch: 991 \t iter-Loss: 0.000\n",
            "Epoch: 992 \t iter-Loss: 0.000\n",
            "Epoch: 993 \t iter-Loss: 0.000\n",
            "Epoch: 994 \t iter-Loss: 0.000\n",
            "Epoch: 995 \t iter-Loss: 0.000\n",
            "Epoch: 996 \t iter-Loss: 0.000\n",
            "Epoch: 997 \t iter-Loss: 0.000\n",
            "Epoch: 998 \t iter-Loss: 0.000\n",
            "Epoch: 999 \t iter-Loss: 0.000\n",
            "Epoch: 1000 \t iter-Loss: 0.000\n",
            "final loss: 0.0002\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"GPU is avaible: {device}\")\n",
        "\n",
        "# Define the different inputs in our model\n",
        "num_epochs = 1000\n",
        "BATCH_SIZE = 16\n",
        "LR = 1e-1\n",
        "INPUT_SIZE = len(vocab)\n",
        "OUTPUT_SIZE = num_classes\n",
        "USE_CNN = True\n",
        "SHUFFLE = True\n",
        "\n",
        "# Define model, optimizer, loss and scheduler (Q: ¬øWhat is it?)\n",
        "model = CNNClassifier(INPUT_SIZE, num_classes=OUTPUT_SIZE, use_cnn=USE_CNN).to(device)\n",
        "optimizer = SGD(model.parameters(), lr=LR)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=[lambda epoch: .9 ** (epoch // 10)])\n",
        "\n",
        "print(f'train: {len(train_list)} elements')\n",
        "\n",
        "# We train the model using the intents\n",
        "loss_list= []\n",
        "for epoch in range(1, num_epochs):\n",
        "  train_loader = DataLoader(train_list, shuffle=SHUFFLE, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "  for i, (texts, offsets, cls) in enumerate(train_loader):\n",
        "    texts = texts.to(device)\n",
        "    offsets = offsets.to(device)\n",
        "    cls = cls.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    output = model(texts, offsets)\n",
        "    loss = criterion(output, cls)\n",
        "    total_loss += loss.item()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  loss_list.append(loss.item())\n",
        "  print('\\rEpoch: {0:03d} \\t iter-Loss: {1:.3f}'.format(epoch+1, loss.item()))\n",
        "\n",
        "print(f'final loss: {loss.item():.4f}')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9dlS4_X-L3DN"
      },
      "source": [
        "##### A probar! üß™"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6IhhAKFXL3eH",
        "outputId": "b7c9ae59-d5d3-4e4a-8864-1ec72eb54e98"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'funny'"
            ]
          },
          "execution_count": 102,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# This is working?, Try the next example!\n",
        "qText = \"Do you know any joke?\" # this must classify the label \"funny\"\n",
        "\n",
        "# X = torch.tensor([vocab.stoi[t] for t in tokenizer(qText)]).to(device)\n",
        "X = torch.tensor([vocab[t] if t in stoi else 0 for t in tokenizer(qText)]).to(device)\n",
        "if X.shape[0] < model.cnn_kernel_size:\n",
        "    pad = torch.zeros((model.cnn_kernel_size - X.shape[0]), dtype=torch.long).to(X.device)\n",
        "    X = torch.concat([X, pad])\n",
        "\n",
        "model.eval()\n",
        "output = model(X, torch.tensor([0], dtype=torch.long).to(device))\n",
        "_, predicted = torch.max(output, dim=1)\n",
        "labels[predicted]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "udemze3zL549"
      },
      "source": [
        "Ya pero prometiste hacer un chatbot, no una simple clasificaci√≥n...."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OpSYGx2tL0tC"
      },
      "source": [
        "##### Guardamos modelo ü¶∫ (opcional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBC4TyiqLzDv",
        "outputId": "2f479166-e1c6-4f24-8d15-1d45ffb9818d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training complete. file saved to ../../models/tarea4.pth\n"
          ]
        }
      ],
      "source": [
        "# We save de model using pytorch (this is optional, just to learn how to do this in pytorch)\n",
        "data = {\n",
        "    \"model_state\": model.state_dict(),\n",
        "    \"input_size\": INPUT_SIZE,\n",
        "    \"output_size\": OUTPUT_SIZE,\n",
        "    \"use_cnn\": USE_CNN,\n",
        "    \"labels\": labels\n",
        "}\n",
        "\n",
        "FILE_PATH = \"../../models/tarea4.pth\"\n",
        "torch.save(data, FILE_PATH)\n",
        "\n",
        "print(f'training complete. file saved to {FILE_PATH}')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYClbTtsMCjE"
      },
      "source": [
        "##### Chatbot üí¨"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c249zUwiMBxb",
        "outputId": "1b14ebaf-88aa-482c-c20c-317de34bf6c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Let's chat! (type 'finish_chat' to finish the chat)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GA-97: Hello, thanks for visiting.\n",
            "GA-97: Hey\n",
            "GA-97: I can talk and do things for you\n",
            "GA-97: I can talk and do things for you\n",
            "GA-97: I'm in doubt about that\n",
            "GA-97: Menu: Fuzzy Tauntaun, Bloody Rancor, Jedi Mind Trick, T-16 Skyhopper, Yub Nub, Jet Juice, Hyperdrive, Rancor Beer.\n",
            "GA-97: Whas is you problem?\n",
            "GA-97: Ask for jedi or sith or bounti hounter.\n",
            "GA-97: Luke Skywalker, Yoda, Obi-Wan Kenobi, Anakin Skywalker, Qui-Gon Jinn, Mace Windu, Ahsoka Tano, Plo Koon, Aalya Secura, Kit Fisto.\n",
            "GA-97: Here is top 10 sith you are looking for: Darth Vader, Darth Plagueis, Darth Revan, Darth Traya, Darth Sidious, Darth Maul, Ulic Qel-Droma, Asajj Ventress, Kylo Ren, Marka Ragnos.\n",
            "GA-97: Here is top 10 bounti hounter you are looking for: Jango Fett, Boba Fett, Cad Bane, Durge, Embo, Dengar, Black Krrsantan, IG-88, Aurra Sing, Sabine Wren.\n",
            "GA-97: You are a dumb person asking a machine about yourself\n",
            "GA-97: Mr. Fbravo is my creator.\n",
            "GA-97: A wise and intelligent man\n",
            "GA-97: You would get bored if I do so.\n",
            "GA-97: I'm in doubt about that\n",
            "GA-97: Maybe yes, maybe nop\n",
            "GA-97: Glad to help!\n",
            "GA-97: How does Darth Vader like his toast? On the Dark Side.\n",
            "GA-97: Luke Skywalker, Yoda, Obi-Wan Kenobi, Anakin Skywalker, Qui-Gon Jinn, Mace Windu, Ahsoka Tano, Plo Koon, Aalya Secura, Kit Fisto.\n",
            "GA-97: Hope will cath up sortly.\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "with open(star_wars_chatbot_path, 'r') as json_data:\n",
        "    intents = json.load(json_data)\n",
        "\n",
        "FILE_PATH = \"../../models/tarea4.pth\"\n",
        "data = torch.load(FILE_PATH)\n",
        "\n",
        "INPUT_SIZE = data[\"input_size\"]\n",
        "OUTPUT_SIZE = data[\"output_size\"]\n",
        "USE_CNN = data[\"use_cnn\"]\n",
        "labels = data['labels']\n",
        "model_state = data[\"model_state\"]\n",
        "\n",
        "model = CNNClassifier(INPUT_SIZE, num_classes=OUTPUT_SIZE, use_cnn=USE_CNN).to(device)\n",
        "model.load_state_dict(model_state)\n",
        "model.eval()\n",
        "\n",
        "# Dictionary with the answers\n",
        "responses = {key['tag']: key['responses'] for key in dataset['intents']}\n",
        "\n",
        "bot_name = \"GA-97\"\n",
        "print(\"Let's chat! (type 'finish_chat' to finish the chat)\")\n",
        "while True:\n",
        "    q_text = input(\"You: \")\n",
        "    if q_text == 'finish_chat':\n",
        "        break\n",
        "\n",
        "    # X = torch.tensor([vocab.stoi[t] for t in tokenizer(q_text)]).to(device)\n",
        "    X = torch.tensor([vocab[t] if t in stoi else 0 for t in tokenizer(q_text)]).to(device)\n",
        "    if X.shape[0] < model.cnn_kernel_size:\n",
        "        pad = torch.zeros((model.cnn_kernel_size - X.shape[0]), dtype=torch.long).to(X.device)\n",
        "        X = torch.concat([X, pad])\n",
        "    output = model(X, torch.tensor([0], dtype=torch.long).to(device))\n",
        "    _, predicted = torch.max(output, dim=1)\n",
        "\n",
        "    tag = labels[predicted.item()]\n",
        "\n",
        "    probs = torch.softmax(output, dim=1)\n",
        "    prob = probs[0][predicted.item()]\n",
        "    if prob.item() > 0.50:\n",
        "      print(f\"{bot_name}: {random.choice(responses[tag])}\")\n",
        "    else:\n",
        "      print(f\"{bot_name}: My model can't understand you...\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5Hu2QTuSURCt"
      },
      "source": [
        "#### Comente los resultados aqu√≠ (0,5 puntos)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fdFV63WVUX32"
      },
      "source": [
        ">El modelo es capaz de responder correctamente a todos los prompts con los que se ha entrenado, pues al ser un ejemplo introductorio no se ha verificado la existencia de overfitting, ni se ha separado la data de entrenamiento en train/val/test. En general suele entregar respuestas acorde a lo que se pide, excepto cuando existen 2 o m√°s tokens muy relacionado a alguna categoria. Por ejemplo, al usar la palabra \"Tell\" junto a \"joke\" es muy posible que el modelo se incline por determinar que la categor√≠a es \"story\", \"myself\" u otra en vez de \"funny\", entregando una respuesta distinta a la esperada."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
