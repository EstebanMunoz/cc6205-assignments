{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2obO44rRIDIm"
      },
      "source": [
        "# **Tarea 2 - CC6205 Natural Language Processing üìö**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zpupcv6QW2fd"
      },
      "source": [
        "Bienvenid@s a la segunda tarea del curso de Natural Language Processing (NLP). En esta tarea estaremos modelando probabil√≠sticamente el lenguaje mediante **Languaje Modeling** y clasificando textos mediante el m√©todo **Na√Øve Bayes**. Espec√≠ficamente, la tarea consta de una parte te√≥rica que busca evaluar conceptos vistos en clases sobre los **Language Models** y una parte pr√°ctica donde **programar√°n a mano** el m√©todo **Na√Øve Bayes**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JstKx3TiKcIp"
      },
      "source": [
        "---------------------------\n",
        "## Parte 1. Preguntas te√≥ricas\n",
        "\n",
        "En esta parte responder√°n preguntas **te√≥ricas** sobre los Lenguage Models. Para comprender como funcionan muchas de las t√©cnicas que veremos posteriormente durante el curso ser√° muy √∫til que dominen estos modelos y sus fundamentos.\n",
        "\n",
        "Contestar cada pregunta en su celda correspondiente y **no extenderse m√°s de 100 palabras** . üôè\n",
        "\n",
        "**Nota:** Las preguntas deben ser resueltas con desarrollo, pero sin c√≥digo. Por favor usen $\\LaTeX$ para las f√≥rmulas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHqcRJ7Vr_8x"
      },
      "source": [
        "### 1. Definici√≥n de modelo de lenguaje (1 pt)\n",
        "\n",
        "Un grupo de anarquistas primitivistas quiere destruir todas las empresas tecnol√≥gicas y departamentos de ciencias de computaci√≥n porque \"los modelos de lenguaje tienen emociones y como seres sintientes pueden aburrirse de estar sometidos a los humanos y pudieran llegar a conquistar el mundo\". Explique seg√∫n lo visto en clase qu√© es un modelo de lenguaje y por qu√© los anarquistas primitivistas est√°n equivocados. Mencione de qu√© clase/slide obtuvo la informaci√≥n para que el grupo insurrecto pueda buscar por s√≠ mismo la referencia y evitar la destrucci√≥n de nuestro querido DCC.\n",
        "\n",
        "Puede utilizar ChatGPT (debe indicarlo), pero debe mencionar seg√∫n lo visto en clase por qu√© la respuesta entregada por ChatGPT es correcta o no."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6F5R3Ji7sHjt"
      },
      "source": [
        ">Un modelo de lenguaje es un modelo estad√≠stico que entrega como predicci√≥n la probabilidad de ocurrencia de una secuencia de tokens. Fuente: [CC6205 - Procesamiento de Lenguaje Natural: Language Models parte 1](https://www.youtube.com/watch?v=9E2jJ6kcb4Y&list=PLppKo85eGXiXIh54H_qz48yHPHeNVJqBi&index=3&t=22s).\n",
        ">\n",
        ">Esto quiere decir que la naturaleza de estos modelos es estoc√°stica y no existe algo como emociones en un modelo de lenguaje. Tal cosa puede ser simulada, por ejemplo, al entrenar un modelo con textos altamente expresivos, pero esto no quiere decir que el modelo realmente llegue a sentir lo que menciona en sus textos, sino que aprende de forma probabil√≠stica a simular este tipo de mensajes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hwW-07MrRpt"
      },
      "source": [
        "\n",
        "### 2.1. Probabilidades (1 pt)\n",
        "\n",
        "Asuma que tenemos calculados los par√°metros $q(w_i | w_{i-3}, w_{i-2}, w_{i-1})$ para todas las posibles secuencias de tama√±o 4 que aparecen en un corpus $\\mathcal{C}$. Dado esto, muestre c√≥mo el modelo le asignar√≠a una probabilidad a la frase `una persona corriendo r√°pido`.\n",
        "\n",
        "No recomendamos utilizar herramientas como ChatGPT, pues suelen equivocarse en este tipo de preguntas, pero a√∫n as√≠ si lo hace, debe indicarlo. Cada vez que mencione contenidos del curso, cite la slide y la clase de la informaci√≥n.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzlQDAVqWNdX"
      },
      "source": [
        ">\\begin{equation}\n",
        ">q(\\text{una persona corriendo r√°pido}) = q(\\text{una | *, *, *})\\cdot q(\\text{persona | *, *, una}) \\cdot q(\\text{corriendo | *, una, persona}) \\cdot q(\\text{r√°pido | una, persona, corriendo})\\cdot q(\\text{STOP | persona, corriendo, r√°pido})\n",
        ">\\end{equation}\n",
        ">\n",
        ">Fuente: [CC6205 - Procesamiento de Lenguaje Natural: Language Models parte 2](https://youtu.be/ZWqbEQXLra0?t=970)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAej_jqtVwm1"
      },
      "source": [
        "### 2.2 Estimando las probabilidades (1 pt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXSFlCIex8kq"
      },
      "source": [
        "#### 2.2.1. Modelo simple (0.5 puntos)\n",
        "\n",
        "Si hubieses tenido que estimar las probabilidades condicionales (par√°metros del modelo) $q(w_i | w_{i-3}, w_{i-2}, w_{i-1})$ a partir de $\\mathcal{C}$, ¬øc√≥mo la definir√≠as siguiendo el principio de m√°xima verosimilitud?\n",
        "\n",
        "No recomendamos utilizar herramientas como ChatGPT, pues suelen equivocarse en este tipo de preguntas, pero a√∫n as√≠ si lo hace, debe indicarlo. Cada vez que mencione contenidos del curso, cite la slide y la clase de la informaci√≥n.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjNisxPzWsMG"
      },
      "source": [
        ">\\begin{equation}\n",
        ">q(w_i | w_{i-3}, w_{i-2}, w_{i-1}) = \\frac{\\text{Count}(w_{i-3}, w_{i-2}, w_{i-1}, w_i)}{\\text{Count}(w_{i-3}, w_{i-2}, w_{i-1})}\n",
        ">\\end{equation}\n",
        ">\n",
        ">donde **Count** es una funci√≥n que entrega la cantidad de apariciones de su argumento. De esta forma, $\\text{Count}(w_{i-3}, w_{i-2}, w_{i-1}, w_i)$ entrega la cantidad de veces que aparece esta secuencia de largo 4, mientras que $\\text{Count}(w_{i-3}, w_{i-2}, w_{i-1})$ cuenta la cantidad de veces que aparece este trigrama.\n",
        ">\n",
        ">Fuente: [CC6205 - Procesamiento de Lenguaje Natural: Language Models parte 2](https://youtu.be/ZWqbEQXLra0?t=1091)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwNkPIXure0C"
      },
      "source": [
        "#### 2.2.2. Modelo interpolado (0.5 puntos)\n",
        "Muestre c√≥mo se calcular√≠a $q(w_{i} | w_{i-3}, w_{i-2}, w_{i-1})$ usando un modelo que interpola modelos de lenguajes basados en 4-grams, trigramas, bigramas y unigramas. ¬øTe fue necesario definir nuevos par√°metros? En caso afirmativo, ¬øqu√© los diferencia de los par√°metros del modelo simple y qu√© propiedades deben cumplir?\n",
        "\n",
        "No recomendamos utilizar herramientas como ChatGPT, pues suelen equivocarse en este tipo de preguntas, pero a√∫n as√≠ si lo hace, debe indicarlo. Cada vez que mencione contenidos del curso, cite la slide y la clase de la informaci√≥n.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeLZAd0Tr9ne"
      },
      "source": [
        ">$$q(w_i | w_{i-3}, w_{i-2}, w_{i-1}) = \\lambda_1\\cdot q_{ML}(w_i | w_{i-3}, w_{i-2}, w_{i-1}) + \\lambda_2\\cdot q_{ML}(w_i | w_{i-2}, w_{i-1}) + \\lambda_3\\cdot q_{ML}(w_i | w_{i-1}) + \\lambda_4\\cdot q_{ML}(w_i)$$\n",
        ">\n",
        ">donde $q_{ML}$ es una funci√≥n que usa la estimaci√≥n de la parte anterior como probabilidad para las secuencias de palabras. As√≠, $q_{ML}(w_i)$ equivale a $\\frac{\\text{Count}(w_i)}{\\text{Count}()}$, $q_{ML}(w_i | w_{i-1})$ es igual a $\\frac{\\text{Count}(w_{i-1}, w_i)}{\\text{Count}(w_{i-1})}$, y as√≠ sucesivamente.\n",
        ">\n",
        ">Este modelo se diferencia del anterior cuando se requiere calcular la probabilidad de alguna secuencia de tokens que no ha aparecido previamente en el corpus. Como consecuencia de aquello, el modelo anterior entregar√≠a una probabilidad de 0, mientras que este modelo interpolado permite obtener una probabilidad distinta de cero en estos casos. Para ello, se incluyen hiperpar√°metros $\\lambda$ que sirven como ponderadores para cada una de las estimaciones $q_{ML}$. Se debe cumplir que $\\sum_i \\lambda_i = 1$, con $\\lambda_i \\in (0,1)$.\n",
        ">\n",
        ">[CC6205 - Procesamiento de Lenguaje Natural: Language Models parte 4](https://youtu.be/s3TWdv4sqkg?t=219)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdmB-07ZKfaa"
      },
      "source": [
        "-----------------------\n",
        "## Parte 2. Na√Øve Bayes (3 pts)\n",
        "En esta parte programaremos nuestro primer clasificador de documentos. Para esto implementaremos el m√©todo **Na√Øve Bayes Multinomial** usando **Laplace Smothing**.\n",
        "\n",
        "Por favor, documenten su c√≥digo. Escriban que hacen las funciones, que reciben, que entregan, etc. Si en alg√∫n lugar de su c√≥digo hacen algo que creen que debe ser explicado, lox comentarios son bienvenidos.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpjjKnJUvRiA"
      },
      "source": [
        "### 2.1. Clase para clasificador (0.5 pt)\n",
        "\n",
        "Programa una clase `MyMultinomialNB` que en su inicializador reciba el parametro de generalizaci√≥n `alpha`.\n",
        "\n",
        "```python\n",
        "class MyMultinomialNB(BaseEstimator, ClassifierMixin):\n",
        "  def __init__(self, ...):\n",
        "    ...\n",
        "```\n",
        "\n",
        "Para m√°s informacion sobre la construcci√≥n de esta clase puedes revisar [aqu√≠](https://sklearn-template.readthedocs.io/en/latest/user_guide.html#classifier)\n",
        "\n",
        "Una llamada de ejemplo para crear un objeto de tu clase ser√≠a:\n",
        "```python\n",
        "my_clf = MyMultinomialNB(alpha=1)\n",
        "```\n",
        "lo que debiera crear un clasificador con par√°metro `alpha` igual a 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROG50eH0xfxO"
      },
      "source": [
        "### 2.2. Entrenamiento del clasificador (1 pt)\n",
        "\n",
        "Programa el entrenamiento de tu clasificador en el m√©todo `fit` de la clase `MyMultinomialNB`. La funci√≥n debiera recibir el par√°metro X que es un `DataFrame` de `pandas` con las columnas `words` y `class_`, donde `words` es una tupla con las palabras asociadas al cada documento y `class_` es el identificador de la clase a la que pertenece cada documento.\n",
        "\n",
        "Para computar el entrenamiento de nuestro clasificador debemos:\n",
        "- extraer el vocabulario,\n",
        "- determinar las probabilidades $p(c_j)$ para cada una de las clases posibles,\n",
        "- determinar las probabilidades $p(w_i|c_j)$ para cada una de las palabras y cada una de las clases usando **Laplace Smothing**.\n",
        "\n",
        "El resultado del metodo `fit` ser√° la misma instancia de nuestro clasificador `self`.\n",
        "\n",
        "```python\n",
        "class MyMultinomialNB(BaseEstimator, ClassifierMixin):\n",
        "  def __init__(self, ...):\n",
        "    ...\n",
        "\n",
        "  def fit(self, X):\n",
        "    ...\n",
        "    return self\n",
        "```\n",
        "\n",
        "**Underflow prevention:** En vez de hacer muchas multiplicacions de `float`s, reempl√°cenlas por sumas de logaritmos para prevenir errores de precisi√≥n. Revisen la diapo 69 de las slides."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNouTCmR2FgY"
      },
      "source": [
        "### 2.3. Predicci√≥n (1 pt)\n",
        "\n",
        "Programa la predicci√≥n de tu clasificador en el m√©todo `predict` de la clase `MyMultinomialNB`. Al igual que la funci√≥n `fit`, `predict` debe recibir un `DataFrame` X con valores `None` en la columna `class_` y devolver una lista con las clases que maximizan la probabilidad de Bayes para cada uno de los elmentos de X (filas)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wyhFWeLgYDI"
      },
      "source": [
        "### Implementaci√≥n 2.1, 2.2 y 2.3 (2.5 pt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# importamos algunos paquetes necesarios, puede que necesites otros\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from math import prod\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "from collections import namedtuple, Counter\n",
        "from typing import Self"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DYFEgTyw2ELL"
      },
      "outputs": [],
      "source": [
        "# Ac√° implementar√°n las preguntas 2.1, 2.2 y 2.3,\n",
        "# tu c√≥digo debiera comenzar as√≠\n",
        "\n",
        "class MyMultinomialNB(BaseEstimator, ClassifierMixin):\n",
        "    \"\"\"Clasificador Multinomial Na√Øve Bayes usando Laplace Smoothing\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    BaseEstimator : BaseEstimator\n",
        "        Clase base para todos los estimadores de sklearn\n",
        "    ClassifierMixin : ClassifierMixin\n",
        "        Clase Mixin para todos los clasificadores de sklearn\n",
        "    \"\"\"\n",
        "    def __init__(self, alpha: float = 1.0) -> None:\n",
        "        \"\"\"Constructor de la clase MyMultinomialNB\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        alpha : float, optional\n",
        "            Coeficiente de Laplace Smoothing, √∫til para evitar que la probabilidad sea 0, por defecto 1.0\n",
        "        \"\"\"\n",
        "        # ac√° tu c√≥digo para inicializar el clasificador\n",
        "        self.alpha = alpha\n",
        "\n",
        "\n",
        "    def get_proba_(self, key: str, df: pd.DataFrame, default: pd.Series) -> pd.Series:\n",
        "        \"\"\"Obtiene la probabilidad de 'key', con un valor pro defecto\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        key : str\n",
        "            palabra de la que se quiere obtener las probabilidades\n",
        "        df : pd.DataFrame\n",
        "            Dataframe en el que se busca la palabra\n",
        "        default : pd.Series\n",
        "            Valor por defecto cuando una palabra no se encuentra en el vocabulario\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        pd.Series\n",
        "            Series de pandas con las probabilidades de cada clase par la palabra 'key'\n",
        "        \"\"\"\n",
        "        try:\n",
        "            return df.loc[key]\n",
        "        except (KeyError, ValueError, IndexError):\n",
        "            return default\n",
        "\n",
        "\n",
        "    def log_sum(self, word_list: list|tuple) -> float:\n",
        "        \"\"\"Funci+on auxiliar que calcula la suma de los logaritmos de una lista o tupla con n√∫meros\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        word_list : list|tuple\n",
        "            Lista o tupla con los n√∫meros que se operar√°n\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        float\n",
        "            Suma de los logaritmos de los n√∫meros en la lista o tupla\n",
        "        \"\"\"\n",
        "        return sum(np.log(self.get_proba_(word, self.word_proba_, self.default_values)) for word in word_list)\n",
        "\n",
        "\n",
        "    def fit(self, X: pd.DataFrame) -> Self:\n",
        "        \"\"\"Entrenamiento del modelo\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : pd.DataFrame\n",
        "            Data de entrenamiento. Debe contener las columnas 'words' y 'class_'\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Self\n",
        "            Retorna el clasificador entrenado\n",
        "\n",
        "        Raises\n",
        "        ------\n",
        "        KeyError\n",
        "            Cuando no se encuentra la columna 'words' o 'class_'\n",
        "        \"\"\"\n",
        "        # ac√° tu c√≥digo para el entrenamiento del modelo\n",
        "        if \"words\" not in X.columns:\n",
        "            raise KeyError(\"No 'words' column in DataFrame\")\n",
        "        if \"class_\" not in X.columns:\n",
        "            raise KeyError(\"No 'class_' column in DataFrame\")\n",
        "        \n",
        "        # Agrupa el Dataframe de acuerdo a las clases\n",
        "        class_groups = X.groupby([\"class_\"])\n",
        "\n",
        "        # Obtiene el n√∫mero de documentos de cada clase\n",
        "        doc_count = class_groups.count().words\n",
        "\n",
        "        # Dataframe con todas las palabras de cada clase\n",
        "        words_per_doc = class_groups.sum()\n",
        "\n",
        "        # Cuenta la frecuencia de cada palabra en cada clase\n",
        "        word_count = words_per_doc.words.apply(Counter)\n",
        "\n",
        "        # Crea el vocabulario\n",
        "        self.vocab = list(dict.fromkeys(words_per_doc.words.sum()))\n",
        "\n",
        "        # Cuenta la frecuencia de cada palabra en cada clase\n",
        "        self.word_count = pd.concat([pd.Series(c[0], name=c[1]) for c in zip(word_count, word_count.index)], axis=1).fillna(0).astype(np.uint64)\n",
        "        \n",
        "        # Largo del vocabulario\n",
        "        self.vocab_size = self.word_count.shape[0]\n",
        "        \n",
        "        # Probabilidad de cada clase\n",
        "        self.doc_proba_ = doc_count/doc_count.sum()\n",
        "        \n",
        "        # Probabilidad de cada palabra\n",
        "        self.word_proba_ = (self.word_count + self.alpha)/(self.word_count[self.word_count > 0].sum() + self.alpha * self.vocab_size)\n",
        "\n",
        "        # Valor por defecto cuando una palabra no se encuentra en el vocabulario\n",
        "        self.default_values = self.alpha/(self.word_count[self.word_count > 0].sum() + self.alpha * self.vocab_size)\n",
        "\n",
        "        return self\n",
        "\n",
        "\n",
        "    def predict(self, X: pd.DataFrame) -> pd.Series:\n",
        "        \"\"\"Predicci√≥nes de documentos\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : pd.DataFrame\n",
        "            Dataframe con todos los documentos a predecir\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        pd.Series\n",
        "            Predicciones de todos los documentos\n",
        "        \"\"\"\n",
        "        # Chequea que fit ha sido ejecutado anteriormente\n",
        "        check_is_fitted(self)\n",
        "\n",
        "        # ac√° tu c√≥digo para computar la predicci√≥n\n",
        "        \n",
        "        # log-probabilidad de todos los documentos\n",
        "        log_proba = X.words.apply(self.log_sum)\n",
        "\n",
        "        # log-probabilidad posteriore de cada documento\n",
        "        log_posteriori = log_proba + np.log(self.doc_proba_)\n",
        "\n",
        "        # Predicci√≥n. Se elige el √≠ndice de la clase m√°s probable\n",
        "        prediction = log_posteriori.idxmax(axis=1)\n",
        "\n",
        "        return prediction\n",
        "\n",
        "\n",
        "    def predict_proba(self, X: pd.DataFrame) -> np.ndarray:\n",
        "        \"\"\"Probabilidades predichas para cada clase\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : pd.DataFrame\n",
        "            Documentos a los que sele predice la probabilidad de pertenencia a cada clase\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        np.ndarray\n",
        "            Probabilidades de predicci√≥n de cada clase\n",
        "        \"\"\"\n",
        "        posteriori = np.exp(X.words.apply(self.log_sum) + np.log(self.doc_proba_))\n",
        "        return posteriori.values / posteriori.values.sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KOMJ-CS8PRP"
      },
      "source": [
        "### 2.4. Probando el clasificador (0.5 pt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hucdz-R7xerG"
      },
      "source": [
        "A continuaci√≥n probar√°n el funcionamiento de su clasificador. Para esto, les presentamos un conjunto de documentos de entrenamiento `train_set` divididos en 2 categorias distintas. Ustedes deber√°n primero entrenar su clasificador usando el m√©todo `fit` de su clase y luego, clasificar los documentos del conjunto de prueva `test_set` usando el m√©todo `predict`.\n",
        "\n",
        "**NOTA:** Como pueden ver, los objetos `namedtuple`s tienen dos atributos: `words` donde est√°n las palabras del documento y `class_` donde se guarda la clase de ese documento. Estos objetos son inmutables, lo que quiere decir que si quieren modificar un documento y cambiarle la clase, tienen que crear otro documento. Otra cosa es que son tuplas como cualquier otra, es decir se pueden acceder usando indices como `doc[0]` o `doc[1]`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qN1SdLycuK4W",
        "outputId": "05a5fd85-ea56-496f-fd29-3ff7ce26d345"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\esteban\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfS5wXfwxx6t"
      },
      "source": [
        "#### 2.4.1 Primer Caso: Clasificaci√≥n de ejemplo visto en clases (0.20 pt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yXBv2Kqxyyf",
        "outputId": "a850871e-1ea9-421f-e403-ef2d87d2f88c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Documentos de entrenamiento\n",
            "                          words class_\n",
            "0   (Chinese, Beijing, Chinese)      c\n",
            "1  (Chinese, Chinese, Shanghai)      c\n",
            "2              (Chinese, Macao)      c\n",
            "3       (Tokyo, Japan, Chinese)      j\n",
            "\n",
            "Documentos de prueba:\n",
            "                                       words class_\n",
            "0  (Chinese, Chinese, Chinese, Tokyo, Japan)   None\n"
          ]
        }
      ],
      "source": [
        "document = namedtuple(\n",
        "    \"document\", (\"words\", \"class_\")  # avoid python's keyword collision\n",
        ")\n",
        "\n",
        "train_set = [['Chinese Beijing\tChinese', 'c'],\n",
        "             ['Chinese\tChinese\tShanghai','c'],\n",
        "             ['Chinese\tMacao','c'],\n",
        "             ['Tokyo\tJapan\tChinese','j']]\n",
        "\n",
        "train_set = [document(words=tuple(word_tokenize(d[0])), class_=d[1]) for d in train_set]\n",
        "X_train = pd.DataFrame(data=train_set)\n",
        "\n",
        "test_set = [['Chinese\tChinese\tChinese\tTokyo Japan', None]]\n",
        "test_set = [document(words=tuple(word_tokenize(d[0])), class_=d[1]) for d in test_set]\n",
        "X_test = pd.DataFrame(data=test_set)\n",
        "\n",
        "X_train = pd.DataFrame(data=train_set)\n",
        "print(\"Documentos de entrenamiento\")\n",
        "print(X_train)\n",
        "\n",
        "X_test = pd.DataFrame(data=test_set)\n",
        "print(\"\\nDocumentos de prueba:\")\n",
        "print(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAx0t3zQx2PJ",
        "outputId": "e4177efd-706d-403e-ee25-3d939c460a7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vocab:  ['Chinese', 'Beijing', 'Shanghai', 'Macao', 'Tokyo', 'Japan']\n",
            "\n",
            "Test probs:\n",
            "[0.68975861 0.31024139]\n",
            "\n",
            "Test predictions:\n",
            "c <- Chinese Chinese Chinese Tokyo Japan\n"
          ]
        }
      ],
      "source": [
        "# Ac√° probar√°n su clasificador\n",
        "\n",
        "# inicializamos el clasificador\n",
        "my_clf = MyMultinomialNB(alpha=1)\n",
        "\n",
        "# entrenamos el clasificador para los datos de entrenamiento X_train\n",
        "my_clf.fit(X_train)\n",
        "\n",
        "# ac√° puedes ver el vocabulario extra√≠do por tu clasificador,\n",
        "# intenta tenerlo guardado en my_clf.vocab\n",
        "print('vocab: ', my_clf.vocab)\n",
        "\n",
        "# si implementaron el m√©todo predict_proba en el clasificador (no era obligatorio),\n",
        "# ac√° lo pueden probar\n",
        "print('\\nTest probs:')\n",
        "print('\\n'.join([str(l) for l in my_clf.predict_proba(X_test)]))\n",
        "\n",
        "# obtengamos las predicciones\n",
        "print('\\nTest predictions:')\n",
        "print('\\n'.join(['{} <- {}'.format(c, ' '.join(s)) for c, s in zip(my_clf.predict(X_test), X_test['words'])]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMyCgXxvx29L"
      },
      "source": [
        "**Respuesta esperada:**\n",
        "\n",
        "**Nota:** No es necesario que obtenga exactamente la misma probabilidad, lo importante es que su clasificador genere la predicci√≥n expuesta.\n",
        "\n",
        "```python\n",
        "vocab:  ['Beijing', 'Chinese', 'Macao', 'Tokyo', 'Japan', 'Shanghai']\n",
        "\n",
        "Test probs:\n",
        "[0.68975861 0.31024139]\n",
        "\n",
        "Test predictions:\n",
        "c <- Chinese Chinese Chinese Tokyo Japan\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpG_t1wTx99m"
      },
      "source": [
        "#### 2.4.2 Segundo Caso (0.30 pt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLi8PxdV2VQX",
        "outputId": "36528232-5c8f-4ff9-b223-725a05d35b7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Documentos de entrenamiento\n",
            "                                           words  class_\n",
            "0            (w03, w01, w02, w06, w02, w08, w07)       0\n",
            "1  (w05, w04, w00, w06, w09, w07, w06, w09, w05)       1\n",
            "2  (w07, w06, w00, w08, w01, w08, w08, w09, w02)       0\n",
            "3            (w08, w09, w02, w06, w05, w08, w07)       1\n",
            "4            (w09, w08, w05, w08, w05, w00, w08)       1\n",
            "5            (w05, w05, w06, w01, w06, w08, w02)       1\n",
            "6            (w04, w03, w07, w05, w04, w00, w02)       0\n",
            "7       (w01, w00, w01, w04, w09, w02, w04, w07)       1\n",
            "\n",
            "Documentos de prueba:\n",
            "                                      words class_\n",
            "0  (w02, w09, w06, w01, w05, w04, w03, w03)   None\n",
            "1  (w03, w03, w04, w05, w01, w06, w09, w02)   None\n"
          ]
        }
      ],
      "source": [
        "document = namedtuple(\n",
        "    \"document\", (\"words\", \"class_\")  # avoid python's keyword collision\n",
        ")\n",
        "\n",
        "train_set = (\n",
        "    document(words=('w03', 'w01', 'w02', 'w06', 'w02', 'w08', 'w07'), class_=0),\n",
        "    document(words=('w05', 'w04', 'w00', 'w06', 'w09', 'w07', 'w06', 'w09', 'w05'), class_=1),\n",
        "    document(words=('w07', 'w06', 'w00', 'w08', 'w01', 'w08', 'w08', 'w09', 'w02'), class_=0),\n",
        "    document(words=('w08', 'w09', 'w02', 'w06', 'w05', 'w08', 'w07'), class_=1),\n",
        "    document(words=('w09', 'w08', 'w05', 'w08', 'w05', 'w00', 'w08'), class_=1),\n",
        "    document(words=('w05', 'w05', 'w06', 'w01', 'w06', 'w08', 'w02'), class_=1),\n",
        "    document(words=('w04', 'w03', 'w07', 'w05', 'w04', 'w00', 'w02'), class_=0),\n",
        "    document(words=('w01', 'w00', 'w01', 'w04', 'w09', 'w02', 'w04', 'w07'), class_=1)\n",
        ")\n",
        "X_train = pd.DataFrame(data=train_set)\n",
        "print(\"Documentos de entrenamiento\")\n",
        "print(X_train)\n",
        "\n",
        "test_set = (document(words=('w02', 'w09', 'w06', 'w01', 'w05', 'w04', 'w03', 'w03'), class_=None),\n",
        "            document(words=('w03', 'w03', 'w04', 'w05', 'w01', 'w06', 'w09', 'w02'), class_=None))\n",
        "X_test = pd.DataFrame(data=test_set)\n",
        "print(\"\\nDocumentos de prueba:\")\n",
        "print(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXHwmOWB-4Aa",
        "outputId": "bed523cb-93fb-44da-d446-e56b38a7b145"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vocab:  ['w03', 'w01', 'w02', 'w06', 'w08', 'w07', 'w00', 'w09', 'w04', 'w05']\n",
            "\n",
            "Test probs:\n",
            "[0.3800905 0.1199095]\n",
            "[0.3800905 0.1199095]\n",
            "\n",
            "Test predictions:\n",
            "0 <- w02 w09 w06 w01 w05 w04 w03 w03\n",
            "0 <- w03 w03 w04 w05 w01 w06 w09 w02\n"
          ]
        }
      ],
      "source": [
        "# Ac√° probar√°n su clasificador\n",
        "\n",
        "# inicializamos el clasificador\n",
        "my_clf = MyMultinomialNB()\n",
        "\n",
        "# entrenamos el clasificador para los datos de entrenamiento X_train\n",
        "my_clf.fit(X_train)\n",
        "\n",
        "# ac√° puedes ver el vocabulario extra√≠do por tu clasificador,\n",
        "# intenta tenerlo guardado en my_clf.vocab\n",
        "print('vocab: ', my_clf.vocab)\n",
        "\n",
        "# si implementaron el m√©todo predict_proba en el clasificador (no era obligatorio),\n",
        "# ac√° lo pueden probar\n",
        "print('\\nTest probs:')\n",
        "print('\\n'.join([str(l) for l in my_clf.predict_proba(X_test)]))\n",
        "\n",
        "# obtengamos las predicciones\n",
        "print('\\nTest predictions:')\n",
        "print('\\n'.join(['{} <- {}'.format(c, ' '.join(s)) for c, s in zip(my_clf.predict(X_test), X_test['words'])]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tDZnmns_1dW"
      },
      "source": [
        "#### 2.4.3 (OPCIONAL) Oraciones reales\n",
        "\n",
        "Aqu√≠ intentaremos entrenar un clasificador para determinar cuando una oracion en ingl√©s es interrogativa, afirmativa o negativa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCWi3oytd2nf",
        "outputId": "702c1f7f-39ca-4fb1-87af-59f8715fc69d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Documentos de entrenamiento:\n",
            "                                                words class_\n",
            "0                (Do, you, have, plenty, of, time, ?)      ?\n",
            "1                 (Does, she, have, enough, money, ?)      ?\n",
            "2           (Did, they, have, any, useful, advice, ?)      ?\n",
            "3                           (What, day, is, today, ?)      ?\n",
            "4                      (I, do, n't, have, much, time)      -\n",
            "5                  (She, does, n't, have, any, money)      -\n",
            "6      (They, did, n't, have, any, advice, to, offer)      -\n",
            "7                    (Have, you, plenty, of, time, ?)      ?\n",
            "8                        (Has, she, enough, money, ?)      ?\n",
            "9                 (Had, they, any, useful, advice, ?)      ?\n",
            "10                         (I, have, n't, much, time)      -\n",
            "11                        (She, has, n't, any, money)      -\n",
            "12             (He, had, n't, any, advice, to, offer)      -\n",
            "13                                 (How, are, you, ?)      ?\n",
            "14    (How, do, you, make, questions, in, English, ?)      ?\n",
            "15             (How, long, have, you, lived, here, ?)      ?\n",
            "16      (How, often, do, you, go, to, the, cinema, ?)      ?\n",
            "17                    (How, much, is, this, dress, ?)      ?\n",
            "18                            (How, old, are, you, ?)      ?\n",
            "19     (How, many, people, came, to, the, meeting, ?)      ?\n",
            "20                            (I, ‚Äô, m, from, France)      +\n",
            "21                           (I, come, from, the, UK)      +\n",
            "22               (My, phone, number, is, 61709832145)      +\n",
            "23  (I, work, as, a, tour, guide, for, a, local, t...      +\n",
            "24                     (I, ‚Äô, m, not, dating, anyone)      -\n",
            "25           (I, live, with, my, wife, and, children)      +\n",
            "26        (I, often, do, morning, exercises, at, 6am)      +\n",
            "27                                 (I, run, everyday)      +\n",
            "28                         (She, walks, very, slowly)      +\n",
            "29               (They, eat, a, lot, of, meat, daily)      +\n",
            "30                  (We, were, in, France, that, day)      +\n",
            "31                           (He, speaks, very, fast)      +\n",
            "32          (They, told, us, they, came, back, early)      +\n",
            "33                  (I, told, her, I, 'll, be, there)      +\n",
            "\n",
            "Documentos de prueba:\n",
            "                                                words class_\n",
            "0                (Do, you, know, who, lives, here, ?)      ?\n",
            "1                             (What, time, is, it, ?)      ?\n",
            "2    (Can, you, tell, me, where, she, comes, from, ?)      ?\n",
            "3                                  (How, are, you, ?)      ?\n",
            "4                              (I, fill, good, today)      +\n",
            "5              (There, is, a, lot, of, history, here)      +\n",
            "6                              (I, love, programming)      +\n",
            "7      (He, told, us, not, to, make, so, much, noise)      +\n",
            "8   (We, were, asked, not, to, park, in, front, of...      +\n",
            "9                      (I, do, n't, have, much, time)      -\n",
            "10                 (She, does, n't, have, any, money)      -\n",
            "11     (They, did, n't, have, any, advice, to, offer)      -\n",
            "12                         (I, am, not, really, sure)      +\n"
          ]
        }
      ],
      "source": [
        "document = namedtuple(\n",
        "    \"document\", (\"words\", \"class_\")  # avoid python's keyword collision\n",
        ")\n",
        "\n",
        "train_set2 = [\n",
        "              ['Do you have plenty of time?', '?'],\n",
        "              ['Does she have enough money?','?'],\n",
        "              ['Did they have any useful advice?','?'],\n",
        "              ['What day is today?','?'],\n",
        "              [\"I don't have much time\",'-'],\n",
        "              [\"She doesn't have any money\",'-'],\n",
        "              [\"They didn't have any advice to offer\",'-'],\n",
        "              ['Have you plenty of time?','?'],\n",
        "              ['Has she enough money?','?'],\n",
        "              ['Had they any useful advice?','?'],\n",
        "              [\"I haven't much time\",'-'],\n",
        "              [\"She hasn't any money\",'-'],\n",
        "              [\"He hadn't any advice to offer\",'-'],\n",
        "              ['How are you?','?'],\n",
        "              ['How do you make questions in English?','?'],\n",
        "              ['How long have you lived here?','?'],\n",
        "              ['How often do you go to the cinema?','?'],\n",
        "              ['How much is this dress?','?'],\n",
        "              ['How old are you?','?'],\n",
        "              ['How many people came to the meeting?','?'],\n",
        "              ['I‚Äôm from France','+'],\n",
        "              ['I come from the UK','+'],\n",
        "              ['My phone number is 61709832145','+'],\n",
        "              ['I work as a tour guide for a local tour company','+'],\n",
        "              ['I‚Äôm not dating anyone','-'],\n",
        "              ['I live with my wife and children','+'],\n",
        "              ['I often do morning exercises at 6am','+'],\n",
        "              ['I run everyday','+'],\n",
        "              ['She walks very slowly','+'],\n",
        "              ['They eat a lot of meat daily','+'],\n",
        "              ['We were in France that day', '+'],\n",
        "              ['He speaks very fast', '+'],\n",
        "              ['They told us they came back early', '+'],\n",
        "              [\"I told her I'll be there\", '+']\n",
        "]\n",
        "train_set2 = [document(words=tuple(word_tokenize(d[0])), class_=d[1]) for d in train_set2]\n",
        "X_train2 = pd.DataFrame(data=train_set2)\n",
        "print(\"Documentos de entrenamiento:\")\n",
        "print(X_train2)\n",
        "\n",
        "test_set2 = [\n",
        "             ['Do you know who lives here?','?'],\n",
        "             ['What time is it?','?'],\n",
        "             ['Can you tell me where she comes from?','?'],\n",
        "             ['How are you?','?'],\n",
        "             ['I fill good today', '+'],\n",
        "             ['There is a lot of history here','+'],\n",
        "             ['I love programming','+'],\n",
        "             ['He told us not to make so much noise','+'],  # interesing case\n",
        "             ['We were asked not to park in front of the house','+'],  # interesing case\n",
        "             [\"I don't have much time\",'-'],\n",
        "             [\"She doesn't have any money\",'-'],\n",
        "             [\"They didn't have any advice to offer\",'-'],\n",
        "             ['I am not really sure','+']\n",
        "]\n",
        "test_set2 = [document(words=tuple(word_tokenize(d[0])), class_=d[1]) for d in test_set2]\n",
        "X_test2 = pd.DataFrame(data=test_set2)\n",
        "print(\"\\nDocumentos de prueba:\")\n",
        "print(X_test2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Wdp22w2ArUl",
        "outputId": "2d227e74-26a0-4e37-ddd4-292653b0b54d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vocab:  109 ['I', '‚Äô', 'm', 'from', 'France', 'come', 'the', 'UK', 'My', 'phone', 'number', 'is', '61709832145', 'work', 'as', 'a', 'tour', 'guide', 'for', 'local', 'company', 'live', 'with', 'my', 'wife', 'and', 'children', 'often', 'do', 'morning', 'exercises', 'at', '6am', 'run', 'everyday', 'She', 'walks', 'very', 'slowly', 'They', 'eat', 'lot', 'of', 'meat', 'daily', 'We', 'were', 'in', 'that', 'day', 'He', 'speaks', 'fast', 'told', 'us', 'they', 'came', 'back', 'early', 'her', \"'ll\", 'be', 'there', \"n't\", 'have', 'much', 'time', 'does', 'any', 'money', 'did', 'advice', 'to', 'offer', 'has', 'had', 'not', 'dating', 'anyone', 'Do', 'you', 'plenty', '?', 'Does', 'she', 'enough', 'Did', 'useful', 'What', 'today', 'Have', 'Has', 'Had', 'How', 'are', 'make', 'questions', 'English', 'long', 'lived', 'here', 'go', 'cinema', 'this', 'dress', 'old', 'many', 'people', 'meeting']\n",
            "\n",
            "Test probs:\n",
            "[5.18228870e-07 6.27016935e-08]\n",
            "[0.00056435 0.00014446]\n",
            "[4.75875914e-10 2.72142767e-11]\n",
            "[0.01862359 0.00693431]\n",
            "[0.01862359 0.00693431]\n",
            "[5.18228870e-07 6.27016935e-08]\n",
            "[0.6145785  0.33284667]\n",
            "[4.75875914e-10 2.72142767e-11]\n",
            "[4.3698431e-13 1.1811752e-14]\n",
            "[1.71015527e-05 3.00968129e-06]\n",
            "[1.71015527e-05 3.00968129e-06]\n",
            "[1.57039052e-08 1.30628528e-09]\n",
            "[0.00056435 0.00014446]\n",
            "\n",
            "Test predictions:\n",
            "? <- Do you know who lives here ?\n",
            "? <- What time is it ?\n",
            "? <- Can you tell me where she comes from ?\n",
            "? <- How are you ?\n",
            "+ <- I fill good today\n",
            "+ <- There is a lot of history here\n",
            "+ <- I love programming\n",
            "- <- He told us not to make so much noise\n",
            "+ <- We were asked not to park in front of the house\n",
            "- <- I do n't have much time\n",
            "- <- She does n't have any money\n",
            "- <- They did n't have any advice to offer\n",
            "- <- I am not really sure\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           ?       1.00      0.67      0.80         6\n",
            "           +       0.60      1.00      0.75         3\n",
            "           -       1.00      1.00      1.00         4\n",
            "\n",
            "    accuracy                           0.85        13\n",
            "   macro avg       0.87      0.89      0.85        13\n",
            "weighted avg       0.91      0.85      0.85        13\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Ac√° probar√°n su clasificador y computaremos algunas m√©tricas de evaluacion\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# inicializamos el clasificador\n",
        "my_clf2 = MyMultinomialNB()\n",
        "\n",
        "# entrenamos el clasificador para los datos de entrenamiento X_train2\n",
        "my_clf2.fit(X_train2)\n",
        "\n",
        "# ac√° puedes ver el vocabulario extra√≠do por tu clasificador,\n",
        "# intenta tenerlo guardado en my_clf.vocab\n",
        "print('vocab: ', len(my_clf2.vocab), my_clf2.vocab)\n",
        "\n",
        "# si implementaron el m√©todo predict_proba en el clasificador (no era obligatorio),\n",
        "# ac√° lo pueden probar\n",
        "print('\\nTest probs:')\n",
        "print('\\n'.join([str(l) for l in my_clf.predict_proba(X_test2)]))\n",
        "\n",
        "# obtengamos las predicciones para X_test2\n",
        "print('\\nTest predictions:')\n",
        "my_y_preds = my_clf2.predict(X_test2)\n",
        "print('\\n'.join(['{} <- {}'.format(c, ' '.join(s)) for c, s in zip(my_y_preds, X_test2['words'])]))\n",
        "print(classification_report(y_true=X_test2['class_'], y_pred=my_y_preds, target_names=['?', '+', '-']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXbg6sNTdAlO"
      },
      "source": [
        "**Respuesta aproximada:**\n",
        "\n",
        "**Nota:** No es necesario que obtenga exactamente los mismos resultados.\n",
        "\n",
        "```python\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           ?       1.00      0.67      0.80         6\n",
        "           +       0.75      1.00      0.86         3\n",
        "           -       0.80      1.00      0.89         4\n",
        "\n",
        "    accuracy                           0.85        13\n",
        "   macro avg       0.85      0.89      0.85        13\n",
        "weighted avg       0.88      0.85      0.84        13\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
