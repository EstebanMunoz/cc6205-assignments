{"cells":[{"cell_type":"markdown","metadata":{"id":"Ckbt7VPDhBwb"},"source":["# **Tarea 3 - Word Embeddings 游닄**"]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2020-03-19T18:30:18.109327Z","start_time":"2020-03-19T18:30:18.103344Z"},"id":"q5CSRY4oNCHK"},"source":["\n","**Instrucciones:**\n","- El ejercicio consiste en:\n","    - Responder preguntas relativas a los contenidos vistos en los v칤deos y slides de las clases.\n","    - Implementar el m칠todo de la Word Context Matrix. \n","    - Entrenar Word2Vec y FastText sobre un peque침o corpus.\n","    - Evaluar los embeddings obtenidos en una tarea de clasificaci칩n."]},{"cell_type":"markdown","metadata":{"id":"G4wYf0vgnbTv"},"source":["## **Preguntas te칩ricas 游늿 (3 puntos).** ##\n","Para estas preguntas no es necesario implementar c칩digo, pero pueden utilizar pseudo c칩digo."]},{"cell_type":"markdown","metadata":{"id":"B5hUG6-8ngoK"},"source":["### **Parte 1: Modelos Lineales (1.5 ptos)**"]},{"cell_type":"markdown","metadata":{"id":"5yRvZbhsoi8f"},"source":["Suponga que tiene un dataset de 10.000 documentos etiquetados por 4 categor칤as: pol칤tica, deporte, negocios y otros. "]},{"cell_type":"markdown","metadata":{"id":"irsqBVmCnx3M"},"source":["**Pregunta 1**: Dise침e un modelo lineal capaz de clasificar un documento seg칰n estas categor칤as donde el output sea un vector con una distribuci칩n de probabilidad con la pertenencia a cada clase. \n","\n","Especifique: representaci칩n de los documentos de entrada, par치metros del modelo, transformaciones necesarias para obtener la probabilidad de cada etiqueta y funci칩n de p칠rdida escogida. **(0.75 puntos)**"]},{"cell_type":"markdown","metadata":{},"source":[">El modelo consiste en un perceptr칩n de 1 capa, con 4 neuronas (una para cada clase). La representaci칩n de los documentos consiste en el promedio de los embedding de cada token presente en el documento. Estos embeddings se pueden conseguir de un modelo pre entrenado, y para prop칩sitos de este ejemplo puede ser de tama침o 300.\n",">\n",">Los par치metros presentes en el modelo son la matriz de pesos $W$ y el vector de bias $b$, de tama침o 300x4 y 4 respectivamente. Para obtener una distribuci칩n de probabilidad sobre la salida se utiliza una funci칩n de activaci칩n Softmax. Finalmente, se utiliza una funci칩n de p칠rdidas categorical cross entropy loss y una heur칤stica SGD para encontrar el 칩ptimo con los datos de entrenamiento (embeddings y etiquetas)"]},{"cell_type":"markdown","metadata":{"id":"G5FaWqBVvL90"},"source":["**Pregunta 2**: Explique c칩mo funciona el proceso de entrenamiento en este tipo de modelos y su evaluaci칩n. **(0.75 puntos)**"]},{"cell_type":"markdown","metadata":{},"source":[">Los modelos como el dise침ado previamente, requieren de un input con una dimensionalidad definida (generalmente llamadas features) y una clase o etiqueta que indica la respuesta correcta. Al entregar el input al modelo, se obtiene una respuesta que luego se compara con la respuesta correcta, que puede ser un vector de la dimensionalidad del output o un 칰nico valor num칠rico. En este 칰ltimo caso, el output que entrega el modelo se transforma, por ejemplo, al obtener el 칤ndice del n칰mero m치s grande (que en este caso, indica la clase m치s probable).\n",">\n",">Una vez que el output y la etiqueta tengan la misma dimensionalidad, se utiliza alguna funci칩n (conoocida como _loss function_) para comparar qu칠 tan parecidas o cercanas son las respuestas. Usualmente, se recurre a funciones como MSE o Cross Entropy, aunque existen m치s funciones que se pueden usar como una funci칩n de p칠rdidas.\n",">\n",">Te칩ricamente, si la funci칩n de p칠rdidas entrega un valor igual a 0, significa que el modelo ha entregado la respuesta correcta. Generalmente este no es el caso, por lo que se usa este valor para actualizar los par치metros del modelo con el objetivo de minimizar la funci칩n de p칠rdidas, de modo que en una futura utilizaci칩n del modelo, est칠 m치s cerca de entregar la respuesta correcta. La actualizaci칩n de los par치metros se realiza por medio de alguna variante de SGD, como ADAM o RMSProp. Estos algoritmos usan el valor obtenido por la funci칩n de p칠rdidas para actualizar los par치metros de la 칰ltima capa del modelo, y de forma recursiva retroceden por las capas del modelo actualizando par치metros gracias a la obtenci칩n de los gradientes entregados por _back propagation_.\n",">\n",">Este procedimiento de usar el modelo, obtener una estimaci칩n, calcular la distancia entre la estimaci칩n y la respuesta, y usar la distancia para actualizar par치metros se puede realizar para cada input, o de forma simult치nea para un conjunto de inputs, a los que se les denomina _batch_. En caso de utilizar batches, la actualizaci칩n de par치metros no se realiza de forma independiente para cada input, sino que se usa una funci칩n de agregaci칩n (como el promedio) sobre todas las distancias del batch, y este 칰ltimo valor es el que se utiliza para actualizar los par치metros del modelo por SGD.\n",">\n",">En cuanto a la evaluaci칩n, se suele separar un conjunto de prueba o test del conjunto con el que ha entrenado el modelo, con el objetivo de verificar que tiene la capacidad de entregar la respuesta correcta cuando se usan datos que no han sido vistos por el modelo. Dependiendo del problema, se usan distintas m칠tricas para determinar la calidad del modelo, como el accuracy, precision, recall, f1-score, entre otros. Tambi칠n es usual la utilizaci칩n de otro conjunto de datos (conocido como conjunto de validaci칩n), para determinar que durante el entrenamiento el modelo est치 efectivamente aprendiendo, en lugar de, por ejemplo, presentar sobreajuste y no tener la capacidad de inferir una respuesta para datos no vistos."]},{"cell_type":"markdown","metadata":{"id":"XkK7pc54njZq"},"source":["### **Parte 2: Redes Neuronales (1.5 ptos)** "]},{"cell_type":"markdown","metadata":{"id":"VUbJjlj_9AFC"},"source":["Supongamos que tenemos la siguiente red neuronal."]},{"cell_type":"markdown","metadata":{"id":"obUfuOYB_TOC"},"source":["![Red Neuronal](../../assets/tarea3-neural-network.png)"]},{"cell_type":"markdown","metadata":{"id":"s2z-8zKW0_6q"},"source":["**Pregunta 1**: En clases les explicaron como se puede representar una red neuronal de una y dos capas de manera matem치tica. Dada la red neuronal anterior, defina la salida $\\vec{\\hat{y}}$ en funci칩n del vector $\\vec{x}$, pesos $W^i$, bias $b^i$ y funciones $g,f,h$. \n","\n","Adicionalmente liste y explicite las dimensiones de cada matriz y vector involucrado en la red neuronal. **(0.75 Puntos)**"]},{"cell_type":"markdown","metadata":{},"source":[">Formula:\n",">$$ \\vec{\\hat{y}} = NN_{MLP3}(\\vec{x}) = g(g(g(\\vec{x}W^1 + \\vec{b^1})W^2 + b^2)W^3 + b^3)W^4 + b^4 $$\n",">\n",">Dimensiones:\n",">$$ \\vec{x} \\in \\mathbb{R^3} \\\\\n","    W^1 \\in \\mathbb{R^3} \\times \\mathbb{R^2} \\\\\n","    \\vec{b^1} \\in \\mathbb{R^2} \\\\\n","    W^2 \\in \\mathbb{R^2} \\times \\mathbb{R^3} \\\\\n","    \\vec{b^2} \\in \\mathbb{R^3} \\\\\n","    W^3 \\in \\mathbb{R^3} \\times \\mathbb{R} \\\\\n","    \\vec{b^3} \\in \\mathbb{R} \\\\\n","    W^4 \\in \\mathbb{R} \\times \\mathbb{R^4} \\\\\n","    \\vec{b^4} \\in \\mathbb{R^4} \\\\ $$"]},{"cell_type":"markdown","metadata":{},"source":["**Pregunta 2**: Explique qu칠 es backpropagation. 쮺uales ser칤an los par치metros a evaluar en la red neuronal anterior durante backpropagation? **(0.25 puntos)**"]},{"cell_type":"markdown","metadata":{},"source":[">Back propagation es un algoritmo que calcula el gradiente de la funci칩n de p칠rdidas, con el objetivo de luego actualizar los par치metros del modelo para minimizar el valor de la funci칩n de p칠rdidas. Para ello, se calcula el gradiente en funci칩n de los par치metros $W$ y $b$ de la capa de salida, para luego usar la regla de la cadena y as칤 obtener el gradiente de las capas anteriores en funci칩n de los gradientes ya calculados."]},{"cell_type":"markdown","metadata":{},"source":["**Pregunta 3**: Explique los pasos de backpropagation. En la red neuronal anterior: Cuales son las derivadas que debemos calcular para poder obtener $\\vec{\\delta^l_{[j]}}$ en todas las capas? **(0.5 puntos)**"]},{"cell_type":"markdown","metadata":{},"source":[">Como el nombre lo indica, backpropagation es un algoritmo que retropropaga el c치lculo del gradiente a tr치ves de las capas del modelo. De forma general, para una capa $l$ con matriz de pesos $W$, el valor del gradiente es:\n",">\n",">$$ \\frac{\\partial L}{\\partial W^l_{i,j}} = \\frac{\\partial L}{\\partial h^l_{j}} \\cdot \\frac{\\partial h^l_{j}}{\\partial W^l_{i,j}} $$\n",">\n",">En donde $h^l_{j}$ es la salida de la unidad $j$ en la capa $l$, y se calcula como\n",">$$ h^l_{j} = \\sum_{i} W^l_{i,j} \\cdot g(h^{l-1}_j) $$\n",">\n",">Cada peso $b$ puede ser a침adido a la sumatoria anterior si se introduce una unidad extra con un valor fijo igual a $1$. Luego, al introducir la notaci칩n delta y obtener la segunda derivada parcial de la primera expresi칩n:\n",">\n",">$$ \\delta^l_j = \\frac{\\partial L}{\\partial h^l_{j}} $$\n",">$$ \\frac{\\partial h^l_{j}}{\\partial W^l_{i,j}} = g(h^{l-1}_j) $$\n",">\n",">De modo que el gradiente resulta en\n",">\n",">$$ \\frac{\\partial L}{\\partial W^l_{i,j}} = \\delta^l_j \\cdot g(h^{l-1}_j) $$\n",">\n",">En la 칰ltima capa del modelo el c치lculo de $\\delta$ es directo, por lo que s칩lo se debe derivar la funci칩n de p칠rdidas. Para el ejemplo, ser칤an las 4 derivadas $\\frac{\\partial L}{\\partial h^4_{j}}$, con $j \\in \\{1,2,3,4\\}$. Para las capas ocultas el c치lculo es\n",">\n",">$$\n",">\\begin{align*}\n",">\\delta^l_j & = \\sum_k \\frac{\\partial L}{\\partial h^{l+1}_{k}} \\cdot \\frac{\\partial h^{l+1}_{k}}{\\partial h^l_{j}} \\\\\n",">\\delta^l_j & = \\sum_k \\delta^{l+1}_k \\cdot \\frac{\\partial h^{l+1}_{k}}{\\partial h^l_{j}} \\\\\n",">\\delta^l_j & = \\sum_k \\delta^{l+1}_k \\cdot W^{l+1}_{j,k} \\cdot g'(h^l_j) \\\\\n",">\\delta^l_j & = g'(h^l_j) \\sum_k \\delta^{l+1}_k \\cdot W^{l+1}_{j,k}\n",">\\end{align*}\n",">$$\n",">\n",">En el ejemplo, la capa 3 del modelo pueder usar los valores $\\delta^4$ para calcular $\\delta^3$, y as칤 retroceder hasta tener los gradientes de la primera capa."]},{"cell_type":"markdown","metadata":{"id":"ocS_vQhR1gcU"},"source":["## **Preguntas pr치cticas 游눹 (3 puntos).** ##"]},{"cell_type":"markdown","metadata":{"id":"D0wk5GBkSE73"},"source":["### Parte 3 A (1 Punto): Word Contex Matrix"]},{"cell_type":"markdown","metadata":{"id":"e_mh12Z9SF-J"},"source":["\n","\n","En esta parte debe crear una matriz palabra contexto, para esto, complete el siguiente template (para esta parte puede utilizar las librer칤as ```numpy``` y/o ```scipy```). Hint: revise como utilizar matrices sparse de ```scipy```"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\esteban\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["from scipy.sparse import dok_array\n","from typing import Callable\n","import numpy as np\n","import pandas as pd\n","import nltk\n","from nltk.tokenize import word_tokenize\n","\n","nltk.download('punkt')"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["class WordContextMatrix:\n","    \"\"\"Clase que construye la matriz palabra contexto\n","    \"\"\"\n","    def __init__(self, vocab_size: int, window_size: int, dataset: list[str], tokenizer: Callable[[str, ...], list[str]]) -> None:\n","        \"\"\"Constructor de la clase\n","\n","        Parameters\n","        ----------\n","        vocab_size : int\n","            tama침o m치ximo del vocabulario\n","        window_size : int\n","            Tama침o de la ventana contextual\n","        dataset : list[str]\n","            Conjunto de documentos a procesar\n","        tokenizer : Callable[[str, ...], list[str]]\n","            tokenizador usado sobre el dataset\n","        \"\"\"\n","\n","        # se sugiere agregar un una estructura de datos para guardar las\n","        # palabras del vocab y para guardar el conteo de coocurrencia\n","        # si lo necesita puede agregar m치s parametros pero no puede cambiar el resto\n","        self.vocab_size = vocab_size\n","        self.window_size = window_size\n","        self.dataset = dataset\n","        self.tokenizer = tokenizer\n","\n","        self.tokenized_dataset = [self.tokenizer(doc.lower()) for doc in self.dataset]\n","\n","    def build_vocab(self) -> None:\n","        \"\"\"Construye el vocabulario a partir del dataset\n","\n","        Raises\n","        ------\n","        ValueError\n","            Si el vocabulario del dataset es mayor que el tama침o maximo permitido\n","        \"\"\"\n","\n","        # Le puede ser 칰til considerar un token unk al vocab\n","        # para palabras fuera del vocab\n","        vocab = set(token for doc in self.tokenized_dataset for token in doc)\n","        stoi = {token: i for i, token in enumerate(vocab)}\n","        if len(stoi) > self.vocab_size:\n","            raise ValueError(f\"dataset vocabulary is larger than vocab_size, {len(vocab)} > {self.vocab_size}.\")\n","\n","        self.vocab = list(stoi.keys())\n","        self.stoi = stoi\n","\n","    \n","    def build_matrix(self) -> dok_array:\n","        \"\"\"Construye la matriz palabra contexto a partir del dataset\n","\n","        Returns\n","        -------\n","        dok_array\n","            matriz dispersa con los valores de la coocurrencia\n","        \"\"\"\n","        self.word_context_matrix = dok_array((self.vocab_size, self.vocab_size), dtype=np.uint16)\n","        for doc in self.tokenized_dataset:\n","            for i, token in enumerate(doc):\n","                token_idx = self.stoi[token]\n","                for j in range(i + 1, min(i + self.window_size + 1, len(doc))):\n","                    context_idx = self.stoi[doc[j]]\n","                    self.word_context_matrix[token_idx, context_idx] += 1\n","                    self.word_context_matrix[context_idx, token_idx] += 1\n","\n","        return self.word_context_matrix\n","\n","    @property\n","    def get_matrix(self) -> dict[str, np.ndarray]:\n","        \"\"\"Obtiene un diccionario que representa la matriz palabra contexto\n","\n","        Returns\n","        -------\n","        dict[str, np.ndarray]\n","            Diccionario en donde cada 'key' es un token del vocabulario y el valor es el embedding del token\n","        \"\"\"\n","\n","        # se recomienda transformar la matrix a un diccionario de embedding.\n","        # por ejemplo {palabra1:vec1, palabra2:vec2, ...}\n","        return {self.vocab[i]: self.word_context_matrix[i:i+1, :len(self.stoi)].toarray().flatten() for i in self.stoi.values()}"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>i</th>\n","      <th>like</th>\n","      <th>enjoy</th>\n","      <th>deep</th>\n","      <th>learning</th>\n","      <th>nlp</th>\n","      <th>flying</th>\n","      <th>.</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>i</th>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>like</th>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>enjoy</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>deep</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>learning</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>nlp</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>flying</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>.</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          i  like  enjoy  deep  learning  nlp  flying  .\n","i         0     2      1     0         0    0       0  0\n","like      2     0      0     1         0    1       0  0\n","enjoy     1     0      0     0         0    0       1  0\n","deep      0     1      0     0         1    0       0  0\n","learning  0     0      0     1         0    0       0  1\n","nlp       0     1      0     0         0    0       0  1\n","flying    0     0      1     0         0    0       0  1\n",".         0     0      0     0         1    1       1  0"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["corpus = [\n","  \"I like deep learning.\",\n","  \"I like NLP.\",\n","  \"I enjoy flying.\"\n","]\n","\n","wcm = WordContextMatrix(vocab_size=50, window_size=1, dataset=corpus, tokenizer=word_tokenize)\n","wcm.build_vocab()\n","sparse = wcm.build_matrix()\n","\n","df = pd.DataFrame(wcm.get_matrix, index=wcm.vocab)\n","new_order = [\"i\", \"like\", \"enjoy\", \"deep\", \"learning\", \"nlp\", \"flying\", \".\"]\n","df.reindex(index=new_order, columns=new_order)"]},{"cell_type":"markdown","metadata":{},"source":["Puede modificar los par치metros o m칠todos si lo considera necesario. Para probar la matrix puede utilizar el siguiente corpus.\n","\n","```python\n","corpus = [\n","  \"I like deep learning.\",\n","  \"I like NLP.\",\n","  \"I enjoy flying.\"\n","]\n","```\n","\n","Obteniendo una matriz parecia a esta:\n","\n","***Resultado esperado***: \n","\n","| counts   | I  | like | enjoy | deep | learning | NLP | flying | . |   \n","|----------|---:|-----:|------:|-----:|---------:|----:|-------:|--:|\n","| I        | 0  |  2   |  1    |    0 |  0       |   0 | 0      | 0|            \n","| like     |  2 |    0 |  0    |    1 |  0       |   1 | 0      | 0 | \n","| enjoy    |  1 |    0 |  0    |    0 |  0       |   0 | 1      | 0 |\n","| deep     |  0 |    1 |  0    |    0 |  1       |   0 | 0      | 0 |  \n","| learning |  0 |    0 |  0    |    1 |  0       |   0 | 0      | 1 |          \n","| NLP      |  0 |    1 |  0    |    0 |  0       |   0 | 0      | 1 |\n","| flying   |  0 |    0 |  1    |    0 |  0       |   0 | 0      | 1 | \n","| .        |  0 |    0 |  0    |    0 |  1       |   1 | 1      | 0 | \n","\n","\n","Verifique si su matrix es igual a esta utilizando el corpus de ejemplo. Ojo que este es s칩lo un ejemplo, su algoritmo debe **generalizar** a otros ejemplos."]},{"cell_type":"markdown","metadata":{"id":"Ol82nJ0FnmcP"},"source":["### **Parte 3 B (1 Punto): Word Embeddings**"]},{"cell_type":"markdown","metadata":{"id":"OgmeSFqKLpFL"},"source":["En la auxiliar 2 aprendieron como entrenar Word2Vec utilizando gensim. El objetivo de esta parte es comparar los embeddings obtenidos con dos modelos diferentes: Word2Vec y [FastText](https://radimrehurek.com/gensim/models/fasttext.html) (utilizen size=200 en FastText) entrenados en el mismo dataset de di치logos de los Simpson. "]},{"cell_type":"code","execution_count":4,"metadata":{"id":"ecCvnryeQiG7"},"outputs":[],"source":["import re\n","from time import time\n","from multiprocessing import cpu_count\n","from sklearn.linear_model import LogisticRegression\n","from collections import Counter\n","from sklearn.metrics import confusion_matrix, classification_report\n","\n","# word2vec\n","from gensim.models import Word2Vec, FastText\n","from gensim.models.phrases import Phrases, Phraser\n","from gensim.interfaces import TransformedCorpus\n","from sklearn.model_selection import train_test_split\n","import logging\n","\n","logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n","logger = logging.getLogger(__name__)"]},{"cell_type":"markdown","metadata":{"id":"tZgN06q4QPi3"},"source":["Utilizando el dataset adjunto con la tarea:"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eY3kmg4onnsu","outputId":"d3525a54-0c10-401e-b3e2-9c6e9e714a2c"},"outputs":[],"source":["data_file = \"../../data/dialogue-lines-of-the-simpsons.zip\"\n","df = pd.read_csv(data_file)\n","stopwords = pd.read_csv(\n","    'https://raw.githubusercontent.com/Alir3z4/stop-words/master/english.txt'\n",").values\n","stopwords = Counter(stopwords.flatten().tolist())\n","df = df.dropna().reset_index(drop=True) # Quitar filas vacias"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>raw_character_text</th>\n","      <th>spoken_words</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Miss Hoover</td>\n","      <td>No, actually, it was a little of both. Sometim...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Lisa Simpson</td>\n","      <td>Where's Mr. Bergstrom?</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Miss Hoover</td>\n","      <td>I don't know. Although I'd sure like to talk t...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Lisa Simpson</td>\n","      <td>That life is worth living.</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Edna Krabappel-Flanders</td>\n","      <td>The polls will be open from now until the end ...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>131848</th>\n","      <td>Miss Hoover</td>\n","      <td>I'm back.</td>\n","    </tr>\n","    <tr>\n","      <th>131849</th>\n","      <td>Miss Hoover</td>\n","      <td>You see, class, my Lyme disease turned out to ...</td>\n","    </tr>\n","    <tr>\n","      <th>131850</th>\n","      <td>Miss Hoover</td>\n","      <td>Psy-cho-so-ma-tic.</td>\n","    </tr>\n","    <tr>\n","      <th>131851</th>\n","      <td>Ralph Wiggum</td>\n","      <td>Does that mean you were crazy?</td>\n","    </tr>\n","    <tr>\n","      <th>131852</th>\n","      <td>JANEY</td>\n","      <td>No, that means she was faking it.</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>131853 rows 칑 2 columns</p>\n","</div>"],"text/plain":["             raw_character_text  \\\n","0                   Miss Hoover   \n","1                  Lisa Simpson   \n","2                   Miss Hoover   \n","3                  Lisa Simpson   \n","4       Edna Krabappel-Flanders   \n","...                         ...   \n","131848              Miss Hoover   \n","131849              Miss Hoover   \n","131850              Miss Hoover   \n","131851             Ralph Wiggum   \n","131852                    JANEY   \n","\n","                                             spoken_words  \n","0       No, actually, it was a little of both. Sometim...  \n","1                                  Where's Mr. Bergstrom?  \n","2       I don't know. Although I'd sure like to talk t...  \n","3                              That life is worth living.  \n","4       The polls will be open from now until the end ...  \n","...                                                   ...  \n","131848                                          I'm back.  \n","131849  You see, class, my Lyme disease turned out to ...  \n","131850                                 Psy-cho-so-ma-tic.  \n","131851                     Does that mean you were crazy?  \n","131852                  No, that means she was faking it.  \n","\n","[131853 rows x 2 columns]"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["df"]},{"cell_type":"markdown","metadata":{"id":"VAg5a5bmWk3T"},"source":["**Pregunta 1**: Ayud치ndose de los pasos vistos en la auxiliar, entrene los modelos Word2Vec y FastText sobre el dataset anterior. **(1 punto)** (Hint, le puede servir explorar un poco los datos)"]},{"cell_type":"markdown","metadata":{"id":"MWw2fXFRXe5Y"},"source":["**Respuesta**:"]},{"cell_type":"markdown","metadata":{},"source":["Uno de los pasos en el preprocesamiento del texto consiste en juntar tokens que por s칤 solos no representan alguna entidad, y convertirlos en bigramas. Por ejemplo \"Homer Simpson\" en lugar de los tokens \"Homer\" y \"SImpson\" por separado. Para estimar el umbral en que estos tokens deben aparecer para juntarlos, se realiza un conteo para saber cu치ntas veces aparecen en el corpus"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/plain":["Counter({'simpson': 66733,\n","         'homer': 28232,\n","         'marge': 13349,\n","         'bart': 13257,\n","         'lisa': 10903,\n","         'burns': 3157,\n","         'c.': 3085,\n","         'montgomery': 3077,\n","         'moe': 2877,\n","         'szyslak': 2810,\n","         'skinner': 2710,\n","         'flanders': 2522,\n","         'seymour': 2387,\n","         'wiggum': 2345,\n","         'van': 2097,\n","         'houten': 2089,\n","         'ned': 2065,\n","         'the': 2024,\n","         'grampa': 1899,\n","         'milhouse': 1843,\n","         'chief': 1828,\n","         'krusty': 1777,\n","         'clown': 1715,\n","         'lenny': 1188,\n","         'nelson': 1186,\n","         'muntz': 1183,\n","         'dr.': 1176,\n","         'leonard': 1151,\n","         'nahasapeemapetilon': 1133,\n","         'bouvier': 1081,\n","         'apu': 1003,\n","         'smithers': 969,\n","         'waylon': 965,\n","         'brockman': 904,\n","         'carl': 885,\n","         'kent': 885,\n","         'sideshow': 885,\n","         'man': 880,\n","         'announcer': 875,\n","         'carlson': 852,\n","         'edna': 719,\n","         'krabappel-flanders': 719,\n","         'hibbert': 709,\n","         'julius': 676,\n","         'lovejoy': 676,\n","         'bob': 622,\n","         '#1': 618,\n","         'rev.': 578,\n","         'selma': 578,\n","         'guy': 575,\n","         'chalmers': 572,\n","         'barney': 571,\n","         'gary': 566,\n","         'gumble': 560,\n","         'timothy': 550,\n","         'joe': 548,\n","         'willie': 529,\n","         'quimby': 510,\n","         '#2': 508,\n","         'mayor': 501,\n","         'groundskeeper': 500,\n","         'ralph': 481,\n","         'book': 478,\n","         'patty': 475,\n","         'comic': 475,\n","         'young': 426,\n","         'mann': 409,\n","         'otto': 406,\n","         'martin': 404,\n","         'prince': 401,\n","         'professor': 390,\n","         'frink': 360,\n","         'mel': 360,\n","         'jimbo': 355,\n","         'jones': 355,\n","         'lou': 348,\n","         'jonathan': 333,\n","         'tony': 329,\n","         'woman': 325,\n","         'female': 321,\n","         'voice': 309,\n","         'fat': 307,\n","         'captain': 280,\n","         'kearney': 277,\n","         'kirk': 272,\n","         'zzyzwicz': 259,\n","         'agnes': 257,\n","         'executive': 251,\n","         'adult': 240,\n","         'judge': 238,\n","         'spuckler': 237,\n","         'snake': 233,\n","         'old': 232,\n","         'jailbird': 230,\n","         'crowd': 230,\n","         'cletus': 228,\n","         'troy': 225,\n","         'mr.': 222,\n","         'miss': 213,\n","         'teenage': 204,\n","         'dolph': 203,\n","         'todd': 195,\n","         'narrator': 189,\n","         'boy': 177,\n","         \"homer's\": 170,\n","         'male': 169,\n","         'kids': 168,\n","         'thoughts': 165,\n","         'agent': 163,\n","         'lionel': 162,\n","         'texan': 161,\n","         'hutz': 158,\n","         'gil': 158,\n","         'mrs.': 157,\n","         'hoover': 156,\n","         'guard': 155,\n","         'rich': 155,\n","         'gunderson': 154,\n","         'wolfcastle': 153,\n","         'lady': 151,\n","         'herb': 148,\n","         'clerk': 147,\n","         'little': 145,\n","         'horatio': 143,\n","         'mccallister': 143,\n","         'helen': 140,\n","         'nick': 139,\n","         'manjula': 138,\n","         'girl': 137,\n","         'maude': 135,\n","         'jasper': 133,\n","         'snyder': 133,\n","         'tv': 132,\n","         'lawyer': 132,\n","         'mcclure': 131,\n","         'rod': 131,\n","         'tom': 131,\n","         'manager': 129,\n","         'beardly': 126,\n","         'director': 125,\n","         'rainier': 123,\n","         'mona': 122,\n","         'teacher': 120,\n","         'george': 119,\n","         'kang': 119,\n","         'father': 115,\n","         'robot': 113,\n","         'salesman': 113,\n","         'worker': 112,\n","         'riviera': 112,\n","         'john': 111,\n","         'mackleberry': 108,\n","         'in': 107,\n","         'artie': 107,\n","         'bill': 106,\n","         'louie': 106,\n","         'michael': 104,\n","         'jr.': 104,\n","         'roger': 104,\n","         'teen': 103,\n","         'kid': 102,\n","         'host': 102,\n","         '#3': 101,\n","         'brain': 100,\n","         'eddie': 98,\n","         'kodos': 98,\n","         'jack': 97,\n","         'luann': 97,\n","         'duffman': 95,\n","         'lindsay': 94,\n","         'audience': 93,\n","         'lurleen': 92,\n","         'lumpkin': 92,\n","         'bush': 92,\n","         'dan': 92,\n","         'blue-haired': 91,\n","         'jewish': 91,\n","         'declan': 91,\n","         'desmond': 91,\n","         'guide': 90,\n","         'largo': 90,\n","         'naegle': 90,\n","         'ms.': 88,\n","         'dewey': 87,\n","         'roy': 87,\n","         'mary': 87,\n","         'mother': 86,\n","         'rabbi': 85,\n","         'squeaky-voiced': 85,\n","         'reporter': 84,\n","         'luigi': 84,\n","         'hans': 83,\n","         'jacques': 81,\n","         'producer': 81,\n","         'moleman': 80,\n","         'head': 79,\n","         'security': 79,\n","         'driver': 79,\n","         'brandine': 79,\n","         'del': 79,\n","         'buck': 78,\n","         'gaga': 78,\n","         'fbi': 77,\n","         'singers': 77,\n","         'hyman': 76,\n","         'krustofsky': 76,\n","         'nurse': 76,\n","         'principal': 76,\n","         'owner': 76,\n","         'frank': 76,\n","         'annie': 75,\n","         'dad': 74,\n","         'ghost': 74,\n","         'crawford': 74,\n","         'colonel': 73,\n","         'james': 73,\n","         'larry': 72,\n","         'ziff': 72,\n","         'jay': 72,\n","         'howard': 72,\n","         'tour': 71,\n","         'waiter': 71,\n","         'officer': 71,\n","         'richard': 70,\n","         'employee': 70,\n","         'coach': 69,\n","         'all': 69,\n","         'scratchy': 69,\n","         'h.w.': 68,\n","         'king': 68,\n","         'prof.': 68,\n","         'marvin': 67,\n","         'monroe': 67,\n","         'marty': 65,\n","         'alien': 65,\n","         'vendor': 65,\n","         '2nd': 65,\n","         'hank': 65,\n","         'ron': 65,\n","         'lunchlady': 64,\n","         'inspector': 64,\n","         'gillick': 64,\n","         'itchy': 63,\n","         'member': 63,\n","         'herman': 62,\n","         'of': 61,\n","         'god': 61,\n","         'david': 59,\n","         'ray': 59,\n","         'johnny': 58,\n","         'viii': 58,\n","         'harm': 58,\n","         'julio': 58,\n","         'bergstrom': 57,\n","         'pilot': 57,\n","         'j.': 57,\n","         'dean': 57,\n","         'terwilliger': 57,\n","         'grimes': 57,\n","         'real': 57,\n","         'magini': 57,\n","         'sherri': 56,\n","         'terri': 55,\n","         'drederick': 55,\n","         'radio': 55,\n","         'bumblebee': 55,\n","         'goodman': 55,\n","         'tatum': 55,\n","         'shauna': 55,\n","         'charles': 54,\n","         'loren': 54,\n","         'pyror': 54,\n","         'meyers,': 54,\n","         'scorpio': 54,\n","         'white': 53,\n","         'bar': 53,\n","         'vicki': 52,\n","         'zack': 52,\n","         'elon': 52,\n","         'thought': 51,\n","         'soldier': 51,\n","         \"lisa's\": 51,\n","         'cecil': 51,\n","         'kurt': 51,\n","         'cheech': 51,\n","         'musk': 51,\n","         'attendant': 50,\n","         'wife': 50,\n","         'bubble': 50,\n","         'powers': 50,\n","         'dondelinger': 50,\n","         'singer': 50,\n","         'wayne': 50,\n","         'k.': 50,\n","         'husband': 49,\n","         'johnson': 49,\n","         'hugh': 49,\n","         'chinese': 49,\n","         'jim': 49,\n","         'armin': 49,\n","         'rachel': 49,\n","         'hardwick': 49,\n","         'elderly': 48,\n","         'hermann': 48,\n","         'chloe': 48,\n","         'ethan': 48,\n","         'foley': 48,\n","         'janey': 47,\n","         'cowboy': 47,\n","         'warden': 47,\n","         'stu': 47,\n","         'demon': 46,\n","         'disco': 46,\n","         'mark': 46,\n","         'simon': 46,\n","         'heathbar': 46,\n","         'raymondo': 46,\n","         'on': 45,\n","         'duff': 45,\n","         'jacqueline': 45,\n","         'baseball': 45,\n","         'supervisor': 45,\n","         'database': 45,\n","         'eduardo': 45,\n","         '10-year-old': 45,\n","         'gloria': 44,\n","         'dave': 44,\n","         'akira': 44,\n","         'instructor': 44,\n","         'farmer': 44,\n","         'stan': 44,\n","         'pierce': 44,\n","         'bleeding': 44,\n","         'gums': 44,\n","         'princess': 43,\n","         'official': 43,\n","         'billy': 43,\n","         'lanley': 43,\n","         'robert': 43,\n","         'patrick': 43,\n","         'dealer': 42,\n","         'doctor': 42,\n","         'krabappel': 42,\n","         'kim': 42,\n","         'l.t.': 42,\n","         'smash': 42,\n","         'chef': 41,\n","         'steve': 41,\n","         'researcher': 41,\n","         'bender': 41,\n","         'diggs': 41,\n","         'football': 40,\n","         'british': 40,\n","         'bartender': 40,\n","         'doll': 40,\n","         'paul': 40,\n","         'ben': 40,\n","         'saleswoman': 40,\n","         'krupt': 40,\n","         'dwight': 40,\n","         'technician': 39,\n","         'ruth': 39,\n","         'hobo': 39,\n","         'mindy': 39,\n","         'clinton': 39,\n","         'jessica': 39,\n","         'ranger': 39,\n","         'tab': 39,\n","         'spangler': 39,\n","         'august': 39,\n","         'avatar': 39,\n","         'rag': 39,\n","         'townspeople': 38,\n","         'arnie': 38,\n","         'emily': 38,\n","         'chazz': 38,\n","         'busby': 38,\n","         'zander': 38,\n","         'actor': 37,\n","         'becky': 37,\n","         'don': 37,\n","         'scientist': 37,\n","         'devil': 37,\n","         'french': 37,\n","         'new': 37,\n","         'editor': 37,\n","         'shary': 37,\n","         'sara': 37,\n","         'sloane': 37,\n","         'isabel': 37,\n","         'hobbes': 37,\n","         'german': 36,\n","         'lee': 36,\n","         'auctioneer': 36,\n","         'foster': 36,\n","         'cookie': 36,\n","         'cooder': 36,\n","         'baldwin': 36,\n","         'brosnan': 36,\n","         'charlie': 35,\n","         'assistant': 35,\n","         'detective': 35,\n","         'alex': 35,\n","         'glen': 35,\n","         'colt': 35,\n","         'jenda': 35,\n","         'maxine': 35,\n","         'older': 34,\n","         'doris': 34,\n","         'mike': 34,\n","         'mom': 34,\n","         'laura': 34,\n","         'student': 34,\n","         'stewart': 34,\n","         'dick': 34,\n","         'other': 34,\n","         'renee': 34,\n","         'sheriff': 34,\n","         'karl': 34,\n","         'gutierrez': 34,\n","         'clark': 33,\n","         'pye': 33,\n","         'sinclair': 33,\n","         'workers': 33,\n","         'indian': 33,\n","         'number': 33,\n","         'sgt.': 33,\n","         'basinger': 33,\n","         'alec': 33,\n","         'eliza': 33,\n","         'ticket': 32,\n","         'glick': 32,\n","         \"bart's\": 32,\n","         'wiseguy': 32,\n","         'legs': 32,\n","         'waitress': 32,\n","         'danielson': 32,\n","         'queen': 32,\n","         'santa': 32,\n","         'cashier': 32,\n","         'henry': 32,\n","         'businessman': 32,\n","         'sean': 32,\n","         'therapist': 32,\n","         'lombard': 32,\n","         'radioactive': 31,\n","         'engineer': 31,\n","         'irish': 31,\n","         'commandant': 31,\n","         'parson': 31,\n","         'kwan': 31,\n","         'memory': 31,\n","         'cantwell': 31,\n","         'kemi': 31,\n","         'teller': 30,\n","         'game': 30,\n","         'interviewer': 30,\n","         'edward': 30,\n","         'elf': 30,\n","         'receptionist': 30,\n","         'betty': 30,\n","         'jesse': 30,\n","         'grass': 30,\n","         'mason': 30,\n","         'dora': 30,\n","         'mcbain': 30,\n","         \"marge's\": 29,\n","         'orderly': 29,\n","         'president': 29,\n","         'player': 29,\n","         'store': 29,\n","         'nigel': 29,\n","         'bakerbutcher': 29,\n","         'wolfe': 29,\n","         'stacy': 29,\n","         'babcock': 29,\n","         'chorus': 29,\n","         'chester': 29,\n","         'lampwick': 29,\n","         'donny': 29,\n","         'roofi': 29,\n","         'gina': 29,\n","         'vendetti': 29,\n","         'neil': 28,\n","         'girls': 28,\n","         'christian': 28,\n","         'claus': 28,\n","         'rex': 28,\n","         'one': 28,\n","         'springfield': 28,\n","         'uncle': 28,\n","         'stephen': 28,\n","         'spanish': 28,\n","         'juliet': 28,\n","         'mccarthy': 28,\n","         'penelope': 28,\n","         'chong': 28,\n","         'lassen': 28,\n","         'delivery': 27,\n","         'dark': 27,\n","         'track': 27,\n","         'llewellyn': 27,\n","         'booth': 27,\n","         'hope': 27,\n","         'italian': 27,\n","         'al': 27,\n","         'city': 27,\n","         'wally': 27,\n","         'meathook': 27,\n","         'brother': 27,\n","         'plimpton': 27,\n","         'jenny': 27,\n","         'lyle': 27,\n","         'joey': 26,\n","         'leader': 26,\n","         'gulliver': 26,\n","         'p.a.': 26,\n","         'aide': 26,\n","         'stanky': 26,\n","         'foreman': 26,\n","         'cat': 26,\n","         'bobby': 26,\n","         'stillwater': 26,\n","         'dog': 26,\n","         'hamill': 26,\n","         'rep': 26,\n","         'clay': 26,\n","         'sophie': 26,\n","         'maya': 26,\n","         'mick': 26,\n","         'jagger': 26,\n","         'customer': 25,\n","         'barbara': 25,\n","         'sylvia': 25,\n","         'amber': 25,\n","         'dempsey': 25,\n","         'recruiter': 25,\n","         'jerry': 25,\n","         'trucker': 25,\n","         'deep': 25,\n","         'minister': 25,\n","         'cadet': 25,\n","         'anderson': 25,\n","         'faith': 25,\n","         'jockey': 25,\n","         'tightlips': 25,\n","         'greta': 25,\n","         'mcconnell': 25,\n","         'roz': 25,\n","         'cain': 25,\n","         'gretchen': 25,\n","         'lucas': 25,\n","         'photographer': 24,\n","         'operator': 24,\n","         'cop': 24,\n","         'cesar': 24,\n","         'men': 24,\n","         'elizabeth': 24,\n","         'show': 24,\n","         'prisoner': 24,\n","         'dennis': 24,\n","         'hurlbut': 24,\n","         'belle': 24,\n","         'grady': 24,\n","         '8-year-old': 24,\n","         'colby': 24,\n","         'nikki': 24,\n","         'mckenna': 24,\n","         'walt': 24,\n","         'walther': 24,\n","         'hotenhoffer': 24,\n","         'rita': 24,\n","         'terence': 24,\n","         'moog': 24,\n","         'english': 23,\n","         'angel': 23,\n","         'tammy': 23,\n","         'william': 23,\n","         'japanese': 23,\n","         'board': 23,\n","         'wise': 23,\n","         'allison': 23,\n","         'williams': 23,\n","         'chairman': 23,\n","         'bernice': 23,\n","         'nancy': 23,\n","         'ten-year-old': 23,\n","         'martha': 23,\n","         'farnsworth': 23,\n","         'senator': 22,\n","         'jimmy': 22,\n","         'samantha': 22,\n","         'angry': 22,\n","         'doug': 22,\n","         'bear': 22,\n","         'big': 22,\n","         'woods': 22,\n","         'birch': 22,\n","         'barlow': 22,\n","         'peter': 22,\n","         'frog': 22,\n","         'banner': 22,\n","         'gentleman': 22,\n","         'munchie': 22,\n","         'seth': 22,\n","         'hawking': 22,\n","         'lucy': 22,\n","         'kitenge': 22,\n","         'keith': 22,\n","         'giuseppe': 22,\n","         'madam': 22,\n","         'wu': 22,\n","         'tabitha': 22,\n","         'jakob': 22,\n","         'rowan': 22,\n","         'priddis': 22,\n","         'yutaka': 22,\n","         'original': 22,\n","         'meyers': 22,\n","         'v.o.': 22,\n","         'repo': 21,\n","         'shutton': 21,\n","         'bus': 21,\n","         'ted': 21,\n","         'black': 21,\n","         'zombie': 21,\n","         'writer': 21,\n","         'school': 21,\n","         \"apu's\": 21,\n","         'members': 21,\n","         'zweig': 21,\n","         'australian': 21,\n","         'goose': 21,\n","         'mitchell': 21,\n","         'casino': 21,\n","         'gabriel': 21,\n","         'senior': 21,\n","         'bligh': 21,\n","         'darcy': 21,\n","         'marshall': 21,\n","         'delroy': 21,\n","         'g.': 21,\n","         'hooper': 21,\n","         'lewis': 20,\n","         'attorney': 20,\n","         'tourist': 20,\n","         'sr.': 20,\n","         'benjamin': 20,\n","         'second': 20,\n","         'with': 20,\n","         'truck': 20,\n","         'park': 20,\n","         'luke': 20,\n","         'diner': 20,\n","         'accountant': 20,\n","         'first': 20,\n","         'freddy': 20,\n","         'fox': 20,\n","         'fortune': 20,\n","         'atkins': 20,\n","         'arthur': 20,\n","         'lifeways': 20,\n","         'jordan': 20,\n","         'connor': 20,\n","         'gladwell': 20,\n","         'antonio': 20,\n","         'swanson': 20,\n","         'chett': 20,\n","         'obama': 20,\n","         'lloyd': 20,\n","         'willington': 20,\n","         'arnold': 19,\n","         'weasel': 19,\n","         'horst': 19,\n","         'sailor': 19,\n","         'box': 19,\n","         'scioscia': 19,\n","         'ken': 19,\n","         'sergeant': 19,\n","         'mailman': 19,\n","         'gabbo': 19,\n","         'house': 19,\n","         'drill': 19,\n","         'dj': 19,\n","         'thug': 19,\n","         'zoo': 19,\n","         'witch': 19,\n","         'bunyan': 19,\n","         'jennifer': 19,\n","         'wars': 19,\n","         '24-year-old': 19,\n","         'sid': 19,\n","         'mobbs': 19,\n","         'slick': 19,\n","         'publisher': 19,\n","         'sven-golly': 19,\n","         'adil': 19,\n","         'hoxha': 19,\n","         'conductor': 18,\n","         'psychiatrist': 18,\n","         'thomas': 18,\n","         'everyone': 18,\n","         'd.j.': 18,\n","         'teenager': 18,\n","         'kevin': 18,\n","         'station': 18,\n","         '#4': 18,\n","         'lovell': 18,\n","         'class': 18,\n","         'huck': 18,\n","         '1st': 18,\n","         'shelby': 18,\n","         'very': 18,\n","         'parker': 18,\n","         'hamlet': 18,\n","         'hawk': 18,\n","         'deputy': 18,\n","         'shady': 18,\n","         'rabinowitz': 18,\n","         'alberto': 18,\n","         'viktor': 18,\n","         'marlowe': 18,\n","         't-rex': 18,\n","         'naziwa': 18,\n","         'slava': 18,\n","         'zhenya': 18,\n","         'bowditch': 18,\n","         'melon': 18,\n","         'lucille': 18,\n","         'botzcowski': 18,\n","         'bea': 18,\n","         'actress': 17,\n","         'crowley': 17,\n","         'speaker': 17,\n","         'maitre': 17,\n","         '&': 17,\n","         'southern': 17,\n","         'allen': 17,\n","         'usher': 17,\n","         'animal': 17,\n","         'sarah': 17,\n","         'buzz': 17,\n","         'trainer': 17,\n","         'russian': 17,\n","         'evelyn': 17,\n","         'sir': 17,\n","         'rick': 17,\n","         'erin': 17,\n","         'representative': 17,\n","         'st.': 17,\n","         'tenille': 17,\n","         'trent': 17,\n","         'coleman': 17,\n","         'randy': 17,\n","         'caleb': 17,\n","         'finn': 17,\n","         'brokaw': 17,\n","         'mitch': 17,\n","         'stark': 17,\n","         'richdale': 17,\n","         'joseph': 17,\n","         'milo': 17,\n","         'vamp': 17,\n","         'superior': 17,\n","         'goldman': 17,\n","         'kissingher': 17,\n","         'marketing': 17,\n","         'portia': 17,\n","         'janet': 17,\n","         'reno': 17,\n","         'kumiko': 17,\n","         'pennycandy': 16,\n","         'recording': 16,\n","         'librarian': 16,\n","         'general': 16,\n","         'bum': 16,\n","         'students': 16,\n","         'boys': 16,\n","         'bailiff': 16,\n","         'counter': 16,\n","         'workman': 16,\n","         'robber': 16,\n","         'evan': 16,\n","         'bank': 16,\n","         'joan': 16,\n","         'moses/milhouse': 16,\n","         '2': 16,\n","         'gameshow': 16,\n","         'canadian': 16,\n","         'lawless': 16,\n","         'velimirovic': 16,\n","         'activist': 16,\n","         'linux': 16,\n","         'richards': 16,\n","         'byrne': 16,\n","         'lord': 16,\n","         'michelle': 16,\n","         'howell': 16,\n","         'huser': 16,\n","         'thorn': 16,\n","         'smothers': 16,\n","         'hiram': 16,\n","         'spy': 16,\n","         'regular': 16,\n","         'wendell': 15,\n","         'borton': 15,\n","         'phone': 15,\n","         'sportscaster': 15,\n","         'lois': 15,\n","         'broker': 15,\n","         'hippie': 15,\n","         'nun': 15,\n","         'company': 15,\n","         'lincoln': 15,\n","         'cameraman': 15,\n","         'lance': 15,\n","         'pepi': 15,\n","         'car': 15,\n","         'stage': 15,\n","         'skinny': 15,\n","         'women': 15,\n","         'newsreel': 15,\n","         '칚콉ter': 15,\n","         'conover': 15,\n","         'middle-aged': 15,\n","         'shop': 15,\n","         'chase': 15,\n","         'american': 15,\n","         'comptroller': 15,\n","         'ginger': 15,\n","         'conroy': 15,\n","         'daltrey': 15,\n","         'costington': 15,\n","         'carrie': 15,\n","         'bellamy': 15,\n","         'salieri': 15,\n","         'mysterious': 15,\n","         'dante': 15,\n","         'gendarme': 15,\n","         'royce': 15,\n","         'midwestern': 15,\n","         'erik': 15,\n","         'murphy': 15,\n","         'maddow': 15,\n","         'flight': 14,\n","         'scott': 14,\n","         \"d'\": 14,\n","         'scully': 14,\n","         'brad': 14,\n","         'barflies': 14,\n","         'boss': 14,\n","         'bret': 14,\n","         'smith': 14,\n","         'prosecutor': 14,\n","         'insurance': 14,\n","         'mountain': 14,\n","         'nimoy': 14,\n","         'mob': 14,\n","         'commissioner': 14,\n","         'counselor': 14,\n","         'admissions': 14,\n","         'peterson': 14,\n","         'croupier': 14,\n","         'congregation': 14,\n","         'hockey': 14,\n","         'congressman': 14,\n","         'vitorio': 14,\n","         'shelbyville': 14,\n","         'bully': 14,\n","         'macarthur': 14,\n","         'state': 14,\n","         'dowager': 14,\n","         'prison': 14,\n","         'magician': 14,\n","         'mutant': 14,\n","         'gun': 14,\n","         'jane': 14,\n","         'creepy': 14,\n","         'crew': 14,\n","         'widow': 14,\n","         'bushwell': 14,\n","         'mesmerino': 14,\n","         'gere': 14,\n","         'motherloving': 14,\n","         'sacagawea': 14,\n","         'health': 14,\n","         'krackney': 14,\n","         'vampire': 14,\n","         'w.': 14,\n","         'jarmusch': 14,\n","         \"o'flanagan\": 14,\n","         'costas': 14,\n","         'virgil': 14,\n","         'drug': 14,\n","         'hat': 14,\n","         'klaus': 14,\n","         'ziegler': 14,\n","         'lombardo': 14,\n","         'stagehand': 13,\n","         'commentator': 13,\n","         'perry': 13,\n","         'ugolin': 13,\n","         'commercial': 13,\n","         'strawberry': 13,\n","         'ii': 13,\n","         'abraham': 13,\n","         'alan': 13,\n","         'murdock': 13,\n","         'cartoon': 13,\n","         'barry': 13,\n","         'crandall': 13,\n","         'taylor': 13,\n","         'people': 13,\n","         \"woman's\": 13,\n","         'ashley': 13,\n","         'stonecutters': 13,\n","         'ambassador': 13,\n","         'brooks': 13,\n","         'committee': 13,\n","         'computer': 13,\n","         'mall': 13,\n","         'lucius': 13,\n","         'temperance': 13,\n","         'spud': 13,\n","         'admiral': 13,\n","         'pirate': 13,\n","         'pharaoh': 13,\n","         'ak': 13,\n","         'sawyer': 13,\n","         'sad': 13,\n","         'tina': 13,\n","         'petty': 13,\n","         'knight': 13,\n","         'cyrus': 13,\n","         'indigent': 13,\n","         'nash': 13,\n","         'castor': 13,\n","         'moezekiel': 13,\n","         'bachelor': 13,\n","         'marbles': 13,\n","         'golem': 13,\n","         'nugent': 13,\n","         'art': 13,\n","         'annette': 13,\n","         'albert': 13,\n","         'gladness': 13,\n","         'gwendolyn': 13,\n","         'edmund': 13,\n","         'millwood': 13,\n","         'intellectual': 13,\n","         'cregg': 13,\n","         'cia': 13,\n","         'kamala': 13,\n","         'gratman': 13,\n","         'bennett': 13,\n","         'secretary': 12,\n","         'larson': 12,\n","         'from': 12,\n","         'collette': 12,\n","         'darryl': 12,\n","         'players': 12,\n","         'adam': 12,\n","         'pig': 12,\n","         'opal': 12,\n","         'bag': 12,\n","         'attractive': 12,\n","         '3rd': 12,\n","         'elvis': 12,\n","         'child': 12,\n","         'hillbilly': 12,\n","         'ramone': 12,\n","         'optometrist': 12,\n","         'recorded': 12,\n","         'mission': 12,\n","         'beekeeper': 12,\n","         'rock': 12,\n","         'plumber': 12,\n","         'tow': 12,\n","         'thurston': 12,\n","         'sports': 12,\n","         'animator': 12,\n","         'june': 12,\n","         'larsen': 12,\n","         'chauffeur': 12,\n","         'asian': 12,\n","         'bruce': 12,\n","         'janitor': 12,\n","         'ed': 12,\n","         'taffy': 12,\n","         'astrid': 12,\n","         'weller': 12,\n","         'face': 12,\n","         'qtoktok': 12,\n","         'morehouse': 12,\n","         'huntington': 12,\n","         'jeff': 12,\n","         'jet': 12,\n","         'death': 12,\n","         'suit': 12,\n","         'good-looking': 12,\n","         ...})"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["word_counts = Counter(' '.join(df.raw_character_text).lower().split())\n","word_counts"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["def tokenize(text: str, stopwords: list[str]|None = None) -> list[str]:\n","    \"\"\"Tokeniza un texto\n","\n","    Parameters\n","    ----------\n","    text : str\n","        Texto a ser tokenizado\n","    stopwords : list[str] | None, opcional\n","        Lista de stopwords, por defecto None\n","\n","    Returns\n","    -------\n","    list[str]\n","        Lista con los tokens del texto\n","    \"\"\"\n","    # Reemplaza las ap칩strofes por guiones bajos. As칤, textos como \"I'm\" o \"it's\" se consideran como un token\n","    text_without_apos = text.replace(\"'\", \"_\")\n","    pattern = r\"\\s+|[^\\w\\s+]\" #Separa en espacios y en caracteres no palabras\n","    tokens = list(filter(None, re.split(pattern, text_without_apos.lower()))) # Filtra todos los 'None'\n","\n","    if stopwords:\n","        return [token for token in tokens if token not in stopwords]\n","    return tokens\n","\n","\n","def phrase_unifier(tokenized_sentences: pd.Series, min_count: int = 50) -> TransformedCorpus:\n","    \"\"\"Convierte en bigramas tokens que suelen aparecer juntos\n","\n","    Parameters\n","    ----------\n","    tokenized_sentences : pd.Series\n","        Texto ya tokenizado\n","    min_count : int, opcional\n","        Cantidad m칤nima de apariciones para contar como bigrama, por defecto 50\n","\n","    Returns\n","    -------\n","    TransformedCorpus\n","        Objeto con los bigramas encontrados en el corpus\n","    \"\"\"\n","    phrases = Phrases(tokenized_sentences, min_count=min_count, progress_per=5000)\n","    bigram = Phraser(phrases)\n","    sentences = bigram[tokenized_sentences]\n","    return sentences"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-03-05 21:32:29,793 : INFO : collecting all words and their counts\n","2024-03-05 21:32:29,794 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n","2024-03-05 21:32:29,830 : INFO : PROGRESS: at sentence #5000, processed 49622 words and 33155 word types\n","2024-03-05 21:32:29,861 : INFO : PROGRESS: at sentence #10000, processed 97620 words and 56822 word types\n","2024-03-05 21:32:29,891 : INFO : PROGRESS: at sentence #15000, processed 143188 words and 76754 word types\n","2024-03-05 21:32:29,934 : INFO : PROGRESS: at sentence #20000, processed 197460 words and 98732 word types\n","2024-03-05 21:32:29,974 : INFO : PROGRESS: at sentence #25000, processed 250684 words and 119821 word types\n","2024-03-05 21:32:30,013 : INFO : PROGRESS: at sentence #30000, processed 307225 words and 141052 word types\n","2024-03-05 21:32:30,052 : INFO : PROGRESS: at sentence #35000, processed 360500 words and 159166 word types\n","2024-03-05 21:32:30,088 : INFO : PROGRESS: at sentence #40000, processed 408306 words and 174541 word types\n","2024-03-05 21:32:30,132 : INFO : PROGRESS: at sentence #45000, processed 455383 words and 190009 word types\n","2024-03-05 21:32:30,170 : INFO : PROGRESS: at sentence #50000, processed 501360 words and 204886 word types\n","2024-03-05 21:32:30,205 : INFO : PROGRESS: at sentence #55000, processed 546044 words and 218791 word types\n","2024-03-05 21:32:30,240 : INFO : PROGRESS: at sentence #60000, processed 588188 words and 231320 word types\n","2024-03-05 21:32:30,274 : INFO : PROGRESS: at sentence #65000, processed 633768 words and 244564 word types\n","2024-03-05 21:32:30,320 : INFO : PROGRESS: at sentence #70000, processed 685506 words and 260200 word types\n","2024-03-05 21:32:30,363 : INFO : PROGRESS: at sentence #75000, processed 736914 words and 275173 word types\n","2024-03-05 21:32:30,405 : INFO : PROGRESS: at sentence #80000, processed 788600 words and 289629 word types\n","2024-03-05 21:32:30,446 : INFO : PROGRESS: at sentence #85000, processed 840072 words and 303844 word types\n","2024-03-05 21:32:30,486 : INFO : PROGRESS: at sentence #90000, processed 889499 words and 317405 word types\n","2024-03-05 21:32:30,525 : INFO : PROGRESS: at sentence #95000, processed 940846 words and 330829 word types\n","2024-03-05 21:32:30,571 : INFO : PROGRESS: at sentence #100000, processed 991325 words and 344368 word types\n","2024-03-05 21:32:30,621 : INFO : PROGRESS: at sentence #105000, processed 1043008 words and 357643 word types\n","2024-03-05 21:32:30,668 : INFO : PROGRESS: at sentence #110000, processed 1095289 words and 371424 word types\n","2024-03-05 21:32:30,705 : INFO : PROGRESS: at sentence #115000, processed 1145424 words and 383859 word types\n","2024-03-05 21:32:30,747 : INFO : PROGRESS: at sentence #120000, processed 1196470 words and 396613 word types\n","2024-03-05 21:32:30,789 : INFO : PROGRESS: at sentence #125000, processed 1246827 words and 408111 word types\n","2024-03-05 21:32:30,847 : INFO : PROGRESS: at sentence #130000, processed 1296674 words and 418691 word types\n","2024-03-05 21:32:30,870 : INFO : collected 422632 token types (unigram + bigrams) from a corpus of 1315234 words and 131853 sentences\n","2024-03-05 21:32:30,872 : INFO : merged Phrases<422632 vocab, min_count=50, threshold=10.0, max_vocab_size=40000000>\n","2024-03-05 21:32:30,872 : INFO : Phrases lifecycle event {'msg': 'built Phrases<422632 vocab, min_count=50, threshold=10.0, max_vocab_size=40000000> in 1.08s', 'datetime': '2024-03-05T21:32:30.872769', 'gensim': '4.3.2', 'python': '3.11.7 (main, Jan  8 2024, 06:21:35) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'created'}\n","2024-03-05 21:32:30,874 : INFO : exporting phrases from Phrases<422632 vocab, min_count=50, threshold=10.0, max_vocab_size=40000000>\n","2024-03-05 21:32:31,327 : INFO : FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<290 phrases, min_count=50, threshold=10.0> from Phrases<422632 vocab, min_count=50, threshold=10.0, max_vocab_size=40000000> in 0.45s', 'datetime': '2024-03-05T21:32:31.327650', 'gensim': '4.3.2', 'python': '3.11.7 (main, Jan  8 2024, 06:21:35) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'created'}\n"]}],"source":["tokenized_sentences = df.spoken_words.apply(tokenize)\n","sentences = phrase_unifier(tokenized_sentences)"]},{"cell_type":"markdown","metadata":{},"source":["Entrenamiento de Word2Vec"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-03-05 21:32:31,352 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=200, alpha=0.03>', 'datetime': '2024-03-05T21:32:31.352491', 'gensim': '4.3.2', 'python': '3.11.7 (main, Jan  8 2024, 06:21:35) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'created'}\n"]}],"source":["simpsons_w2v = Word2Vec(\n","    min_count=10,\n","    window=3,\n","    vector_size=200,\n","    sample=6e-5,\n","    alpha=0.03,\n","    min_alpha=0.0007,\n","    negative=20,\n","    workers=cpu_count()\n",")"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-03-05 21:32:31,364 : INFO : collecting all words and their counts\n","2024-03-05 21:32:31,365 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n","2024-03-05 21:32:31,452 : INFO : PROGRESS: at sentence #10000, processed 94804 words, keeping 9927 word types\n","2024-03-05 21:32:31,532 : INFO : PROGRESS: at sentence #20000, processed 191865 words, keeping 14963 word types\n","2024-03-05 21:32:31,599 : INFO : PROGRESS: at sentence #30000, processed 298592 words, keeping 19424 word types\n","2024-03-05 21:32:31,668 : INFO : PROGRESS: at sentence #40000, processed 396670 words, keeping 22383 word types\n","2024-03-05 21:32:31,725 : INFO : PROGRESS: at sentence #50000, processed 487128 words, keeping 25140 word types\n","2024-03-05 21:32:31,778 : INFO : PROGRESS: at sentence #60000, processed 571614 words, keeping 27426 word types\n","2024-03-05 21:32:31,842 : INFO : PROGRESS: at sentence #70000, processed 666360 words, keeping 29763 word types\n","2024-03-05 21:32:31,908 : INFO : PROGRESS: at sentence #80000, processed 766710 words, keeping 32154 word types\n","2024-03-05 21:32:31,982 : INFO : PROGRESS: at sentence #90000, processed 865157 words, keeping 34248 word types\n","2024-03-05 21:32:32,058 : INFO : PROGRESS: at sentence #100000, processed 964386 words, keeping 36145 word types\n","2024-03-05 21:32:32,128 : INFO : PROGRESS: at sentence #110000, processed 1065760 words, keeping 38187 word types\n","2024-03-05 21:32:32,192 : INFO : PROGRESS: at sentence #120000, processed 1164443 words, keeping 39926 word types\n","2024-03-05 21:32:32,256 : INFO : PROGRESS: at sentence #130000, processed 1261737 words, keeping 41226 word types\n","2024-03-05 21:32:32,268 : INFO : collected 41472 word types from a corpus of 1279712 raw words and 131853 sentences\n","2024-03-05 21:32:32,269 : INFO : Creating a fresh vocabulary\n","2024-03-05 21:32:32,292 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=10 retains 7338 unique words (17.69% of original 41472, drops 34134)', 'datetime': '2024-03-05T21:32:32.291141', 'gensim': '4.3.2', 'python': '3.11.7 (main, Jan  8 2024, 06:21:35) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n","2024-03-05 21:32:32,292 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=10 leaves 1200333 word corpus (93.80% of original 1279712, drops 79379)', 'datetime': '2024-03-05T21:32:32.292117', 'gensim': '4.3.2', 'python': '3.11.7 (main, Jan  8 2024, 06:21:35) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n","2024-03-05 21:32:32,326 : INFO : deleting the raw counts dictionary of 41472 items\n","2024-03-05 21:32:32,326 : INFO : sample=6e-05 downsamples 727 most-common words\n","2024-03-05 21:32:32,327 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 476337.2578709029 word corpus (39.7%% of prior 1200333)', 'datetime': '2024-03-05T21:32:32.327619', 'gensim': '4.3.2', 'python': '3.11.7 (main, Jan  8 2024, 06:21:35) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n","2024-03-05 21:32:32,363 : INFO : estimated required memory for 7338 words and 200 dimensions: 15409800 bytes\n","2024-03-05 21:32:32,365 : INFO : resetting layer weights\n","2024-03-05 21:32:32,376 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-03-05T21:32:32.376200', 'gensim': '4.3.2', 'python': '3.11.7 (main, Jan  8 2024, 06:21:35) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'build_vocab'}\n"]}],"source":["simpsons_w2v.build_vocab(sentences, progress_per=10000)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-03-05 21:32:32,385 : INFO : Word2Vec lifecycle event {'msg': 'training model with 16 workers on 7338 vocabulary and 200 features, using sg=0 hs=0 sample=6e-05 negative=20 window=3 shrink_windows=True', 'datetime': '2024-03-05T21:32:32.385828', 'gensim': '4.3.2', 'python': '3.11.7 (main, Jan  8 2024, 06:21:35) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'train'}\n","2024-03-05 21:32:33,430 : INFO : EPOCH 0 - PROGRESS: at 50.01% examples, 232301 words/s, in_qsize 31, out_qsize 5\n","2024-03-05 21:32:33,668 : INFO : EPOCH 0: training on 1279712 raw words (476523 effective words) took 1.3s, 380848 effective words/s\n","2024-03-05 21:32:34,711 : INFO : EPOCH 1 - PROGRESS: at 46.22% examples, 210286 words/s, in_qsize 29, out_qsize 15\n","2024-03-05 21:32:34,922 : INFO : EPOCH 1: training on 1279712 raw words (475923 effective words) took 1.2s, 384580 effective words/s\n","2024-03-05 21:32:36,029 : INFO : EPOCH 2 - PROGRESS: at 50.32% examples, 219882 words/s, in_qsize 31, out_qsize 0\n","2024-03-05 21:32:36,287 : INFO : EPOCH 2: training on 1279712 raw words (476075 effective words) took 1.3s, 358823 effective words/s\n","2024-03-05 21:32:37,310 : INFO : EPOCH 3 - PROGRESS: at 52.23% examples, 244340 words/s, in_qsize 32, out_qsize 3\n","2024-03-05 21:32:37,532 : INFO : EPOCH 3: training on 1279712 raw words (476797 effective words) took 1.2s, 387288 effective words/s\n","2024-03-05 21:32:38,602 : INFO : EPOCH 4 - PROGRESS: at 57.89% examples, 258991 words/s, in_qsize 28, out_qsize 3\n","2024-03-05 21:32:38,757 : INFO : EPOCH 4: training on 1279712 raw words (476465 effective words) took 1.2s, 394621 effective words/s\n","2024-03-05 21:32:39,926 : INFO : EPOCH 5 - PROGRESS: at 59.34% examples, 244185 words/s, in_qsize 29, out_qsize 2\n","2024-03-05 21:32:40,103 : INFO : EPOCH 5: training on 1279712 raw words (476967 effective words) took 1.3s, 360333 effective words/s\n","2024-03-05 21:32:41,136 : INFO : EPOCH 6 - PROGRESS: at 54.15% examples, 249075 words/s, in_qsize 31, out_qsize 4\n","2024-03-05 21:32:41,351 : INFO : EPOCH 6: training on 1279712 raw words (476084 effective words) took 1.2s, 385607 effective words/s\n","2024-03-05 21:32:42,375 : INFO : EPOCH 7 - PROGRESS: at 48.50% examples, 226515 words/s, in_qsize 29, out_qsize 13\n","2024-03-05 21:32:42,570 : INFO : EPOCH 7: training on 1279712 raw words (476421 effective words) took 1.2s, 397487 effective words/s\n","2024-03-05 21:32:43,605 : INFO : EPOCH 8 - PROGRESS: at 54.72% examples, 256832 words/s, in_qsize 31, out_qsize 3\n","2024-03-05 21:32:43,809 : INFO : EPOCH 8: training on 1279712 raw words (476467 effective words) took 1.2s, 394761 effective words/s\n","2024-03-05 21:32:44,891 : INFO : EPOCH 9 - PROGRESS: at 47.95% examples, 214247 words/s, in_qsize 32, out_qsize 0\n","2024-03-05 21:32:45,190 : INFO : EPOCH 9: training on 1279712 raw words (476258 effective words) took 1.3s, 354697 effective words/s\n","2024-03-05 21:32:46,244 : INFO : EPOCH 10 - PROGRESS: at 50.92% examples, 235806 words/s, in_qsize 22, out_qsize 13\n","2024-03-05 21:32:46,457 : INFO : EPOCH 10: training on 1279712 raw words (476508 effective words) took 1.2s, 388288 effective words/s\n","2024-03-05 21:32:47,492 : INFO : EPOCH 11 - PROGRESS: at 57.73% examples, 268878 words/s, in_qsize 23, out_qsize 2\n","2024-03-05 21:32:47,715 : INFO : EPOCH 11: training on 1279712 raw words (477100 effective words) took 1.2s, 385208 effective words/s\n","2024-03-05 21:32:48,765 : INFO : EPOCH 12 - PROGRESS: at 57.87% examples, 267308 words/s, in_qsize 16, out_qsize 14\n","2024-03-05 21:32:49,080 : INFO : EPOCH 12: training on 1279712 raw words (476538 effective words) took 1.3s, 356683 effective words/s\n","2024-03-05 21:32:50,179 : INFO : EPOCH 13 - PROGRESS: at 60.16% examples, 265447 words/s, in_qsize 31, out_qsize 0\n","2024-03-05 21:32:50,324 : INFO : EPOCH 13: training on 1279712 raw words (476199 effective words) took 1.2s, 392421 effective words/s\n","2024-03-05 21:32:51,382 : INFO : EPOCH 14 - PROGRESS: at 60.08% examples, 275312 words/s, in_qsize 31, out_qsize 0\n","2024-03-05 21:32:51,529 : INFO : EPOCH 14: training on 1279712 raw words (476379 effective words) took 1.2s, 404568 effective words/s\n","2024-03-05 21:32:51,529 : INFO : Word2Vec lifecycle event {'msg': 'training on 19195680 raw words (7146704 effective words) took 19.1s, 373323 effective words/s', 'datetime': '2024-03-05T21:32:51.529875', 'gensim': '4.3.2', 'python': '3.11.7 (main, Jan  8 2024, 06:21:35) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'train'}\n"]},{"name":"stdout","output_type":"stream","text":["Time to train the model: 0.32 mins\n"]}],"source":["t = time()\n","simpsons_w2v.train(sentences, total_examples=simpsons_w2v.corpus_count, epochs=15, report_delay=10)\n","print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"]},{"cell_type":"markdown","metadata":{},"source":["Entrenamiento de FastText"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-03-05 21:32:51,537 : INFO : FastText lifecycle event {'params': 'FastText<vocab=0, vector_size=200, alpha=0.03>', 'datetime': '2024-03-05T21:32:51.537598', 'gensim': '4.3.2', 'python': '3.11.7 (main, Jan  8 2024, 06:21:35) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'created'}\n"]}],"source":["simpsons_ft = FastText(\n","    min_count=1,\n","    window=3,\n","    vector_size=200,\n","    sample=6e-5,\n","    alpha=0.03,\n","    min_alpha=0.0007,\n","    negative=20,\n","    workers=cpu_count()\n",")"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-03-05 21:32:51,547 : INFO : collecting all words and their counts\n","2024-03-05 21:32:51,548 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n","2024-03-05 21:32:51,631 : INFO : PROGRESS: at sentence #10000, processed 94804 words, keeping 9927 word types\n","2024-03-05 21:32:51,749 : INFO : PROGRESS: at sentence #20000, processed 191865 words, keeping 14963 word types\n","2024-03-05 21:32:51,814 : INFO : PROGRESS: at sentence #30000, processed 298592 words, keeping 19424 word types\n","2024-03-05 21:32:51,907 : INFO : PROGRESS: at sentence #40000, processed 396670 words, keeping 22383 word types\n","2024-03-05 21:32:51,964 : INFO : PROGRESS: at sentence #50000, processed 487128 words, keeping 25140 word types\n","2024-03-05 21:32:52,024 : INFO : PROGRESS: at sentence #60000, processed 571614 words, keeping 27426 word types\n","2024-03-05 21:32:52,090 : INFO : PROGRESS: at sentence #70000, processed 666360 words, keeping 29763 word types\n","2024-03-05 21:32:52,155 : INFO : PROGRESS: at sentence #80000, processed 766710 words, keeping 32154 word types\n","2024-03-05 21:32:52,222 : INFO : PROGRESS: at sentence #90000, processed 865157 words, keeping 34248 word types\n","2024-03-05 21:32:52,287 : INFO : PROGRESS: at sentence #100000, processed 964386 words, keeping 36145 word types\n","2024-03-05 21:32:52,351 : INFO : PROGRESS: at sentence #110000, processed 1065760 words, keeping 38187 word types\n","2024-03-05 21:32:52,418 : INFO : PROGRESS: at sentence #120000, processed 1164443 words, keeping 39926 word types\n","2024-03-05 21:32:52,480 : INFO : PROGRESS: at sentence #130000, processed 1261737 words, keeping 41226 word types\n","2024-03-05 21:32:52,493 : INFO : collected 41472 word types from a corpus of 1279712 raw words and 131853 sentences\n","2024-03-05 21:32:52,494 : INFO : Creating a fresh vocabulary\n","2024-03-05 21:32:52,573 : INFO : FastText lifecycle event {'msg': 'effective_min_count=1 retains 41472 unique words (100.00% of original 41472, drops 0)', 'datetime': '2024-03-05T21:32:52.573767', 'gensim': '4.3.2', 'python': '3.11.7 (main, Jan  8 2024, 06:21:35) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n","2024-03-05 21:32:52,575 : INFO : FastText lifecycle event {'msg': 'effective_min_count=1 leaves 1279712 word corpus (100.00% of original 1279712, drops 0)', 'datetime': '2024-03-05T21:32:52.575313', 'gensim': '4.3.2', 'python': '3.11.7 (main, Jan  8 2024, 06:21:35) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n","2024-03-05 21:32:52,731 : INFO : deleting the raw counts dictionary of 41472 items\n","2024-03-05 21:32:52,732 : INFO : sample=6e-05 downsamples 684 most-common words\n","2024-03-05 21:32:52,734 : INFO : FastText lifecycle event {'msg': 'downsampling leaves estimated 564693.357019342 word corpus (44.1%% of prior 1279712)', 'datetime': '2024-03-05T21:32:52.734877', 'gensim': '4.3.2', 'python': '3.11.7 (main, Jan  8 2024, 06:21:35) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n","2024-03-05 21:32:53,121 : INFO : estimated required memory for 41472 words, 2000000 buckets and 200 dimensions: 1695007244 bytes\n","2024-03-05 21:32:53,122 : INFO : resetting layer weights\n","2024-03-05 21:32:55,928 : INFO : FastText lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-03-05T21:32:55.928798', 'gensim': '4.3.2', 'python': '3.11.7 (main, Jan  8 2024, 06:21:35) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'build_vocab'}\n"]}],"source":["simpsons_ft.build_vocab(sentences, progress_per=10000)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-03-05 21:32:55,939 : INFO : FastText lifecycle event {'msg': 'training model with 16 workers on 41472 vocabulary and 200 features, using sg=0 hs=0 sample=6e-05 negative=20 window=3 shrink_windows=True', 'datetime': '2024-03-05T21:32:55.939674', 'gensim': '4.3.2', 'python': '3.11.7 (main, Jan  8 2024, 06:21:35) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'train'}\n","2024-03-05 21:32:56,965 : INFO : EPOCH 0 - PROGRESS: at 29.78% examples, 169711 words/s, in_qsize 23, out_qsize 8\n","2024-03-05 21:32:57,748 : INFO : EPOCH 0: training on 1279712 raw words (564226 effective words) took 1.8s, 315294 effective words/s\n","2024-03-05 21:32:58,943 : INFO : EPOCH 1 - PROGRESS: at 47.01% examples, 249348 words/s, in_qsize 24, out_qsize 1\n","2024-03-05 21:32:59,767 : INFO : EPOCH 1: training on 1279712 raw words (564565 effective words) took 1.9s, 301767 effective words/s\n","2024-03-05 21:33:00,924 : INFO : EPOCH 2 - PROGRESS: at 32.29% examples, 162297 words/s, in_qsize 30, out_qsize 2\n","2024-03-05 21:33:01,654 : INFO : EPOCH 2: training on 1279712 raw words (564632 effective words) took 1.9s, 302439 effective words/s\n","2024-03-05 21:33:02,682 : INFO : EPOCH 3 - PROGRESS: at 30.58% examples, 174419 words/s, in_qsize 31, out_qsize 0\n","2024-03-05 21:33:03,494 : INFO : EPOCH 3: training on 1279712 raw words (564900 effective words) took 1.8s, 310645 effective words/s\n","2024-03-05 21:33:04,603 : INFO : EPOCH 4 - PROGRESS: at 31.43% examples, 164412 words/s, in_qsize 31, out_qsize 0\n","2024-03-05 21:33:05,668 : INFO : EPOCH 4: training on 1279712 raw words (564745 effective words) took 2.2s, 261491 effective words/s\n","2024-03-05 21:33:06,798 : INFO : EPOCH 5 - PROGRESS: at 32.23% examples, 172269 words/s, in_qsize 31, out_qsize 0\n","2024-03-05 21:33:07,860 : INFO : EPOCH 5: training on 1279712 raw words (564737 effective words) took 2.1s, 264812 effective words/s\n","2024-03-05 21:33:08,894 : INFO : EPOCH 6 - PROGRESS: at 25.74% examples, 147492 words/s, in_qsize 31, out_qsize 4\n","2024-03-05 21:33:09,834 : INFO : EPOCH 6: training on 1279712 raw words (564610 effective words) took 2.0s, 288960 effective words/s\n","2024-03-05 21:33:10,864 : INFO : EPOCH 7 - PROGRESS: at 18.62% examples, 104539 words/s, in_qsize 31, out_qsize 0\n","2024-03-05 21:33:12,124 : INFO : EPOCH 7: training on 1279712 raw words (565102 effective words) took 2.3s, 249142 effective words/s\n","2024-03-05 21:33:13,140 : INFO : EPOCH 8 - PROGRESS: at 15.72% examples, 87050 words/s, in_qsize 32, out_qsize 0\n","2024-03-05 21:33:14,479 : INFO : EPOCH 8: training on 1279712 raw words (564780 effective words) took 2.3s, 241098 effective words/s\n","2024-03-05 21:33:15,533 : INFO : EPOCH 9 - PROGRESS: at 26.49% examples, 148565 words/s, in_qsize 32, out_qsize 0\n","2024-03-05 21:33:16,635 : INFO : EPOCH 9: training on 1279712 raw words (564896 effective words) took 2.1s, 264275 effective words/s\n","2024-03-05 21:33:17,685 : INFO : EPOCH 10 - PROGRESS: at 18.58% examples, 101760 words/s, in_qsize 31, out_qsize 1\n","2024-03-05 21:33:18,886 : INFO : EPOCH 10: training on 1279712 raw words (564772 effective words) took 2.2s, 252953 effective words/s\n","2024-03-05 21:33:19,975 : INFO : EPOCH 11 - PROGRESS: at 28.11% examples, 150714 words/s, in_qsize 26, out_qsize 1\n","2024-03-05 21:33:21,249 : INFO : EPOCH 11: training on 1279712 raw words (564343 effective words) took 2.4s, 239968 effective words/s\n","2024-03-05 21:33:22,327 : INFO : EPOCH 12 - PROGRESS: at 29.75% examples, 161007 words/s, in_qsize 31, out_qsize 0\n","2024-03-05 21:33:23,359 : INFO : EPOCH 12: training on 1279712 raw words (564567 effective words) took 2.1s, 269657 effective words/s\n","2024-03-05 21:33:24,404 : INFO : EPOCH 13 - PROGRESS: at 18.73% examples, 102112 words/s, in_qsize 26, out_qsize 8\n","2024-03-05 21:33:25,486 : INFO : EPOCH 13: training on 1279712 raw words (564376 effective words) took 2.1s, 267191 effective words/s\n","2024-03-05 21:33:26,551 : INFO : EPOCH 14 - PROGRESS: at 28.99% examples, 159556 words/s, in_qsize 31, out_qsize 0\n","2024-03-05 21:33:27,344 : INFO : EPOCH 14: training on 1279712 raw words (564487 effective words) took 1.8s, 307556 effective words/s\n","2024-03-05 21:33:27,344 : INFO : FastText lifecycle event {'msg': 'training on 19195680 raw words (8469738 effective words) took 31.4s, 269704 effective words/s', 'datetime': '2024-03-05T21:33:27.344900', 'gensim': '4.3.2', 'python': '3.11.7 (main, Jan  8 2024, 06:21:35) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'train'}\n"]},{"name":"stdout","output_type":"stream","text":["Time to train the model: 0.54 mins\n"]}],"source":["t = time()\n","simpsons_ft.train(sentences, total_examples=simpsons_ft.corpus_count, epochs=15, report_delay=10)\n","print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"]},{"cell_type":"markdown","metadata":{"id":"-Lr8U5wOTNcr"},"source":["**Pregunta 2**: Encuentre las palabras mas similares a las siguientes: Lisa, Bart, Homer, Marge. C칰al es la diferencia entre ambos resultados? Por qu칠 ocurre esto? Intente comparar ahora Liisa en ambos modelos (doble i). Cuando escoger칤a uno vs el otro? **(0.5 puntos)**"]},{"cell_type":"markdown","metadata":{"id":"yMLyGffVTNcs"},"source":["**Respuesta**:"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["def most_similar(word: str, w2v: Word2Vec, ft: FastText) -> pd.DataFrame:\n","    \"\"\"Compara las 10 palabras m치s similares a 'word' entre Word2Vec y FastText\n","\n","    Parameters\n","    ----------\n","    word : str\n","        Palabra a la que se le desea encontrar las palabras m치s similares\n","    w2v : Word2Vec\n","        Word2Vec entrenado\n","    ft : FastText\n","        FastText entrenado\n","\n","    Returns\n","    -------\n","    pd.DataFrame\n","        DataFrame con los resultados m치s similares\n","    \"\"\"\n","    columns = [(\"Word2Vec\", \"Word\"), (\"Word2Vec\", \"Value\"), (\"FastText\", \"Word\"), (\"FastText\", \"Value\")]\n","    w2v_list = w2v.wv.most_similar(positive=[word])\n","    ft_list = ft.wv.most_similar(positive=[word])\n","    w2v_words, w2v_values = zip(*w2v_list)\n","    ft_words, ft_values = zip(*ft_list)\n","    comp_df = pd.DataFrame(zip(w2v_words, w2v_values, ft_words, ft_values), columns=columns, index=range(1,11))\n","    comp_df.columns = pd.MultiIndex.from_tuples(comp_df.columns)\n","    return comp_df"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"w6RvJGpbTNcs"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead tr th {\n","        text-align: left;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th></th>\n","      <th colspan=\"2\" halign=\"left\">Word2Vec</th>\n","      <th colspan=\"2\" halign=\"left\">FastText</th>\n","    </tr>\n","    <tr>\n","      <th></th>\n","      <th>Word</th>\n","      <th>Value</th>\n","      <th>Word</th>\n","      <th>Value</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>bart</td>\n","      <td>0.827519</td>\n","      <td>lis</td>\n","      <td>0.881775</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>maggie</td>\n","      <td>0.802026</td>\n","      <td>lisui</td>\n","      <td>0.872436</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>sweetie</td>\n","      <td>0.759638</td>\n","      <td>little_girl</td>\n","      <td>0.792820</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>mom</td>\n","      <td>0.732529</td>\n","      <td>mom</td>\n","      <td>0.761338</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>ned</td>\n","      <td>0.731611</td>\n","      <td>womie</td>\n","      <td>0.731402</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>your_father</td>\n","      <td>0.715023</td>\n","      <td>honey</td>\n","      <td>0.731129</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>marge</td>\n","      <td>0.713937</td>\n","      <td>lie</td>\n","      <td>0.731030</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>milhouse</td>\n","      <td>0.711964</td>\n","      <td>apu</td>\n","      <td>0.726771</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>honey</td>\n","      <td>0.710189</td>\n","      <td>dad</td>\n","      <td>0.723257</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>alex</td>\n","      <td>0.706531</td>\n","      <td>milhousey</td>\n","      <td>0.716130</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       Word2Vec               FastText          \n","           Word     Value         Word     Value\n","1          bart  0.827519          lis  0.881775\n","2        maggie  0.802026        lisui  0.872436\n","3       sweetie  0.759638  little_girl  0.792820\n","4           mom  0.732529          mom  0.761338\n","5           ned  0.731611        womie  0.731402\n","6   your_father  0.715023        honey  0.731129\n","7         marge  0.713937          lie  0.731030\n","8      milhouse  0.711964          apu  0.726771\n","9         honey  0.710189          dad  0.723257\n","10         alex  0.706531    milhousey  0.716130"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["most_similar(\"lisa\", simpsons_w2v, simpsons_ft)"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead tr th {\n","        text-align: left;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th></th>\n","      <th colspan=\"2\" halign=\"left\">Word2Vec</th>\n","      <th colspan=\"2\" halign=\"left\">FastText</th>\n","    </tr>\n","    <tr>\n","      <th></th>\n","      <th>Word</th>\n","      <th>Value</th>\n","      <th>Word</th>\n","      <th>Value</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>lisa</td>\n","      <td>0.827519</td>\n","      <td>bart_d</td>\n","      <td>0.861538</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>mom</td>\n","      <td>0.804480</td>\n","      <td>bartdude</td>\n","      <td>0.851226</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>maggie</td>\n","      <td>0.770294</td>\n","      <td>baaart</td>\n","      <td>0.837848</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>dad</td>\n","      <td>0.766192</td>\n","      <td>barto</td>\n","      <td>0.834909</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>milhouse</td>\n","      <td>0.731611</td>\n","      <td>dart</td>\n","      <td>0.833009</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>your_father</td>\n","      <td>0.730824</td>\n","      <td>kvart</td>\n","      <td>0.830519</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>homie</td>\n","      <td>0.721960</td>\n","      <td>beelzebart</td>\n","      <td>0.823468</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>youse</td>\n","      <td>0.703928</td>\n","      <td>kart</td>\n","      <td>0.812005</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>homer</td>\n","      <td>0.700669</td>\n","      <td>mozart</td>\n","      <td>0.805260</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>bongo</td>\n","      <td>0.693523</td>\n","      <td>bartolo</td>\n","      <td>0.797270</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       Word2Vec              FastText          \n","           Word     Value        Word     Value\n","1          lisa  0.827519      bart_d  0.861538\n","2           mom  0.804480    bartdude  0.851226\n","3        maggie  0.770294      baaart  0.837848\n","4           dad  0.766192       barto  0.834909\n","5      milhouse  0.731611        dart  0.833009\n","6   your_father  0.730824       kvart  0.830519\n","7         homie  0.721960  beelzebart  0.823468\n","8         youse  0.703928        kart  0.812005\n","9         homer  0.700669      mozart  0.805260\n","10        bongo  0.693523     bartolo  0.797270"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["most_similar(\"bart\", simpsons_w2v, simpsons_ft)"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead tr th {\n","        text-align: left;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th></th>\n","      <th colspan=\"2\" halign=\"left\">Word2Vec</th>\n","      <th colspan=\"2\" halign=\"left\">FastText</th>\n","    </tr>\n","    <tr>\n","      <th></th>\n","      <th>Word</th>\n","      <th>Value</th>\n","      <th>Word</th>\n","      <th>Value</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>marge</td>\n","      <td>0.823189</td>\n","      <td>homelier</td>\n","      <td>0.926906</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>ned</td>\n","      <td>0.750561</td>\n","      <td>homemaker</td>\n","      <td>0.862293</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>dad</td>\n","      <td>0.745015</td>\n","      <td>homewrecker</td>\n","      <td>0.851214</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>abe</td>\n","      <td>0.739446</td>\n","      <td>hooomer</td>\n","      <td>0.817440</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>homie</td>\n","      <td>0.735338</td>\n","      <td>gomer</td>\n","      <td>0.800946</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>sweetheart</td>\n","      <td>0.730065</td>\n","      <td>misnomer</td>\n","      <td>0.795923</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>ralphie</td>\n","      <td>0.711727</td>\n","      <td>customer</td>\n","      <td>0.794764</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>bongo</td>\n","      <td>0.707165</td>\n","      <td>awesomer</td>\n","      <td>0.794013</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>joking</td>\n","      <td>0.706133</td>\n","      <td>homeo</td>\n","      <td>0.778768</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>bart</td>\n","      <td>0.700669</td>\n","      <td>groomer</td>\n","      <td>0.776997</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      Word2Vec               FastText          \n","          Word     Value         Word     Value\n","1        marge  0.823189     homelier  0.926906\n","2          ned  0.750561    homemaker  0.862293\n","3          dad  0.745015  homewrecker  0.851214\n","4          abe  0.739446      hooomer  0.817440\n","5        homie  0.735338        gomer  0.800946\n","6   sweetheart  0.730065     misnomer  0.795923\n","7      ralphie  0.711727     customer  0.794764\n","8        bongo  0.707165     awesomer  0.794013\n","9       joking  0.706133        homeo  0.778768\n","10        bart  0.700669      groomer  0.776997"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["most_similar(\"homer\", simpsons_w2v, simpsons_ft)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead tr th {\n","        text-align: left;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th></th>\n","      <th colspan=\"2\" halign=\"left\">Word2Vec</th>\n","      <th colspan=\"2\" halign=\"left\">FastText</th>\n","    </tr>\n","    <tr>\n","      <th></th>\n","      <th>Word</th>\n","      <th>Value</th>\n","      <th>Word</th>\n","      <th>Value</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>homer</td>\n","      <td>0.823189</td>\n","      <td>marge_d</td>\n","      <td>0.929213</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>homie</td>\n","      <td>0.797105</td>\n","      <td>_marge</td>\n","      <td>0.886489</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>abe</td>\n","      <td>0.747796</td>\n","      <td>maarge</td>\n","      <td>0.872063</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>bongo</td>\n","      <td>0.747299</td>\n","      <td>margaret</td>\n","      <td>0.829152</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>sweetheart</td>\n","      <td>0.741278</td>\n","      <td>marriott</td>\n","      <td>0.824199</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>becky</td>\n","      <td>0.736347</td>\n","      <td>marriot</td>\n","      <td>0.823181</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>dad</td>\n","      <td>0.733117</td>\n","      <td>margie</td>\n","      <td>0.819057</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>ned</td>\n","      <td>0.731311</td>\n","      <td>maaarge</td>\n","      <td>0.813767</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>lisa</td>\n","      <td>0.713937</td>\n","      <td>margin</td>\n","      <td>0.810192</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>greta</td>\n","      <td>0.713150</td>\n","      <td>ma_</td>\n","      <td>0.797185</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      Word2Vec            FastText          \n","          Word     Value      Word     Value\n","1        homer  0.823189   marge_d  0.929213\n","2        homie  0.797105    _marge  0.886489\n","3          abe  0.747796    maarge  0.872063\n","4        bongo  0.747299  margaret  0.829152\n","5   sweetheart  0.741278  marriott  0.824199\n","6        becky  0.736347   marriot  0.823181\n","7          dad  0.733117    margie  0.819057\n","8          ned  0.731311   maaarge  0.813767\n","9         lisa  0.713937    margin  0.810192\n","10       greta  0.713150       ma_  0.797185"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["most_similar(\"marge\", simpsons_w2v, simpsons_ft)"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"ename":"KeyError","evalue":"\"Key 'liisa' not present in vocabulary\"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmost_similar\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mliisa\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimpsons_w2v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimpsons_ft\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[1;32mIn[16], line 19\u001b[0m, in \u001b[0;36mmost_similar\u001b[1;34m(word, w2v, ft)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compara las 10 palabras m치s similares a 'word' entre Word2Vec y FastText\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03m    DataFrame con los resultados m치s similares\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     18\u001b[0m columns \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWord2Vec\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m\"\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWord2Vec\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValue\u001b[39m\u001b[38;5;124m\"\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFastText\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m\"\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFastText\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValue\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m---> 19\u001b[0m w2v_list \u001b[38;5;241m=\u001b[39m \u001b[43mw2v\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmost_similar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpositive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mword\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m ft_list \u001b[38;5;241m=\u001b[39m ft\u001b[38;5;241m.\u001b[39mwv\u001b[38;5;241m.\u001b[39mmost_similar(positive\u001b[38;5;241m=\u001b[39m[word])\n\u001b[0;32m     21\u001b[0m w2v_words, w2v_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mw2v_list)\n","File \u001b[1;32mc:\\Users\\esteban\\Documents\\Codes\\CC6205-assignments\\.venv\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:841\u001b[0m, in \u001b[0;36mKeyedVectors.most_similar\u001b[1;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[0;32m    838\u001b[0m         weight[idx] \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    840\u001b[0m \u001b[38;5;66;03m# compute the weighted average of all keys\u001b[39;00m\n\u001b[1;32m--> 841\u001b[0m mean \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_mean_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_normalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpost_normalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_missing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    842\u001b[0m all_keys \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    843\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_index(key) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m keys \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, _KEY_TYPES) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_index_for(key)\n\u001b[0;32m    844\u001b[0m ]\n\u001b[0;32m    846\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indexer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(topn, \u001b[38;5;28mint\u001b[39m):\n","File \u001b[1;32mc:\\Users\\esteban\\Documents\\Codes\\CC6205-assignments\\.venv\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:518\u001b[0m, in \u001b[0;36mKeyedVectors.get_mean_vector\u001b[1;34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001b[0m\n\u001b[0;32m    516\u001b[0m         total_weight \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mabs\u001b[39m(weights[idx])\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ignore_missing:\n\u001b[1;32m--> 518\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not present in vocabulary\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total_weight \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    521\u001b[0m     mean \u001b[38;5;241m=\u001b[39m mean \u001b[38;5;241m/\u001b[39m total_weight\n","\u001b[1;31mKeyError\u001b[0m: \"Key 'liisa' not present in vocabulary\""]}],"source":["most_similar(\"liisa\", simpsons_w2v, simpsons_ft)"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"data":{"text/plain":["[('lisui', 0.9513565301895142),\n"," ('lieu', 0.9166699647903442),\n"," ('liy칚', 0.9128382802009583),\n"," ('liugi', 0.9084550142288208),\n"," ('liz', 0.9067040085792542),\n"," ('liii', 0.8914716839790344),\n"," ('liza', 0.8842250108718872),\n"," ('lisa', 0.8714575171470642),\n"," ('liiiiisa', 0.8702735900878906),\n"," ('li', 0.8644471764564514)]"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["simpsons_ft.wv.most_similar(positive=[\"liisa\"])"]},{"cell_type":"markdown","metadata":{},"source":["FastText suele tener mejor rendimiento en tareas sint치cticas, mientras que Word2Vec puede entregar mejor performance en tareas sem치nticas. Adem치s, FastText tiene una ligera ventaja cuando el conjunto de entrenamiento es peque침o, aunque estas diferencias se acortan a medida que se tienen m치s datos de entrenamiento. Fuente: [Word2Vec_FastText_Comparison.ipynb](https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Word2Vec_FastText_Comparison.ipynb)\n","\n","Adem치s de lo anterior, FastText tiene la ventaja de ser capaz de obtener embedding de palabras fuera del vocabulario, de modo que no ocurre lo mismo que en el ejemplo de \"liisa\". Si existe alguna tarea en la que obtener un vocabulario extenso durante entrenamiento es un problema, se puede usar FastText para evitar tener embedding vacios o poco representativos de cada documento en inferencia.\n","\n","Tambi칠n se debe tomar en cuenta que FastText busca subwords, y como se ha apreciado en los resultados anteriores las palabras m치s cercanas no son en realidad palabras, sino subpalabras."]},{"cell_type":"markdown","metadata":{"id":"IRCB-jqgTNcs"},"source":["### **Parte 4 (1 Punto): Aplicar embeddings para clasificar**"]},{"cell_type":"markdown","metadata":{"id":"zlqzlJRSTNcs"},"source":["Ahora utilizaremos los embeddings que acabamos de calcular para clasificar palabras basadas en su polaridad (positivas o negativas). \n","\n","Para esto ocuparemos el lexic칩n AFINN incluido en la tarea, que incluye una lista de palabras y un 1 si su connotaci칩n es positiva y un -1 si es negativa."]},{"cell_type":"code","execution_count":23,"metadata":{"id":"CMskFDmHTNcs"},"outputs":[],"source":["AFINN = '../../data/AFINN_full.csv'\n","df_afinn = pd.read_csv(AFINN, sep='\\t', header=None)"]},{"cell_type":"markdown","metadata":{"id":"uaKl8hsCTNcs"},"source":["Hint: Para w2v son esperables KeyErrors debido a que no todas las palabras del corpus de los simpsons tendr치n una representaci칩n en AFINN. Pueden utilizar esta funci칩n auxiliar para filtrar las filas en el dataframe que no tienen embeddings (como w2v no tiene token UNK se deben ignorar)."]},{"cell_type":"code","execution_count":24,"metadata":{"id":"tWSSuctiTNcs"},"outputs":[],"source":["def try_apply(model, word):\n","    try:\n","        embedding = model.wv[word]\n","        return embedding\n","    except KeyError:\n","        #logger.error('Word {} not in dictionary'.format(word))\n","        return np.random.random(200)"]},{"cell_type":"markdown","metadata":{"id":"LrVPeEzgTNcs"},"source":["**Pregunta 1**: Transforme las palabras del corpus de AFINN a la representaci칩n en embedding que acabamos de calcular (con ambos modelos). \n","\n","Su dataframe final debe ser del estilo [embedding, sentimiento], donde los embeddings corresponden a $X$ y el sentimiento asociado con el embedding a $y$ (positivo/negativo, 1/-1). \n","\n","Para ambos modelos, separar train y test de acuerdo a la siguiente funci칩n. **(0.75 puntos)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Bkt26BwTNcs"},"outputs":[],"source":["# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.1, stratify=y)"]},{"cell_type":"markdown","metadata":{"id":"iDcq5czXTNct"},"source":["**Respuesta**:"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"upAn_eT4TNct"},"outputs":[],"source":["get_embds = lambda word: try_apply(simpsons_w2v, word)  # noqa: E731\n","\n","X_w2v = np.array(df_afinn[0].apply(func=get_embds).to_list())\n","y_w2v = df_afinn[1]\n","X_ft = np.array(df_afinn[0].apply(lambda word: simpsons_ft.wv[word]).to_list())\n","y_ft = df_afinn[1]"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["X_w2v_train, X_w2v_test, y_w2v_train, y_w2v_test = train_test_split(X_w2v, y_w2v, random_state=42, test_size=0.1, stratify=y_w2v)\n","X_ft_train, X_ft_test, y_ft_train, y_ft_test = train_test_split(X_ft, y_ft, random_state=42, test_size=0.1, stratify=y_ft)"]},{"cell_type":"markdown","metadata":{"id":"kDKe4gA3TNct"},"source":["**Pregunta 2**: Entrenar una regresi칩n log칤stica (vista en auxiliar) y reportar accuracy, precision, recall, f1 y confusion_matrix para ambos modelos. Por qu칠 se obtienen estos resultados? C칩mo los mejorar칤as? **(0.75 puntos)**"]},{"cell_type":"markdown","metadata":{"id":"hJMzq_dETNct"},"source":["**Respuesta**:"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"data":{"text/html":["<style>#sk-container-id-1 {\n","  /* Definition of color scheme common for light and dark mode */\n","  --sklearn-color-text: black;\n","  --sklearn-color-line: gray;\n","  /* Definition of color scheme for unfitted estimators */\n","  --sklearn-color-unfitted-level-0: #fff5e6;\n","  --sklearn-color-unfitted-level-1: #f6e4d2;\n","  --sklearn-color-unfitted-level-2: #ffe0b3;\n","  --sklearn-color-unfitted-level-3: chocolate;\n","  /* Definition of color scheme for fitted estimators */\n","  --sklearn-color-fitted-level-0: #f0f8ff;\n","  --sklearn-color-fitted-level-1: #d4ebff;\n","  --sklearn-color-fitted-level-2: #b3dbfd;\n","  --sklearn-color-fitted-level-3: cornflowerblue;\n","\n","  /* Specific color for light theme */\n","  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n","  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n","  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n","  --sklearn-color-icon: #696969;\n","\n","  @media (prefers-color-scheme: dark) {\n","    /* Redefinition of color scheme for dark theme */\n","    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n","    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n","    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n","    --sklearn-color-icon: #878787;\n","  }\n","}\n","\n","#sk-container-id-1 {\n","  color: var(--sklearn-color-text);\n","}\n","\n","#sk-container-id-1 pre {\n","  padding: 0;\n","}\n","\n","#sk-container-id-1 input.sk-hidden--visually {\n","  border: 0;\n","  clip: rect(1px 1px 1px 1px);\n","  clip: rect(1px, 1px, 1px, 1px);\n","  height: 1px;\n","  margin: -1px;\n","  overflow: hidden;\n","  padding: 0;\n","  position: absolute;\n","  width: 1px;\n","}\n","\n","#sk-container-id-1 div.sk-dashed-wrapped {\n","  border: 1px dashed var(--sklearn-color-line);\n","  margin: 0 0.4em 0.5em 0.4em;\n","  box-sizing: border-box;\n","  padding-bottom: 0.4em;\n","  background-color: var(--sklearn-color-background);\n","}\n","\n","#sk-container-id-1 div.sk-container {\n","  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n","     but bootstrap.min.css set `[hidden] { display: none !important; }`\n","     so we also need the `!important` here to be able to override the\n","     default hidden behavior on the sphinx rendered scikit-learn.org.\n","     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n","  display: inline-block !important;\n","  position: relative;\n","}\n","\n","#sk-container-id-1 div.sk-text-repr-fallback {\n","  display: none;\n","}\n","\n","div.sk-parallel-item,\n","div.sk-serial,\n","div.sk-item {\n","  /* draw centered vertical line to link estimators */\n","  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n","  background-size: 2px 100%;\n","  background-repeat: no-repeat;\n","  background-position: center center;\n","}\n","\n","/* Parallel-specific style estimator block */\n","\n","#sk-container-id-1 div.sk-parallel-item::after {\n","  content: \"\";\n","  width: 100%;\n","  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n","  flex-grow: 1;\n","}\n","\n","#sk-container-id-1 div.sk-parallel {\n","  display: flex;\n","  align-items: stretch;\n","  justify-content: center;\n","  background-color: var(--sklearn-color-background);\n","  position: relative;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item {\n","  display: flex;\n","  flex-direction: column;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item:first-child::after {\n","  align-self: flex-end;\n","  width: 50%;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item:last-child::after {\n","  align-self: flex-start;\n","  width: 50%;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item:only-child::after {\n","  width: 0;\n","}\n","\n","/* Serial-specific style estimator block */\n","\n","#sk-container-id-1 div.sk-serial {\n","  display: flex;\n","  flex-direction: column;\n","  align-items: center;\n","  background-color: var(--sklearn-color-background);\n","  padding-right: 1em;\n","  padding-left: 1em;\n","}\n","\n","\n","/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n","clickable and can be expanded/collapsed.\n","- Pipeline and ColumnTransformer use this feature and define the default style\n","- Estimators will overwrite some part of the style using the `sk-estimator` class\n","*/\n","\n","/* Pipeline and ColumnTransformer style (default) */\n","\n","#sk-container-id-1 div.sk-toggleable {\n","  /* Default theme specific background. It is overwritten whether we have a\n","  specific estimator or a Pipeline/ColumnTransformer */\n","  background-color: var(--sklearn-color-background);\n","}\n","\n","/* Toggleable label */\n","#sk-container-id-1 label.sk-toggleable__label {\n","  cursor: pointer;\n","  display: block;\n","  width: 100%;\n","  margin-bottom: 0;\n","  padding: 0.5em;\n","  box-sizing: border-box;\n","  text-align: center;\n","}\n","\n","#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n","  /* Arrow on the left of the label */\n","  content: \"郊\";\n","  float: left;\n","  margin-right: 0.25em;\n","  color: var(--sklearn-color-icon);\n","}\n","\n","#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n","  color: var(--sklearn-color-text);\n","}\n","\n","/* Toggleable content - dropdown */\n","\n","#sk-container-id-1 div.sk-toggleable__content {\n","  max-height: 0;\n","  max-width: 0;\n","  overflow: hidden;\n","  text-align: left;\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-toggleable__content.fitted {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-toggleable__content pre {\n","  margin: 0.2em;\n","  border-radius: 0.25em;\n","  color: var(--sklearn-color-text);\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n","  /* Expand drop-down */\n","  max-height: 200px;\n","  max-width: 100%;\n","  overflow: auto;\n","}\n","\n","#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n","  content: \"郊쬪";\n","}\n","\n","/* Pipeline/ColumnTransformer-specific style */\n","\n","#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Estimator-specific style */\n","\n","/* Colorize estimator box */\n","#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n","#sk-container-id-1 div.sk-label label {\n","  /* The background is the default theme color */\n","  color: var(--sklearn-color-text-on-default-background);\n","}\n","\n","/* On hover, darken the color of the background */\n","#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","/* Label box, darken color on hover, fitted */\n","#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Estimator label */\n","\n","#sk-container-id-1 div.sk-label label {\n","  font-family: monospace;\n","  font-weight: bold;\n","  display: inline-block;\n","  line-height: 1.2em;\n","}\n","\n","#sk-container-id-1 div.sk-label-container {\n","  text-align: center;\n","}\n","\n","/* Estimator-specific */\n","#sk-container-id-1 div.sk-estimator {\n","  font-family: monospace;\n","  border: 1px dotted var(--sklearn-color-border-box);\n","  border-radius: 0.25em;\n","  box-sizing: border-box;\n","  margin-bottom: 0.5em;\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-estimator.fitted {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","/* on hover */\n","#sk-container-id-1 div.sk-estimator:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-estimator.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Specification for estimator info (e.g. \"i\" and \"?\") */\n","\n","/* Common style for \"i\" and \"?\" */\n","\n",".sk-estimator-doc-link,\n","a:link.sk-estimator-doc-link,\n","a:visited.sk-estimator-doc-link {\n","  float: right;\n","  font-size: smaller;\n","  line-height: 1em;\n","  font-family: monospace;\n","  background-color: var(--sklearn-color-background);\n","  border-radius: 1em;\n","  height: 1em;\n","  width: 1em;\n","  text-decoration: none !important;\n","  margin-left: 1ex;\n","  /* unfitted */\n","  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-unfitted-level-1);\n","}\n","\n",".sk-estimator-doc-link.fitted,\n","a:link.sk-estimator-doc-link.fitted,\n","a:visited.sk-estimator-doc-link.fitted {\n","  /* fitted */\n","  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-fitted-level-1);\n","}\n","\n","/* On hover */\n","div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",".sk-estimator-doc-link:hover,\n","div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",".sk-estimator-doc-link:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",".sk-estimator-doc-link.fitted:hover,\n","div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",".sk-estimator-doc-link.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","/* Span, style for the box shown on hovering the info icon */\n",".sk-estimator-doc-link span {\n","  display: none;\n","  z-index: 9999;\n","  position: relative;\n","  font-weight: normal;\n","  right: .2ex;\n","  padding: .5ex;\n","  margin: .5ex;\n","  width: min-content;\n","  min-width: 20ex;\n","  max-width: 50ex;\n","  color: var(--sklearn-color-text);\n","  box-shadow: 2pt 2pt 4pt #999;\n","  /* unfitted */\n","  background: var(--sklearn-color-unfitted-level-0);\n","  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n","}\n","\n",".sk-estimator-doc-link.fitted span {\n","  /* fitted */\n","  background: var(--sklearn-color-fitted-level-0);\n","  border: var(--sklearn-color-fitted-level-3);\n","}\n","\n",".sk-estimator-doc-link:hover span {\n","  display: block;\n","}\n","\n","/* \"?\"-specific style due to the `<a>` HTML tag */\n","\n","#sk-container-id-1 a.estimator_doc_link {\n","  float: right;\n","  font-size: 1rem;\n","  line-height: 1em;\n","  font-family: monospace;\n","  background-color: var(--sklearn-color-background);\n","  border-radius: 1rem;\n","  height: 1rem;\n","  width: 1rem;\n","  text-decoration: none;\n","  /* unfitted */\n","  color: var(--sklearn-color-unfitted-level-1);\n","  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n","}\n","\n","#sk-container-id-1 a.estimator_doc_link.fitted {\n","  /* fitted */\n","  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-fitted-level-1);\n","}\n","\n","/* On hover */\n","#sk-container-id-1 a.estimator_doc_link:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-3);\n","}\n","</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LogisticRegression<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(max_iter=1000000)</pre></div> </div></div></div></div>"],"text/plain":["LogisticRegression(max_iter=1000000)"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["clf_w2v = LogisticRegression(max_iter=1000000)\n","clf_w2v.fit(X_w2v_train, y_w2v_train)\n","\n","\n","clf_ft = LogisticRegression(max_iter=1000000)\n","clf_ft.fit(X_ft_train, y_ft_train)"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[[193  28]\n"," [ 88  30]]\n"]}],"source":["y_w2v_pred = clf_w2v.predict(X_w2v_test)\n","conf_matrix_w2v = confusion_matrix(y_w2v_test, y_w2v_pred)\n","print(conf_matrix_w2v)"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[[192  29]\n"," [ 72  46]]\n"]}],"source":["y_ft_pred = clf_ft.predict(X_ft_test)\n","conf_matrix_ft = confusion_matrix(y_ft_test, y_ft_pred)\n","print(conf_matrix_ft)"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","          -1       0.69      0.87      0.77       221\n","           1       0.52      0.25      0.34       118\n","\n","    accuracy                           0.66       339\n","   macro avg       0.60      0.56      0.55       339\n","weighted avg       0.63      0.66      0.62       339\n","\n"]}],"source":["print(classification_report(y_w2v_test, y_w2v_pred))"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","          -1       0.73      0.87      0.79       221\n","           1       0.61      0.39      0.48       118\n","\n","    accuracy                           0.70       339\n","   macro avg       0.67      0.63      0.63       339\n","weighted avg       0.69      0.70      0.68       339\n","\n"]}],"source":["print(classification_report(y_ft_test, y_ft_pred))"]},{"cell_type":"markdown","metadata":{},"source":["Las m칠tricas globales favorecen al modelo de FastText, aunque la diferencia entre ambos no resulta considerable. Es dif칤cil dilucidar sobre los motivos que lleva a un model oser mejor sobre otro, cuando el conjunto de testeo posee menos de 500 elementos y podr칤a no ser representativo, se ha probado solamente un clasificador y sin variar los hiperpar치metros. Como se ha mencionado anteriormente, FastText debiese rendir mejor ante datasets con pocos datos, aunque en una tarea sem치ntica Word2Vec debiese entregar mejores resultados. Es posible que las palabras que no est치n presentes en el corpus y a las que se ha asignado un embedding aleatorio jueguen un rol en las m칠tricas, desfavoreciendo a Word2Vec."]},{"cell_type":"markdown","metadata":{"id":"izppruGQTNct"},"source":["# Bonus: +0.25 puntos en cualquier pregunta"]},{"cell_type":"markdown","metadata":{"id":"YW0aeK2KTNct"},"source":["**Pregunta 1**: Replicar la parte anterior utilizando embeddings pre-entrenados en un dataset m치s grande y obtener mejores resultados. Les puede servir [칠sta](https://radimrehurek.com/gensim/downloader.html#module-gensim.downloader) documentacion de gensim **(0.25 puntos)**."]},{"cell_type":"markdown","metadata":{"id":"qvHcVS3sTNct"},"source":["**Respuesta**:"]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":0}
