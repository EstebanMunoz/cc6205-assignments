{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYExzNuwx-Ke"
      },
      "source": [
        "# **Tarea 5 - Transformers y BERT 📚**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4wYf0vgnbTv"
      },
      "source": [
        "## **Preguntas teóricas 📕 (3 puntos).** ##\n",
        "Para estas preguntas no es necesario implementar código, pero pueden utilizar pseudo código."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcnN6ZlOzwLu"
      },
      "source": [
        "### **Parte 1: Arquitecturas de Redes Neuronales**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irsqBVmCnx3M"
      },
      "source": [
        "**Pregunta 1**:\n",
        "\n",
        "Explique el principal problema de las redes Elman recurrentes. Explique cada compuerta de las redes LSTM y la GRU.  **(0.75 puntos)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ">Las RNN Elman sufren el mismo problema que suelen tener algunas redes profundas: desvanecimiento del gradiente. Debido a la naturaleza recursiva de este tipo de red, existen muchas multiplicaciones que involucran valores $|w| < 1$, provocando que eventualmente los valores de desvanezcan. Por el contrario, si existe algún $|w| > 1$, recurrencias muy largas pueden producir el efecto de _exploding gradients_.\n",
        ">\n",
        ">Para compensar estos problemas, existen redes recurrentes que implementan _compuertas_, tales como las redes LSTM y GRU. Las redes LSTM utilizan 3 compuertas, 2 de ellas encargadas de actualizar la memoria de la red y 1 de filtrar cuánta memoria se transmite hacia el output.\n",
        ">- Input: Determina cuánto considerar del input actual para la memoria de la red.\n",
        ">- Forget: Determina cuánto considerar de la memoria anterior para la actual memoria.\n",
        ">- Output: Una vez calculada la memoria de la red, esta compuerta determina cuánto de esa memoria se utiliza en el output de la red.\n",
        ">\n",
        ">Las redes GRU surgieron posterior a las redes LSTM, con el objetivo de disminuir la complejidad de este último tipo de red. Por lo mismo, utiliza sólo 2 compuertas:\n",
        ">- Compuerta $r$: Utilizada para encontrar un candidato a la actualización del estado de la red.\n",
        ">- Compuerta $z$: Usada para obtener el output de la red, al interpolar el estado anterior con el estado propuesto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5FaWqBVvL90"
      },
      "source": [
        "**Pregunta 2**:\n",
        "\n",
        "Explique cuales son las diferencias entre las tres arquitecturas de sequence to sequence vistas en clases (Encoder-Decoder con RNN, Encoder-Decoder con RNN y Attention, y el Transformer) **(0.75 ptos)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ">Una arquitectura Encoder-Decoder RNN sin atención primero utiliza una red RNN como encoder, en la que una secuencia de tokens se usan como inputs y luego sólo se considera el output final $c_n$, el cual condensa toda la información de la cadena de tokens. A continuación una segunda RNN se usa como decoder, y concatena el input de la red con el output final del encoder, para luego obtener una salida y concatenarla con el output del encoder hasta el fin de la secuencia.\n",
        ">\n",
        ">El problema con este tipo de arquitecturas es que no se puede condensar toda la información en sólo un vector, lo que resulta problemático cuando se quiere utilizar una cadena larga de texto. Para solucionar este problema se puede usar atención, el que consiste en la utilización de una red biRNN como encoder, generando una secuencia de vectores $c_{i:n}$. En cuanto al decoder, para cada input de la secuencia ya no se usa solamente el output final del encoder, sino que una combinación convexa de los vectores $c_{i:n}$. La elección de los pesos $\\alpha$ de esta combinación dependen del tipo de atención que se elija, pero dependen del estado del decoder y de toda la secuencia $c_{i:n}$. Gracias a este mecanismo de atención, el decoder puede enfocarse en distintas partes del encoder dependiendo del momento en que se encuentre.\n",
        ">\n",
        ">La gran diferencia de Transformers respecto a las arquitecturas anteriores, es que transformers ya no usa redes recurrentes. En su lugar, usa de forma inteligente una capa de atención para realizar el cómputo de toda la secuencia en paralelo, evitando la necesidad de calcular el input anterior de la secuencia para obtener el output del actual. Para ello utiliza 3 matrices: _key_, _value_ y _query_ que juntas logran encapsular toda la información necesaria. Por medio de las matrices key y query se calcula un score para cada token de la secuencia, el cual se multiplica por la matriz value para obtener una representación del token ponderado por la importancia del mismo. Al sumar todas estas representaciones se obtiene el output del encoder. El decoder tiene un funcionamiento similar al encoder, siendo una diferencia la utilización de 2 mecanismos de atención: uno de ellos permite prestar atención a toda la secuencia, mientras que otro sólo a las partes importantes, de forma similar a cómo se ha explicado para el encoder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9q0SrZS8CqX"
      },
      "source": [
        "**Pregunta 3**:\n",
        "\n",
        "¿Cúal es el principal diferencia entre los Embeddings contextualizados (por ejemplo BERT) vs. los Embeddings estáticos (por ejemplo Word2Vec)? **(0.75 ptos)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ">Los embeddings contextualizados generan embeddings de un token de acuerdo a la secuencia de tokens en la que aparecen. Por otra parte, los embeddings estáticos generan siempre el mismo embedding para un token, independientemente del contexto en el que aparezcan. Por extensión, al poseer los tokens un único embedding, cada uno de ellos posee solo un significado, cuando en realidad cada palabra puede poseer múltiples significados o comportamiento de acuerdo a la sintaxis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cX0C2x5j8DNR"
      },
      "source": [
        "**Pregunta 4**:\n",
        "\n",
        "Explique en que tareas y las arquitecturas con las que se entrenan ELMO y BERT **(0.75 ptos)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ">ELMo se entrena en una tarea de Natural Language Model (NLM), la cual predice una palabra en base a la palabra anterior. El motivo detrás de la elección de esta tarea radica en que, al usar como input la palabra actual, la red en su estado intermedio creará una representación de esta palabra junto al contexto en el que se encuentra. Si en lugar de enfocarse en la tarea misma y se extraen estos vectores intermedios, es posible crear embeddings contextuales.\n",
        ">\n",
        "> La arquitectura de ELMo es un stack de 2 redes biLSTM. Para los embeddings iniciales se una una red CNN a nivel de caracteres, 2048 filtros de n-gramas de caracteres, 2 highway layers y una dimensionalidad de 512. Los estados constan de 4096 dimensiones y posee conexiones residuales.\n",
        ">\n",
        ">BERT se entrena para una tarea llamada Masked Language Modeling (MLM), similar a un modelo de lenguaje. La diferencia se encuentra en que en MLM, en lugar de predecir la siguiente palabra se enmascara una porcentaje del input, y se le pide a la red que reconstruya la secuencia. También se incorpota una segunda tarea que trata sobre _next sentence prediction_, en la que se le entrega al modelo 2 oraciones, y predice si la segunda oración sigue de la primera.\n",
        ">\n",
        ">A diferencia de ELMo, BERT se entrena usando un Transformer, obteniendo los embedding de los outputs del encoder. Existen originalmente 2 modelos: BERT-Base de 12 capas, 768 unidaes ocultas y 12 cabezas de atención. El otro modelo es BERT-Large, con 24 capas, 1024 unidades ocultas y 16 cabezas de atención."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocS_vQhR1gcU"
      },
      "source": [
        "## **Preguntas prácticas 💻 (3 puntos).** ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ol82nJ0FnmcP"
      },
      "source": [
        "### BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W28UEwvGYTbg"
      },
      "source": [
        "Lo primero es instalar las librerías necesarias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3X4Gbx7wYWDD"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertForNextSentencePrediction, BertForMaskedLM, BertForQuestionAnswering\n",
        "from torch.nn import functional as F\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIQo-VciYz2V"
      },
      "source": [
        "Para las preguntas que siguen, utilizaremos distintas variantes de BERT disponibles en la librería transformers. [Aquí](https://huggingface.co/transformers/model_doc/bert.html) pueden encontrar toda la documentación necesaria. El modelo pre-entrenado a utilizar es \"bert-base-uncased\" (salvo para question answering)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4_WHbz8bXx2"
      },
      "source": [
        "BERT es un modelo de lenguaje que fue entrenado exhaustivamente sobre dos tareas: 1) Next sentence prediction. 2) Masked language modeling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyMb4YZRMYkm"
      },
      "source": [
        "#### **BertForNextSentencePrediction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yt6CbBtyadRb"
      },
      "source": [
        "**Pregunta 1 (1 pto en total):**  Utilizando el modelo BertForNextSentencePrediction de la librería transformers, muestre cual de las 2 oraciones es **más probable** que sea una continuación de la primera. Para esto defina la función $oracion\\_mas\\_probable$, que recibe el inicio de una frase, las alternativas para continuar esta frase y retorna un string indicando cual de las dos oraciones es más probable **(0.25 puntos cada una)**.\n",
        "\n",
        "Por ejemplo:\n",
        "\n",
        "Initial: \"The sky is blue.\"\\\n",
        "A: \"This is due to the shorter wavelength of blue light.\"\\\n",
        "B: \"Chile is one of the world's greatest economies.\"\n",
        "\n",
        "Debería retornar \"La oración que continúa más probable es A\", justificándolo con la evaluación de BERT.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "IOX0bwser8OM"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "WoBKxPt-mz-e"
      },
      "outputs": [],
      "source": [
        "def oracion_mas_probable(first, sentA, sentB):\n",
        "    #Tu implementacion\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n",
        "    \n",
        "    # Tokenize the sentences\n",
        "    tokenized_A = tokenizer.encode_plus(first, sentA, return_tensors='pt')\n",
        "    tokenized_B = tokenizer.encode_plus(first, sentB, return_tensors='pt')\n",
        "    \n",
        "    # Calculate the probabilities\n",
        "    logits_A = model(**tokenized_A).logits\n",
        "    probs_A = F.softmax(logits_A, dim=1)\n",
        "    \n",
        "    logits_B = model(**tokenized_B).logits\n",
        "    probs_B = F.softmax(logits_B, dim=1)\n",
        "    \n",
        "    # Return the sentence with the higher probability\n",
        "    if probs_A[0,0] > probs_B[0,0]:\n",
        "        highest_prob = \"A\"\n",
        "    else:\n",
        "        highest_prob = \"B\"\n",
        "    \n",
        "    return f\"La oración que continúa más probable es {highest_prob}. Probabilidades: (A: {probs_A[0,0]}, B: {probs_B[0,0]})\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goXIGaief8Bi"
      },
      "source": [
        "1.1)\n",
        "Initial: \"My cat is fluffy.\"\\\n",
        "A: \"My dog has a curling tail.\"\\\n",
        "B: \"A song can make or ruin a person’s day if they let it get to them.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "zea5pybzf9x1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'La oración que continúa más probable es A. Probabilidades: (A: 0.9999964237213135, B: 0.0025219712406396866)'"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "oracion_mas_probable(\n",
        "    \"My cat is fluffy.\",\n",
        "    \"My dog has a curling tail.\",\n",
        "    \"A song can make or ruin a person’s day if they let it get to them.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHd7UzXpgCY-"
      },
      "source": [
        "1.2)\n",
        "Initial: \"The Big Apple is famous worldwide.\"\\\n",
        "A: \"You can add cinnamon for the perfect combination.\"\\\n",
        "B: \"It is America's largest city.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "a7XbCBDmgCJ5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'La oración que continúa más probable es B. Probabilidades: (A: 0.9999865293502808, B: 0.9999897480010986)'"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "oracion_mas_probable(\n",
        "    \"The Big Apple is famous worldwide.\",\n",
        "    \"You can add cinnamon for the perfect combination.\",\n",
        "    \"It is America's largest city.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1Y7oxM7d0jM"
      },
      "source": [
        "1.3)\n",
        "Initial: \"Roses are red.\"\\\n",
        "A: \"Violets are blue.\"\\\n",
        "B: \"Fertilize them regularly for impressive flowers.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "e6t1QxlOf-O7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'La oración que continúa más probable es A. Probabilidades: (A: 0.9999957084655762, B: 0.9999926090240479)'"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "oracion_mas_probable(\n",
        "    \"Roses are red.\",\n",
        "    \"Violets are blue.\",\n",
        "    \"Fertilize them regularly for impressive flowers.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgC5BcoDiIWV"
      },
      "source": [
        "1.4)\n",
        "Initial: \"I play videogames the whole day.\"\\\n",
        "A: \"They make me happy.\"\\\n",
        "B: \"They make me rage.\"\\"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "jJK7PhmwiItA"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'La oración que continúa más probable es A. Probabilidades: (A: 0.9999908208847046, B: 0.9999867677688599)'"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "oracion_mas_probable(\n",
        "    \"I play videogames the whole day.\",\n",
        "    \"They make me happy.\",\n",
        "    \"They make me rage.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3ArRo_zMdtr"
      },
      "source": [
        "#### **BertForMaskedLM**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCN8XM7qihx1"
      },
      "source": [
        "**Pregunta 2 (1 pto en total):**  Ahora utilizaremos BertForMaskedLM para **predecir una palabra oculta** en una oración. **(0.25 puntos cada una)**\\\n",
        "Por ejemplo:\\\n",
        "BERT input: \"I want to _ a new car.\"\\\n",
        "BERT prediction: \"buy\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "QZwFxbJOv_aW"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForMaskedLM.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "id": "QSGkdSokv-3G"
      },
      "outputs": [],
      "source": [
        "def palabra_mas_probable(sentence):\n",
        "    #Tu implementacion\n",
        "    # Index of the [MASK] token\n",
        "    mask_token_idx = 103\n",
        "\n",
        "    # Tokenize the sentence\n",
        "    indexed_tokens = tokenizer.encode(sentence, add_special_tokens=False)\n",
        "    tokens_tensor = torch.Tensor([indexed_tokens]).long()\n",
        "\n",
        "    # Find the index of the [MASK] token in the sentence\n",
        "    masked_idx = (tokens_tensor.flatten() == mask_token_idx).nonzero().item()\n",
        "\n",
        "    # Makes a prediction\n",
        "    predictions = model(tokens_tensor)\n",
        "\n",
        "    # Find the token with the highest probability\n",
        "    pred_idx = torch.argmax(predictions[0][0], dim=1)[masked_idx].item()\n",
        "    pred_token = tokenizer.convert_ids_to_tokens([pred_idx])[0]\n",
        "\n",
        "    print(f\"\"\"BERT input: \"{sentence}\"\\nBERT prediction: \"{pred_token}\\\"\"\"\")\n",
        "    return pred_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gua8HPhfkYOb"
      },
      "source": [
        "2.1)\n",
        "BERT input: \"[CLS] I love [MASK] . [SEP]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "id": "xKRFJnURk7ut"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BERT input: \"[CLS] I love [MASK]. [SEP]\"\n",
            "BERT prediction: \"you\"\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'you'"
            ]
          },
          "execution_count": 160,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "palabra_mas_probable(\"[CLS] I love [MASK]. [SEP]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7npsRd9Nk8hi"
      },
      "source": [
        "2.2)\n",
        "BERT input: \"[CLS] I hear that Karen is very [MASK] . [SEP]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "id": "EW7CosmKk87e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BERT input: \"[CLS] I hear that Karen is very [MASK] . [SEP]\"\n",
            "BERT prediction: \"upset\"\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'upset'"
            ]
          },
          "execution_count": 161,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "palabra_mas_probable(\"[CLS] I hear that Karen is very [MASK] . [SEP]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SqzWh8Wk9TD"
      },
      "source": [
        "2.3)\n",
        "BERT input: \"[CLS] She had the gift of being able to [MASK] . [SEP]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "K6YFd1Xpk9qd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BERT input: \"[CLS] She had the gift of being able to [MASK] . [SEP]\"\n",
            "BERT prediction: \"fly\"\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'fly'"
            ]
          },
          "execution_count": 162,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "palabra_mas_probable(\"[CLS] She had the gift of being able to [MASK] . [SEP]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoV_5suNk-1X"
      },
      "source": [
        "2.4)\n",
        "BERT input: \"[CLS] It's not often you find a [MASK] on the street. [SEP]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "id": "C8ayI5VIk_Hr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BERT input: \"[CLS] It's not often you find a [MASK] on the street. [SEP]\"\n",
            "BERT prediction: \"car\"\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'car'"
            ]
          },
          "execution_count": 163,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "palabra_mas_probable(\"[CLS] It's not often you find a [MASK] on the street. [SEP]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJFPpgoYMim5"
      },
      "source": [
        "#### **BertForQuestionAnswering**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmCSmftALYbA"
      },
      "source": [
        "**Pregunta 3 (1 pto):**  Utilizando el modelo BertForQuestionAnswering pre-entrenado con 'bert-large-uncased-whole-word-masking-finetuned-squad', **extraiga la respuesta** a cada una de las siguientes 4 preguntas y su contexto. **(0.25 puntos cada una)**. Recuerde cambiar el tokenizer para que coincida con el modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "id": "8UhKkKpyToFf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 206,
      "metadata": {
        "id": "JFejTyj98XDC"
      },
      "outputs": [],
      "source": [
        "def entregar_respuesta(qst, cntxt):\n",
        "    #Tu implementacion\n",
        "    inputs = tokenizer(qst, cntxt, return_tensors='pt')\n",
        "    outputs = model(**inputs)\n",
        "    start_logits = outputs.start_logits\n",
        "    end_logits = outputs.end_logits\n",
        "\n",
        "    # Find the slice of the input that contains the answer\n",
        "    answer_start = start_logits.argmax(dim=1).item()\n",
        "    answer_end = end_logits.argmax(dim=1).item()\n",
        "\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
        "    answer_tokens = tokens[answer_start:answer_end+1]\n",
        "\n",
        "    answer = tokenizer.convert_tokens_to_string(answer_tokens)\n",
        "    return answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7zizotdKVIF"
      },
      "source": [
        "3.1)\n",
        "\n",
        "Pregunta: \"When was the Battle of Iquique?\"\n",
        "\n",
        "Contexto: \"The Battle of Iquique was a naval engagement that occurred between a Chilean corvette under the command of Arturo Prat and a Peruvian ironclad under the command of Miguel Grau Seminario on 21 May 1879, during the naval stage of the War of the Pacific, and resulted in a Peruvian victory.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 207,
      "metadata": {
        "id": "q_TrDijrKnQH"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'21 may 1879'"
            ]
          },
          "execution_count": 207,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "entregar_respuesta(\n",
        "    \"When was the Battle of Iquique?\",\n",
        "    \"The Battle of Iquique was a naval engagement that occurred between a Chilean corvette under the command of Arturo Prat and a Peruvian ironclad under the command of Miguel Grau Seminario on 21 May 1879, during the naval stage of the War of the Pacific, and resulted in a Peruvian victory.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0au9XCqNB2TY"
      },
      "source": [
        "3.2)\n",
        "\n",
        "Pregunta: \"Who won the Battle of Iquique?\"\n",
        "\n",
        "Contexto: \"The Battle of Iquique was a naval engagement that occurred between a Chilean corvette under the command of Arturo Prat and a Peruvian ironclad under the command of Miguel Grau Seminario on 21 May 1879, during the naval stage of the War of the Pacific, and resulted in a Peruvian victory.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "metadata": {
        "id": "3DlTUMxAB_0n"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'peruvian'"
            ]
          },
          "execution_count": 208,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "entregar_respuesta(\n",
        "    \"Who won the Battle of Iquique?\",\n",
        "    \"The Battle of Iquique was a naval engagement that occurred between a Chilean corvette under the command of Arturo Prat and a Peruvian ironclad under the command of Miguel Grau Seminario on 21 May 1879, during the naval stage of the War of the Pacific, and resulted in a Peruvian victory.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryRajxniCKbp"
      },
      "source": [
        "3.3)\n",
        "\n",
        "Pregunta: \"Who introduced peephole connections to LSTM networks?\"\n",
        "Contexto: \"What I’ve described so far is a pretty normal LSTM. But not all LSTMs are the same as the above. In fact, it seems like almost every paper involving LSTMs uses a slightly different version. The differences are minor, but it’s worth mentioning some of them. One popular LSTM variant, introduced by Gers & Schmidhuber (2000), is adding “peephole connections.” This means that we let the gate layers look at the cell state.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "metadata": {
        "id": "S1rT9kgLCKuy"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'gers & schmidhuber'"
            ]
          },
          "execution_count": 209,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "entregar_respuesta(\n",
        "    \"Who introduced peephole connections to LSTM networks?\",\n",
        "    \"What I’ve described so far is a pretty normal LSTM. But not all LSTMs are the same as the above. In fact, it seems like almost every paper involving LSTMs uses a slightly different version. The differences are minor, but it’s worth mentioning some of them. One popular LSTM variant, introduced by Gers & Schmidhuber (2000), is adding “peephole connections.” This means that we let the gate layers look at the cell state.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqn0vNvKCLAq"
      },
      "source": [
        "3.4)\n",
        "\n",
        "Pregunta: \"When is the cat most active?\"\n",
        "\n",
        "Contexto: \"The cat is similar in anatomy to the other felid species: it has a strong flexible body, quick reflexes, sharp teeth and retractable claws adapted to killing small prey. Its night vision and sense of smell are well developed. Cat communication includes vocalizations like meowing, purring, trilling, hissing, growling and grunting as well as cat-specific body language. It is a solitary hunter but a social species. It can hear sounds too faint or too high in frequency for human ears, such as those made by mice and other small mammals. It is a predator that is most active at dawn and dusk. It secretes and perceives pheromones.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {
        "id": "o5n37FwOCLTG"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'dawn and dusk'"
            ]
          },
          "execution_count": 210,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "entregar_respuesta(\n",
        "    \"When is the cat most active?\",\n",
        "    \"The cat is similar in anatomy to the other felid species: it has a strong flexible body, quick reflexes, sharp teeth and retractable claws adapted to killing small prey. Its night vision and sense of smell are well developed. Cat communication includes vocalizations like meowing, purring, trilling, hissing, growling and grunting as well as cat-specific body language. It is a solitary hunter but a social species. It can hear sounds too faint or too high in frequency for human ears, such as those made by mice and other small mammals. It is a predator that is most active at dawn and dusk. It secretes and perceives pheromones.\"\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
