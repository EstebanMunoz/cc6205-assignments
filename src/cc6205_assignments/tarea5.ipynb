{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYExzNuwx-Ke"
      },
      "source": [
        "# **Tarea 5 - Transformers y BERT üìö**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4wYf0vgnbTv"
      },
      "source": [
        "## **Preguntas te√≥ricas üìï (3 puntos).** ##\n",
        "Para estas preguntas no es necesario implementar c√≥digo, pero pueden utilizar pseudo c√≥digo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcnN6ZlOzwLu"
      },
      "source": [
        "### **Parte 1: Arquitecturas de Redes Neuronales**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irsqBVmCnx3M"
      },
      "source": [
        "**Pregunta 1**:\n",
        "\n",
        "Explique el principal problema de las redes Elman recurrentes. Explique cada compuerta de las redes LSTM y la GRU.  **(0.75 puntos)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ">Las RNN Elman sufren el mismo problema que suelen tener algunas redes profundas: desvanecimiento del gradiente. Debido a la naturaleza recursiva de este tipo de red, existen muchas multiplicaciones que involucran valores $|w| < 1$, provocando que eventualmente los valores de desvanezcan. Por el contrario, si existe alg√∫n $|w| > 1$, recurrencias muy largas pueden producir el efecto de _exploding gradients_.\n",
        ">\n",
        ">Para compensar estos problemas, existen redes recurrentes que implementan _compuertas_, tales como las redes LSTM y GRU. Las redes LSTM utilizan 3 compuertas, 2 de ellas encargadas de actualizar la memoria de la red y 1 de filtrar cu√°nta memoria se transmite hacia el output.\n",
        ">- Input: Determina cu√°nto considerar del input actual para la memoria de la red.\n",
        ">- Forget: Determina cu√°nto considerar de la memoria anterior para la actual memoria.\n",
        ">- Output: Una vez calculada la memoria de la red, esta compuerta determina cu√°nto de esa memoria se utiliza en el output de la red.\n",
        ">\n",
        ">Las redes GRU surgieron posterior a las redes LSTM, con el objetivo de disminuir la complejidad de este √∫ltimo tipo de red. Por lo mismo, utiliza s√≥lo 2 compuertas:\n",
        ">- Compuerta $r$: Utilizada para encontrar un candidato a la actualizaci√≥n del estado de la red.\n",
        ">- Compuerta $z$: Usada para obtener el output de la red, al interpolar el estado anterior con el estado propuesto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5FaWqBVvL90"
      },
      "source": [
        "**Pregunta 2**:\n",
        "\n",
        "Explique cuales son las diferencias entre las tres arquitecturas de sequence to sequence vistas en clases (Encoder-Decoder con RNN, Encoder-Decoder con RNN y Attention, y el Transformer) **(0.75 ptos)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ">Una arquitectura Encoder-Decoder RNN sin atenci√≥n primero utiliza una red RNN como encoder, en la que una secuencia de tokens se usan como inputs y luego s√≥lo se considera el output final $c_n$, el cual condensa toda la informaci√≥n de la cadena de tokens. A continuaci√≥n una segunda RNN se usa como decoder, y concatena el input de la red con el output final del encoder, para luego obtener una salida y concatenarla con el output del encoder hasta el fin de la secuencia.\n",
        ">\n",
        ">El problema con este tipo de arquitecturas es que no se puede condensar toda la informaci√≥n en s√≥lo un vector, lo que resulta problem√°tico cuando se quiere utilizar una cadena larga de texto. Para solucionar este problema se puede usar atenci√≥n, el que consiste en la utilizaci√≥n de una red biRNN como encoder, generando una secuencia de vectores $c_{i:n}$. En cuanto al decoder, para cada input de la secuencia ya no se usa solamente el output final del encoder, sino que una combinaci√≥n convexa de los vectores $c_{i:n}$. La elecci√≥n de los pesos $\\alpha$ de esta combinaci√≥n dependen del tipo de atenci√≥n que se elija, pero dependen del estado del decoder y de toda la secuencia $c_{i:n}$. Gracias a este mecanismo de atenci√≥n, el decoder puede enfocarse en distintas partes del encoder dependiendo del momento en que se encuentre.\n",
        ">\n",
        ">La gran diferencia de Transformers respecto a las arquitecturas anteriores, es que transformers ya no usa redes recurrentes. En su lugar, usa de forma inteligente una capa de atenci√≥n para realizar el c√≥mputo de toda la secuencia en paralelo, evitando la necesidad de calcular el input anterior de la secuencia para obtener el output del actual. Para ello utiliza 3 matrices: _key_, _value_ y _query_ que juntas logran encapsular toda la informaci√≥n necesaria. Por medio de las matrices key y query se calcula un score para cada token de la secuencia, el cual se multiplica por la matriz value para obtener una representaci√≥n del token ponderado por la importancia del mismo. Al sumar todas estas representaciones se obtiene el output del encoder. El decoder tiene un funcionamiento similar al encoder, siendo una diferencia la utilizaci√≥n de 2 mecanismos de atenci√≥n: uno de ellos permite prestar atenci√≥n a toda la secuencia, mientras que otro s√≥lo a las partes importantes, de forma similar a c√≥mo se ha explicado para el encoder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9q0SrZS8CqX"
      },
      "source": [
        "**Pregunta 3**:\n",
        "\n",
        "¬øC√∫al es el principal diferencia entre los Embeddings contextualizados (por ejemplo BERT) vs. los Embeddings est√°ticos (por ejemplo Word2Vec)? **(0.75 ptos)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ">Los embeddings contextualizados generan embeddings de un token de acuerdo a la secuencia de tokens en la que aparecen. Por otra parte, los embeddings est√°ticos generan siempre el mismo embedding para un token, independientemente del contexto en el que aparezcan. Por extensi√≥n, al poseer los tokens un √∫nico embedding, cada uno de ellos posee solo un significado, cuando en realidad cada palabra puede poseer m√∫ltiples significados o comportamiento de acuerdo a la sintaxis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cX0C2x5j8DNR"
      },
      "source": [
        "**Pregunta 4**:\n",
        "\n",
        "Explique en que tareas y las arquitecturas con las que se entrenan ELMO y BERT **(0.75 ptos)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ">ELMo se entrena en una tarea de Natural Language Model (NLM), la cual predice una palabra en base a la palabra anterior. El motivo detr√°s de la elecci√≥n de esta tarea radica en que, al usar como input la palabra actual, la red en su estado intermedio crear√° una representaci√≥n de esta palabra junto al contexto en el que se encuentra. Si en lugar de enfocarse en la tarea misma y se extraen estos vectores intermedios, es posible crear embeddings contextuales.\n",
        ">\n",
        "> La arquitectura de ELMo es un stack de 2 redes biLSTM. Para los embeddings iniciales se una una red CNN a nivel de caracteres, 2048 filtros de n-gramas de caracteres, 2 highway layers y una dimensionalidad de 512. Los estados constan de 4096 dimensiones y posee conexiones residuales.\n",
        ">\n",
        ">BERT se entrena para una tarea llamada Masked Language Modeling (MLM), similar a un modelo de lenguaje. La diferencia se encuentra en que en MLM, en lugar de predecir la siguiente palabra se enmascara una porcentaje del input, y se le pide a la red que reconstruya la secuencia. Tambi√©n se incorpota una segunda tarea que trata sobre _next sentence prediction_, en la que se le entrega al modelo 2 oraciones, y predice si la segunda oraci√≥n sigue de la primera.\n",
        ">\n",
        ">A diferencia de ELMo, BERT se entrena usando un Transformer, obteniendo los embedding de los outputs del encoder. Existen originalmente 2 modelos: BERT-Base de 12 capas, 768 unidaes ocultas y 12 cabezas de atenci√≥n. El otro modelo es BERT-Large, con 24 capas, 1024 unidades ocultas y 16 cabezas de atenci√≥n."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocS_vQhR1gcU"
      },
      "source": [
        "## **Preguntas pr√°cticas üíª (3 puntos).** ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ol82nJ0FnmcP"
      },
      "source": [
        "### BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W28UEwvGYTbg"
      },
      "source": [
        "Lo primero es instalar las librer√≠as necesarias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3X4Gbx7wYWDD"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertForNextSentencePrediction, BertForMaskedLM, BertForQuestionAnswering\n",
        "from torch.nn import functional as F\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIQo-VciYz2V"
      },
      "source": [
        "Para las preguntas que siguen, utilizaremos distintas variantes de BERT disponibles en la librer√≠a transformers. [Aqu√≠](https://huggingface.co/transformers/model_doc/bert.html) pueden encontrar toda la documentaci√≥n necesaria. El modelo pre-entrenado a utilizar es \"bert-base-uncased\" (salvo para question answering)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4_WHbz8bXx2"
      },
      "source": [
        "BERT es un modelo de lenguaje que fue entrenado exhaustivamente sobre dos tareas: 1) Next sentence prediction. 2) Masked language modeling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyMb4YZRMYkm"
      },
      "source": [
        "#### **BertForNextSentencePrediction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yt6CbBtyadRb"
      },
      "source": [
        "**Pregunta 1 (1 pto en total):**  Utilizando el modelo BertForNextSentencePrediction de la librer√≠a transformers, muestre cual de las 2 oraciones es **m√°s probable** que sea una continuaci√≥n de la primera. Para esto defina la funci√≥n $oracion\\_mas\\_probable$, que recibe el inicio de una frase, las alternativas para continuar esta frase y retorna un string indicando cual de las dos oraciones es m√°s probable **(0.25 puntos cada una)**.\n",
        "\n",
        "Por ejemplo:\n",
        "\n",
        "Initial: \"The sky is blue.\"\\\n",
        "A: \"This is due to the shorter wavelength of blue light.\"\\\n",
        "B: \"Chile is one of the world's greatest economies.\"\n",
        "\n",
        "Deber√≠a retornar \"La oraci√≥n que contin√∫a m√°s probable es A\", justific√°ndolo con la evaluaci√≥n de BERT.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "IOX0bwser8OM"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "WoBKxPt-mz-e"
      },
      "outputs": [],
      "source": [
        "def oracion_mas_probable(first, sentA, sentB):\n",
        "    #Tu implementacion\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n",
        "    \n",
        "    # Tokenize the sentences\n",
        "    tokenized_A = tokenizer.encode_plus(first, sentA, return_tensors='pt')\n",
        "    tokenized_B = tokenizer.encode_plus(first, sentB, return_tensors='pt')\n",
        "    \n",
        "    # Calculate the probabilities\n",
        "    logits_A = model(**tokenized_A).logits\n",
        "    probs_A = F.softmax(logits_A, dim=1)\n",
        "    \n",
        "    logits_B = model(**tokenized_B).logits\n",
        "    probs_B = F.softmax(logits_B, dim=1)\n",
        "    \n",
        "    # Return the sentence with the higher probability\n",
        "    if probs_A[0,0] > probs_B[0,0]:\n",
        "        highest_prob = \"A\"\n",
        "    else:\n",
        "        highest_prob = \"B\"\n",
        "    \n",
        "    return f\"La oraci√≥n que contin√∫a m√°s probable es {highest_prob}. Probabilidades: (A: {probs_A[0,0]}, B: {probs_B[0,0]})\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goXIGaief8Bi"
      },
      "source": [
        "1.1)\n",
        "Initial: \"My cat is fluffy.\"\\\n",
        "A: \"My dog has a curling tail.\"\\\n",
        "B: \"A song can make or ruin a person‚Äôs day if they let it get to them.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "zea5pybzf9x1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'La oraci√≥n que contin√∫a m√°s probable es A. Probabilidades: (A: 0.9999964237213135, B: 0.0025219712406396866)'"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "oracion_mas_probable(\n",
        "    \"My cat is fluffy.\",\n",
        "    \"My dog has a curling tail.\",\n",
        "    \"A song can make or ruin a person‚Äôs day if they let it get to them.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHd7UzXpgCY-"
      },
      "source": [
        "1.2)\n",
        "Initial: \"The Big Apple is famous worldwide.\"\\\n",
        "A: \"You can add cinnamon for the perfect combination.\"\\\n",
        "B: \"It is America's largest city.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "a7XbCBDmgCJ5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'La oraci√≥n que contin√∫a m√°s probable es B. Probabilidades: (A: 0.9999865293502808, B: 0.9999897480010986)'"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "oracion_mas_probable(\n",
        "    \"The Big Apple is famous worldwide.\",\n",
        "    \"You can add cinnamon for the perfect combination.\",\n",
        "    \"It is America's largest city.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1Y7oxM7d0jM"
      },
      "source": [
        "1.3)\n",
        "Initial: \"Roses are red.\"\\\n",
        "A: \"Violets are blue.\"\\\n",
        "B: \"Fertilize them regularly for impressive flowers.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "e6t1QxlOf-O7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'La oraci√≥n que contin√∫a m√°s probable es A. Probabilidades: (A: 0.9999957084655762, B: 0.9999926090240479)'"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "oracion_mas_probable(\n",
        "    \"Roses are red.\",\n",
        "    \"Violets are blue.\",\n",
        "    \"Fertilize them regularly for impressive flowers.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgC5BcoDiIWV"
      },
      "source": [
        "1.4)\n",
        "Initial: \"I play videogames the whole day.\"\\\n",
        "A: \"They make me happy.\"\\\n",
        "B: \"They make me rage.\"\\"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "jJK7PhmwiItA"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'La oraci√≥n que contin√∫a m√°s probable es A. Probabilidades: (A: 0.9999908208847046, B: 0.9999867677688599)'"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "oracion_mas_probable(\n",
        "    \"I play videogames the whole day.\",\n",
        "    \"They make me happy.\",\n",
        "    \"They make me rage.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3ArRo_zMdtr"
      },
      "source": [
        "#### **BertForMaskedLM**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCN8XM7qihx1"
      },
      "source": [
        "**Pregunta 2 (1 pto en total):**  Ahora utilizaremos BertForMaskedLM para **predecir una palabra oculta** en una oraci√≥n. **(0.25 puntos cada una)**\\\n",
        "Por ejemplo:\\\n",
        "BERT input: \"I want to _ a new car.\"\\\n",
        "BERT prediction: \"buy\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "QZwFxbJOv_aW"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForMaskedLM.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "id": "QSGkdSokv-3G"
      },
      "outputs": [],
      "source": [
        "def palabra_mas_probable(sentence):\n",
        "    #Tu implementacion\n",
        "    # Index of the [MASK] token\n",
        "    mask_token_idx = 103\n",
        "\n",
        "    # Tokenize the sentence\n",
        "    indexed_tokens = tokenizer.encode(sentence, add_special_tokens=False)\n",
        "    tokens_tensor = torch.Tensor([indexed_tokens]).long()\n",
        "\n",
        "    # Find the index of the [MASK] token in the sentence\n",
        "    masked_idx = (tokens_tensor.flatten() == mask_token_idx).nonzero().item()\n",
        "\n",
        "    # Makes a prediction\n",
        "    predictions = model(tokens_tensor)\n",
        "\n",
        "    # Find the token with the highest probability\n",
        "    pred_idx = torch.argmax(predictions[0][0], dim=1)[masked_idx].item()\n",
        "    pred_token = tokenizer.convert_ids_to_tokens([pred_idx])[0]\n",
        "\n",
        "    print(f\"\"\"BERT input: \"{sentence}\"\\nBERT prediction: \"{pred_token}\\\"\"\"\")\n",
        "    return pred_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gua8HPhfkYOb"
      },
      "source": [
        "2.1)\n",
        "BERT input: \"[CLS] I love [MASK] . [SEP]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "id": "xKRFJnURk7ut"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BERT input: \"[CLS] I love [MASK]. [SEP]\"\n",
            "BERT prediction: \"you\"\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'you'"
            ]
          },
          "execution_count": 160,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "palabra_mas_probable(\"[CLS] I love [MASK]. [SEP]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7npsRd9Nk8hi"
      },
      "source": [
        "2.2)\n",
        "BERT input: \"[CLS] I hear that Karen is very [MASK] . [SEP]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "id": "EW7CosmKk87e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BERT input: \"[CLS] I hear that Karen is very [MASK] . [SEP]\"\n",
            "BERT prediction: \"upset\"\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'upset'"
            ]
          },
          "execution_count": 161,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "palabra_mas_probable(\"[CLS] I hear that Karen is very [MASK] . [SEP]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SqzWh8Wk9TD"
      },
      "source": [
        "2.3)\n",
        "BERT input: \"[CLS] She had the gift of being able to [MASK] . [SEP]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "K6YFd1Xpk9qd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BERT input: \"[CLS] She had the gift of being able to [MASK] . [SEP]\"\n",
            "BERT prediction: \"fly\"\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'fly'"
            ]
          },
          "execution_count": 162,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "palabra_mas_probable(\"[CLS] She had the gift of being able to [MASK] . [SEP]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoV_5suNk-1X"
      },
      "source": [
        "2.4)\n",
        "BERT input: \"[CLS] It's not often you find a [MASK] on the street. [SEP]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "id": "C8ayI5VIk_Hr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BERT input: \"[CLS] It's not often you find a [MASK] on the street. [SEP]\"\n",
            "BERT prediction: \"car\"\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'car'"
            ]
          },
          "execution_count": 163,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "palabra_mas_probable(\"[CLS] It's not often you find a [MASK] on the street. [SEP]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJFPpgoYMim5"
      },
      "source": [
        "#### **BertForQuestionAnswering**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmCSmftALYbA"
      },
      "source": [
        "**Pregunta 3 (1 pto):**  Utilizando el modelo BertForQuestionAnswering pre-entrenado con 'bert-large-uncased-whole-word-masking-finetuned-squad', **extraiga la respuesta** a cada una de las siguientes 4 preguntas y su contexto. **(0.25 puntos cada una)**. Recuerde cambiar el tokenizer para que coincida con el modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "id": "8UhKkKpyToFf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 206,
      "metadata": {
        "id": "JFejTyj98XDC"
      },
      "outputs": [],
      "source": [
        "def entregar_respuesta(qst, cntxt):\n",
        "    #Tu implementacion\n",
        "    inputs = tokenizer(qst, cntxt, return_tensors='pt')\n",
        "    outputs = model(**inputs)\n",
        "    start_logits = outputs.start_logits\n",
        "    end_logits = outputs.end_logits\n",
        "\n",
        "    # Find the slice of the input that contains the answer\n",
        "    answer_start = start_logits.argmax(dim=1).item()\n",
        "    answer_end = end_logits.argmax(dim=1).item()\n",
        "\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
        "    answer_tokens = tokens[answer_start:answer_end+1]\n",
        "\n",
        "    answer = tokenizer.convert_tokens_to_string(answer_tokens)\n",
        "    return answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7zizotdKVIF"
      },
      "source": [
        "3.1)\n",
        "\n",
        "Pregunta: \"When was the Battle of Iquique?\"\n",
        "\n",
        "Contexto: \"The Battle of Iquique was a naval engagement that occurred between a Chilean corvette under the command of Arturo Prat and a Peruvian ironclad under the command of Miguel Grau Seminario on 21 May 1879, during the naval stage of the War of the Pacific, and resulted in a Peruvian victory.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 207,
      "metadata": {
        "id": "q_TrDijrKnQH"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'21 may 1879'"
            ]
          },
          "execution_count": 207,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "entregar_respuesta(\n",
        "    \"When was the Battle of Iquique?\",\n",
        "    \"The Battle of Iquique was a naval engagement that occurred between a Chilean corvette under the command of Arturo Prat and a Peruvian ironclad under the command of Miguel Grau Seminario on 21 May 1879, during the naval stage of the War of the Pacific, and resulted in a Peruvian victory.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0au9XCqNB2TY"
      },
      "source": [
        "3.2)\n",
        "\n",
        "Pregunta: \"Who won the Battle of Iquique?\"\n",
        "\n",
        "Contexto: \"The Battle of Iquique was a naval engagement that occurred between a Chilean corvette under the command of Arturo Prat and a Peruvian ironclad under the command of Miguel Grau Seminario on 21 May 1879, during the naval stage of the War of the Pacific, and resulted in a Peruvian victory.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "metadata": {
        "id": "3DlTUMxAB_0n"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'peruvian'"
            ]
          },
          "execution_count": 208,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "entregar_respuesta(\n",
        "    \"Who won the Battle of Iquique?\",\n",
        "    \"The Battle of Iquique was a naval engagement that occurred between a Chilean corvette under the command of Arturo Prat and a Peruvian ironclad under the command of Miguel Grau Seminario on 21 May 1879, during the naval stage of the War of the Pacific, and resulted in a Peruvian victory.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryRajxniCKbp"
      },
      "source": [
        "3.3)\n",
        "\n",
        "Pregunta: \"Who introduced peephole connections to LSTM networks?\"\n",
        "Contexto: \"What I‚Äôve described so far is a pretty normal LSTM. But not all LSTMs are the same as the above. In fact, it seems like almost every paper involving LSTMs uses a slightly different version. The differences are minor, but it‚Äôs worth mentioning some of them. One popular LSTM variant, introduced by Gers & Schmidhuber (2000), is adding ‚Äúpeephole connections.‚Äù This means that we let the gate layers look at the cell state.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "metadata": {
        "id": "S1rT9kgLCKuy"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'gers & schmidhuber'"
            ]
          },
          "execution_count": 209,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "entregar_respuesta(\n",
        "    \"Who introduced peephole connections to LSTM networks?\",\n",
        "    \"What I‚Äôve described so far is a pretty normal LSTM. But not all LSTMs are the same as the above. In fact, it seems like almost every paper involving LSTMs uses a slightly different version. The differences are minor, but it‚Äôs worth mentioning some of them. One popular LSTM variant, introduced by Gers & Schmidhuber (2000), is adding ‚Äúpeephole connections.‚Äù This means that we let the gate layers look at the cell state.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqn0vNvKCLAq"
      },
      "source": [
        "3.4)\n",
        "\n",
        "Pregunta: \"When is the cat most active?\"\n",
        "\n",
        "Contexto: \"The cat is similar in anatomy to the other felid species: it has a strong flexible body, quick reflexes, sharp teeth and retractable claws adapted to killing small prey. Its night vision and sense of smell are well developed. Cat communication includes vocalizations like meowing, purring, trilling, hissing, growling and grunting as well as cat-specific body language. It is a solitary hunter but a social species. It can hear sounds too faint or too high in frequency for human ears, such as those made by mice and other small mammals. It is a predator that is most active at dawn and dusk. It secretes and perceives pheromones.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {
        "id": "o5n37FwOCLTG"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'dawn and dusk'"
            ]
          },
          "execution_count": 210,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "entregar_respuesta(\n",
        "    \"When is the cat most active?\",\n",
        "    \"The cat is similar in anatomy to the other felid species: it has a strong flexible body, quick reflexes, sharp teeth and retractable claws adapted to killing small prey. Its night vision and sense of smell are well developed. Cat communication includes vocalizations like meowing, purring, trilling, hissing, growling and grunting as well as cat-specific body language. It is a solitary hunter but a social species. It can hear sounds too faint or too high in frequency for human ears, such as those made by mice and other small mammals. It is a predator that is most active at dawn and dusk. It secretes and perceives pheromones.\"\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
